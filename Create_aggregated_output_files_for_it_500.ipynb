{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate all necessary output files into reference datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matsim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import gzip\n",
    "import duckdb\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Runtime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Import run_info.csv for all sample sizes, alpha and stuck-time values and create data frames with relevant specifications:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 pct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateRuntimeAndReturnDfRow(pathToFile,sampleSize, sampleNr, alpha_value, stuckTime, globalSeed):\n",
    "    temp = pd.read_csv(pathToFile, sep=\",\")\n",
    "    start= pd.Timestamp(temp.iloc[6,1])\n",
    "    end= pd.Timestamp(temp.iloc[7,1])\n",
    "    diff = end - start\n",
    "    rt = pd.DataFrame({'runtime': diff, 'sample_size': sampleSize, 'sample_nr': sampleNr, 'alpha': alpha_value, 'stuck_time': stuckTime, 'global_seed': globalSeed}, index = [0])\n",
    "    return rt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.01\"]\n",
    "storCapF =  [\"0.01\", \"0.03162\"]\n",
    "\n",
    "runtimes_1pct = pd.DataFrame()\n",
    "\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "        for sampleNr in range(1,11,1):\n",
    "            # calculate adjusted stuck time\n",
    "            default_stuck_time = 30.0\n",
    "            adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "            # declare sample size as str \"1-pct\"\n",
    "            sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "\n",
    "            if ((fCf == \"0.01\") & (sCf == '0.01')):\n",
    "                alpha = 1.0\n",
    "                path_case1 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_default_3765/analysis/general/run_info.csv\"\n",
    "                rt_case1= calculateRuntimeAndReturnDfRow(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "\n",
    "                path_case3 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\"+str(sampleNr)+\"-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/analysis/general/run_info.csv\"\n",
    "                rt_case3 = calculateRuntimeAndReturnDfRow(path_case3, sample_size_as_string, sampleNr, alpha, adjusted_stuck_time, 'default')\n",
    "                runtimes_1pct = pd.concat([runtimes_1pct, rt_case1, rt_case3], ignore_index = True)\n",
    "            else:\n",
    "                alpha = 0.75\n",
    "                path_case2 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" + str(sampleNr) + \"-fCf_0.01_sCF_\" +sCf + \"_gS_default_3765/analysis/general/run_info.csv\"\n",
    "                rt_case2 = calculateRuntimeAndReturnDfRow(path_case2, sample_size_as_string, sampleNr, alpha, default_stuck_time, 'default')\n",
    "\n",
    "                path_case4 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\"+str(sampleNr)+\"-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/analysis/general/run_info.csv\"\n",
    "                rt_case4 = calculateRuntimeAndReturnDfRow(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "                runtimes_1pct = pd.concat([runtimes_1pct, rt_case2, rt_case4], ignore_index = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time_1pct_rGs = pd.DataFrame()\n",
    "\n",
    "rGs = [ 4711,3254, 2306, 6384,4338, 6003, 5502, 9377, 5621, 9002 ]\n",
    "for seed in rGs:\n",
    "    if (seed == 4711):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        rt_tmp = runtimes_1pct[(runtimes_1pct['alpha']== 1) & (runtimes_1pct['stuck_time'] == 30.0) & (runtimes_1pct['sample_nr']==1)]['runtime']\n",
    "        temp = {'runtime': runtimes_1pct[\"runtime\"].iloc[0], 'sample_size': '1-pct', 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': global_seed  }\n",
    "        temp = pd.DataFrame(data=temp, index=[rGs.index(4711)])\n",
    "        run_time_1pct_rGs = pd.concat([run_time_1pct_rGs, temp])\n",
    "    elif (seed == 3254):\n",
    "        path =\"/home/lola/math_cluster/output/output-lausitz-1pct-1-fCf_sCF_0.01_gS_3254_3765/analysis/general/run_info.csv\"\n",
    "        temp = pd.read_csv(path, sep = \",\")\n",
    "        start= pd.Timestamp(temp.iloc[6,1])\n",
    "        end= pd.Timestamp(temp.iloc[7,1])\n",
    "        diff = end - start\n",
    "        temp2 = pd.DataFrame({'runtime': diff, 'sample_size': '1-pct', 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': \"rnd_\" + str(seed)  }, index = [rGs.index(3254)])\n",
    "        run_time_1pct_rGs = pd.concat([run_time_1pct_rGs, temp2], axis = 0)\n",
    "    else:\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-1.0-pct-1-fCf_sCF_0.01_gS_\"+str(seed) + \"_3765/analysis/general/run_info.csv\"\n",
    "        temp = pd.read_csv(path, sep = \",\")\n",
    "        start= pd.Timestamp(temp.iloc[6,1])\n",
    "        end= pd.Timestamp(temp.iloc[7,1])\n",
    "        diff = end - start\n",
    "        temp2 = pd.DataFrame({'runtime': diff, 'sample_size': '1-pct', 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': \"rnd_\" + str(seed)  }, index = [rGs.index(seed)] )\n",
    "        run_time_1pct_rGs = pd.concat([run_time_1pct_rGs, temp2], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time_5pct = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(elem) + \"-fCf_sCF_0.05_gS_4711_3765/analysis/general/run_info.csv\"\n",
    "    temp = pd.read_csv(path, sep=\",\")\n",
    "    start= pd.Timestamp(temp.iloc[6,1])\n",
    "    end= pd.Timestamp(temp.iloc[7,1])\n",
    "    diff = end - start\n",
    "    run_time_5pct.append(diff)\n",
    "run_time_5pct = pd.DataFrame({'runtime': run_time_5pct})\n",
    "run_time_5pct.insert(1, 'sample_size', \"5-pct\")\n",
    "run_time_5pct.insert(2, 'alpha', 1.0)\n",
    "run_time_5pct.insert(3, 'stuck_time', 30.0)\n",
    "run_time_5pct.insert(4,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time_5pct_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(elem) + \"-fCf_0.05_sCF_0.10574_gS_4711_3765/analysis/general/run_info.csv\"\n",
    "    temp = pd.read_csv(path, sep=\",\")\n",
    "    start= pd.Timestamp(temp.iloc[6,1])\n",
    "    end= pd.Timestamp(temp.iloc[7,1])\n",
    "    diff = end - start\n",
    "    run_time_5pct_sCf.append(diff)\n",
    "run_time_5pct_sCf = pd.DataFrame({'runtime': run_time_5pct_sCf})\n",
    "run_time_5pct_sCf.insert(1, 'sample_size', \"5-pct\")\n",
    "run_time_5pct_sCf.insert(2, 'alpha', 0.75)\n",
    "run_time_5pct_sCf.insert(3, 'stuck_time', 30.0)\n",
    "run_time_5pct_sCf.insert(4,'global_seed', \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time_5pct_rGs = pd.DataFrame()\n",
    "rGs = [4711, 3254, 2306, 6384,4338, 6003, 5502, 9377, 5621, 9002 ]\n",
    "\n",
    "for seed in rGs:\n",
    "    if (seed ==4711):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        temp = {'runtime': run_time_5pct[\"runtime\"].iloc[0], 'sample_size': '5-pct', 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': global_seed  }\n",
    "        temp = pd.DataFrame(data=temp, index=[rGs.index(4711)])\n",
    "        run_time_5pct_rGs = pd.concat([run_time_5pct_rGs, temp])\n",
    "    else:\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-1-fCf_sCF_0.05_gS_\"+ str(seed) +\"_3765/analysis/general/run_info.csv\"\n",
    "        temp = pd.read_csv(path, sep = \",\")\n",
    "        start= pd.Timestamp(temp.iloc[6,1])\n",
    "        end= pd.Timestamp(temp.iloc[7,1])\n",
    "        diff = end - start\n",
    "        temp2 = pd.DataFrame({'runtime': diff, 'sample_size': '5-pct', 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': \"rnd_\" + str(seed)  }, index = [rGs.index(seed)])\n",
    "        run_time_5pct_rGs = pd.concat([run_time_5pct_rGs, temp2], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time_5pct_sT = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\"+ str(elem) +\"-fCf_sCF_0.05_gS_4711_sT_600.0_3765/analysis/general/run_info.csv\"\n",
    "    temp = pd.read_csv(path, sep = \",\")\n",
    "    start= pd.Timestamp(temp.iloc[6,1])\n",
    "    end= pd.Timestamp(temp.iloc[7,1])\n",
    "    diff = end - start\n",
    "    run_time_5pct_sT.append(diff)\n",
    "run_time_5pct_sT = pd.DataFrame({'runtime': run_time_5pct_sT})\n",
    "run_time_5pct_sT.insert(1, 'sample_size', \"5-pct\")\n",
    "run_time_5pct_sT.insert(2, 'alpha', 1.0)\n",
    "run_time_5pct_sT.insert(3, 'stuck_time', 600.0)\n",
    "run_time_5pct_sT.insert(4,'global_seed', \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time_5pct_sT_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\"+str(elem)+\"-fCf_0.05_sCF_0.10574_gS_4711_sT_600.0_3765/analysis/general/run_info.csv\"\n",
    "    temp = pd.read_csv(path,sep = \",\")\n",
    "    start= pd.Timestamp(temp.iloc[6,1])\n",
    "    end= pd.Timestamp(temp.iloc[7,1])\n",
    "    diff = end - start\n",
    "    run_time_5pct_sT_sCf.append(diff)\n",
    "run_time_5pct_sT_sCf = pd.DataFrame({'runtime': run_time_5pct_sT_sCf})\n",
    "run_time_5pct_sT_sCf.insert(1, 'sample_size', \"5-pct\")\n",
    "run_time_5pct_sT_sCf.insert(2, 'alpha', 0.75)\n",
    "run_time_5pct_sT_sCf.insert(3, 'stuck_time', 600.0)\n",
    "run_time_5pct_sT_sCf.insert(4,'global_seed', \"default\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 pct #### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time_10pct = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" + str(elem) + \"-fCf_sCF_0.1_gS_4711_3765/analysis/general/run_info.csv\"\n",
    "    temp = pd.read_csv(path, sep=\",\")\n",
    "    start= pd.Timestamp(temp.iloc[6,1])\n",
    "    end= pd.Timestamp(temp.iloc[7,1])\n",
    "    diff = end - start\n",
    "    run_time_10pct.append(diff)\n",
    "run_time_10pct = pd.DataFrame({'runtime': run_time_10pct})\n",
    "run_time_10pct.insert(1, 'sample_size', \"10-pct\")\n",
    "run_time_10pct.insert(2, 'alpha', 1.0)\n",
    "run_time_10pct.insert(3, 'stuck_time', 30.0)\n",
    "run_time_10pct.insert(4,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time_10pct_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" + str(elem) + \"-fCf_0.1_sCF_0.17783_gS_4711_3765/analysis/general/run_info.csv\"\n",
    "    temp = pd.read_csv(path, sep=\",\")\n",
    "    start= pd.Timestamp(temp.iloc[6,1])\n",
    "    end= pd.Timestamp(temp.iloc[7,1])\n",
    "    diff = end - start\n",
    "    run_time_10pct_sCf.append(diff)\n",
    "run_time_10pct_sCf = pd.DataFrame({'runtime': run_time_10pct_sCf})\n",
    "run_time_10pct_sCf.insert(1, 'sample_size', \"10-pct\")\n",
    "run_time_10pct_sCf.insert(2, 'alpha', 0.75)\n",
    "run_time_10pct_sCf.insert(3, 'stuck_time', 30.0)\n",
    "run_time_10pct_sCf.insert(4,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_time_10pct_sT = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\"+ str(elem) + \"-fCf_sCF_0.1_gS_4711_sT_300.0_3765/analysis/general/run_info.csv\"\n",
    "    temp = pd.read_csv(path, sep=\",\")\n",
    "    start= pd.Timestamp(temp.iloc[6,1])\n",
    "    end= pd.Timestamp(temp.iloc[7,1])\n",
    "    diff = end - start\n",
    "    run_time_10pct_sT.append(diff)\n",
    "run_time_10pct_sT = pd.DataFrame({'runtime': run_time_10pct_sT})\n",
    "run_time_10pct_sT.insert(1, 'sample_size', \"10-pct\")\n",
    "run_time_10pct_sT.insert(2, 'alpha', 1.0)\n",
    "run_time_10pct_sT.insert(3, 'stuck_time', 300.0)\n",
    "run_time_10pct_sT.insert(4,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time_10pct_sT_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\" +str(elem) + \"-fCf_0.1_sCF_0.17783_gS_4711_sT_300.0_3765/analysis/general/run_info.csv\"\n",
    "    temp = pd.read_csv(path, sep=\",\")\n",
    "    start= pd.Timestamp(temp.iloc[6,1])\n",
    "    end= pd.Timestamp(temp.iloc[7,1])\n",
    "    diff = end - start\n",
    "    run_time_10pct_sT_sCf.append(diff)\n",
    "run_time_10pct_sT_sCf = pd.DataFrame({'runtime': run_time_10pct_sT_sCf})\n",
    "run_time_10pct_sT_sCf.insert(1, 'sample_size', \"10-pct\")\n",
    "run_time_10pct_sT_sCf.insert(2, 'alpha', 0.75)\n",
    "run_time_10pct_sT_sCf.insert(3, 'stuck_time', 300.0)\n",
    "run_time_10pct_sT_sCf.insert(4,'global_seed', \"default\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_sCF_0.25_gS_4711_3765/analysis/general/run_info.csv\"\n",
    "temp = pd.read_csv(path, sep = \",\")\n",
    "start= pd.Timestamp(temp.iloc[6,1])\n",
    "end= pd.Timestamp(temp.iloc[7,1])\n",
    "diff = end - start\n",
    "run_time_25pct = pd.DataFrame({'runtime': diff, 'sample_size': '25-pct', 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': \"default\"  }, index = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_0.25_sCF_0.35355_gS_4711_3765/analysis/general/run_info.csv\"\n",
    "temp = pd.read_csv(path, sep = \",\")\n",
    "start= pd.Timestamp(temp.iloc[6,1])\n",
    "end= pd.Timestamp(temp.iloc[7,1])\n",
    "diff = end - start\n",
    "run_time_25pct_sCf = pd.DataFrame({'runtime': diff, 'sample_size': '25-pct', 'alpha': 0.75, 'stuck_time': 30.0, 'global_seed': \"default\"  }, index = [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-25-pct-1-fCf_sCF_0.25_gS_4711_sT_120.0_3765/analysis/general/run_info.csv\"\n",
    "temp = pd.read_csv(path, sep = \",\")\n",
    "start= pd.Timestamp(temp.iloc[6,1])\n",
    "end= pd.Timestamp(temp.iloc[7,1])\n",
    "diff = end - start\n",
    "run_time_25pct_sT = pd.DataFrame({'runtime': diff, 'sample_size': '25-pct', 'alpha': 1.0, 'stuck_time': 120.0, 'global_seed': \"default\"  }, index = [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-25-pct-1-fCf_0.25_sCF_0.35355_gS_4711_sT_120.0_3765/analysis/general/run_info.csv\"\n",
    "temp = pd.read_csv(path, sep = \",\")\n",
    "start= pd.Timestamp(temp.iloc[6,1])\n",
    "end= pd.Timestamp(temp.iloc[7,1])\n",
    "diff = end - start\n",
    "run_time_25pct_sT_sCf = pd.DataFrame({'runtime': diff, 'sample_size': '25-pct', 'alpha': 0.75, 'stuck_time': 120.0, 'global_seed': \"default\"  }, index = [3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_sCF_0.5_gS_4711_3765/analysis/general/run_info.csv\"\n",
    "temp = pd.read_csv(path, sep = \",\")\n",
    "start= pd.Timestamp(temp.iloc[6,1])\n",
    "end= pd.Timestamp(temp.iloc[7,1])\n",
    "diff = end - start\n",
    "run_time_50pct = pd.DataFrame({'runtime': diff, 'sample_size': '50-pct', 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': \"default\"  }, index = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_0.5_sCF_0.5946_gS_4711_3765/analysis/general/run_info.csv\"\n",
    "temp = pd.read_csv(path, sep = \",\")\n",
    "start= pd.Timestamp(temp.iloc[6,1])\n",
    "end= pd.Timestamp(temp.iloc[7,1])\n",
    "diff = end - start\n",
    "run_time_50pct_sCf = pd.DataFrame({'runtime': diff, 'sample_size': '50-pct', 'alpha': 0.75, 'stuck_time': 30.0, 'global_seed': \"default\"  }, index = [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_sCF_0.5_gS_4711_sT_60.0_3765/analysis/general/run_info.csv\"\n",
    "temp = pd.read_csv(path, sep = \",\")\n",
    "start= pd.Timestamp(temp.iloc[6,1])\n",
    "end= pd.Timestamp(temp.iloc[7,1])\n",
    "diff = end - start\n",
    "run_time_50pct_sT = pd.DataFrame({'runtime': diff, 'sample_size': '50-pct', 'alpha': 1.0, 'stuck_time': 60.0, 'global_seed': \"default\"  }, index = [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_0.5_sCF_0.5946_gS_4711_sT_60.0_3765/analysis/general/run_info.csv\"\n",
    "temp = pd.read_csv(path, sep = \",\")\n",
    "start= pd.Timestamp(temp.iloc[6,1])\n",
    "end= pd.Timestamp(temp.iloc[7,1])\n",
    "diff = end - start\n",
    "run_time_50pct_sT_sCf = pd.DataFrame({'runtime': diff, 'sample_size': '50-pct', 'alpha': 0.75, 'stuck_time': 60.0, 'global_seed': \"default\"  }, index = [3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25-pct-doubled-fCf_0.5_sCF_0.5_gS_4711_3765/analysis/general/run_info.csv\"\n",
    "temp = pd.read_csv(path, sep = \",\")\n",
    "start= pd.Timestamp(temp.iloc[6,1])\n",
    "end= pd.Timestamp(temp.iloc[7,1])\n",
    "diff = end - start\n",
    "run_time_25_pct_doubled = pd.DataFrame({'runtime': diff, 'sample_size': '25-pct-doubled', 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': \"default\"  }, index = [4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  \"/home/lola/math_cluster/output/output-lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765/analysis/general/run_info.csv\"\n",
    "temp = pd.read_csv(path, sep = \",\")\n",
    "start= pd.Timestamp(temp.iloc[6,1])\n",
    "end= pd.Timestamp(temp.iloc[7,1])\n",
    "diff = end - start\n",
    "run_time_100pct = pd.DataFrame({'runtime': diff, 'sample_size': '100-pct', 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': \"default\"  }, index = [0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25.0-pct-quadrupled-fCf_1.0_sCF_1.0_gS_4711_3765/analysis/general/run_info.csv\"\n",
    "temp = pd.read_csv(path, sep = \",\")\n",
    "start= pd.Timestamp(temp.iloc[6,1])\n",
    "end= pd.Timestamp(temp.iloc[7,1])\n",
    "diff = end - start\n",
    "run_time_25_pct_quadrupled = pd.DataFrame({'runtime': diff, 'sample_size': '25-pct-quadrupled', 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': \"default\"  }, index = [4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pd.Concat,  convert minutes into decimal base?! and write csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat !\n",
    "runtimes_1_100 = pd.concat([runtimes_1pct,  run_time_1pct_rGs,\n",
    "                            run_time_5pct,run_time_5pct_sCf, run_time_5pct_sT, run_time_5pct_sT_sCf, run_time_5pct_rGs,\n",
    "                            run_time_10pct,run_time_10pct_sCf, run_time_10pct_sT, run_time_10pct_sT_sCf,                  \n",
    "                              run_time_25pct,run_time_25pct_sCf, run_time_25pct_sT, run_time_25pct_sT_sCf,\n",
    "                                run_time_50pct,run_time_50pct_sCf, run_time_50pct_sT, run_time_50pct_sT_sCf,run_time_25_pct_doubled,\n",
    "                                run_time_100pct, run_time_25_pct_quadrupled  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform minutes into decimal \n",
    "runtimes_1_100['runtime'] =  runtimes_1_100.runtime.astype('timedelta64[s]')/  pd.Timedelta(minutes=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimes_1_100 = runtimes_1_100.iloc[:, [1, 2, 3, 4,0]]\n",
    "runtimes_1_100.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/runtimes_1_to_100pct_correct.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Number of stuck-time violations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to filter the output events and count number of stuck-time violations per vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of stuck vehicles\n",
    "def countNumberOfStuckTimeViolations(pathToEvents, sampleSize, sampleNr, alpha, globalSeed, stuckTime):\n",
    "    events_file = pathToEvents\n",
    "   \n",
    "\n",
    "    # Read events - filter and return the listed event types only\n",
    "    events = matsim.event_reader(\n",
    "        events_file,\n",
    "        types=\"stuckAndContinue\",\n",
    "    )\n",
    "\n",
    "\n",
    "    stuck_time_violations = pd.DataFrame()\n",
    "\n",
    "    # Loop on all filtered events\n",
    "    for event in events:\n",
    "        if event[\"type\"] == \"stuckAndContinue\":\n",
    "            temp = pd.DataFrame(event, index = [0])\n",
    "            stuck_time_violations = pd.concat([stuck_time_violations, temp], ignore_index= True)\n",
    "    \n",
    "    #outputPath = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs/stuck_time_violations_detailed_per_run/stuck_time_violations_lausitz_\" + sampleSize +\"-\"+str(sampleNr) + \"-alpha-\"+ str(alpha) + \"-gS-\" + globalSeed + \"-sT-\" + str(stuckTime) + \".csv\"\n",
    "    #stuck_time_violations.to_csv(outputPath, index = False)\n",
    "    temp = pd.DataFrame({'n_stuck_time_violations': stuck_time_violations.shape[0], 'sample_size': sampleSize, 'sample_nr':sampleNr, 'alpha': alpha, 'global_seed': globalSeed, 'stuck_time': stuckTime }, index =[0])\n",
    "    return temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.01\"]\n",
    "storCapF =  [\"0.01\", \"0.03162\"]\n",
    "\n",
    "stuck_veh_1pct = pd.DataFrame()\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "        for sampleNr in range(1,11,1):\n",
    "            # calculate adjusted stuck time\n",
    "            default_stuck_time = 30.0\n",
    "            adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "            # declare sample size as str \"1-pct\"\n",
    "            sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "\n",
    "\n",
    "            if ((fCf == \"0.01\") & (sCf == '0.01')):\n",
    "                alpha = 1.0\n",
    "                # paths for case 1 and 3 \n",
    "                path_case1 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" + str(sampleNr) + \"-fCf_sCF_\"+ sCf + \"_gS_default_3765/lausitz-1pct-\"+str(sampleNr)+ \"-fCf_sCf_0.01_gS_default_3765.output_events.xml.gz\"\n",
    "                path_case3 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-1-pct-\" + str(sampleNr) + \"-fCf_sCF_0.01_gS_4711_sT_3000.0_3765.output_events.xml.gz\"\n",
    "                \n",
    "                # calculate number of stuck time violations and attach them to the data frame\n",
    "                temp_case1 = countNumberOfStuckTimeViolations(path_case1, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                temp_case3 = countNumberOfStuckTimeViolations(path_case3, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                stuck_veh_1pct = pd.concat([stuck_veh_1pct, temp_case1, temp_case3], ignore_index= True)\n",
    "\n",
    "            else:\n",
    "                alpha = 0.75\n",
    "                # paths for case 2 and 4 \n",
    "                path_case2 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" + str(sampleNr) + \"-fCf_0.01_sCF_\" + sCf + \"_gS_default_3765/lausitz-1pct-\" +str(sampleNr)+ \"-fCf_0.01_sCf_0.03162_gS_default_3765.output_events.xml.gz\"\n",
    "                path_case4 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" + str(sampleNr) + \"-fCf_0.01_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-1-pct-\" + str(sampleNr) +\"-fCf_0.01_sCF_0.03162_gS_4711_sT_3000.0_3765.output_events.xml.gz\"\n",
    "                \n",
    "                # calculate number of stuck time violations and attach them to the data frame\n",
    "                temp_case2 = countNumberOfStuckTimeViolations(path_case2, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                temp_case4 = countNumberOfStuckTimeViolations(path_case4, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                stuck_veh_1pct = pd.concat([stuck_veh_1pct, temp_case2, temp_case4], ignore_index= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1 pct random seed\n",
    "stuck_time_violations_1pct_rGs = pd.DataFrame()\n",
    "rGs = [4711,3254, 2306, 6384,4338, 6003, 5502, 9377, 5621, 9002 ]\n",
    "for seed in rGs:\n",
    "    if (seed == 4711):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        # insert number of stuck time violations from the first 1 pct sample\n",
    "        stuck_time_violations = stuck_veh_1pct[(stuck_veh_1pct['alpha'] == 1.0) & (stuck_veh_1pct['stuck_time'] == 30.0) & (stuck_veh_1pct[\"sample_nr\"] == 1)][\"n_stuck_time_violations\"]\n",
    "        temp = {'n_stuck_time_violations': stuck_time_violations, 'sample_size': '1-pct','sample_nr': 1, 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': global_seed  }\n",
    "        temp = pd.DataFrame(data=temp, index=[rGs.index(4711)])\n",
    "        stuck_time_violations_1pct_rGs = pd.concat([stuck_time_violations_1pct_rGs, temp])\n",
    "    elif (seed == 3254):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        path =\"/home/lola/math_cluster/output/output-lausitz-1pct-1-fCf_sCF_0.01_gS_3254_3765/lausitz-1pct-1-fCf_sCf_0.01_gS_3254_3765.output_events.xml.gz\"\n",
    "        stuck_time_violations = countNumberOfStuckTimeViolations(path, \"1-pct\", 1, 1.0 , global_seed, 30.0)\n",
    "        stuck_time_violations_1pct_rGs = pd.concat([stuck_time_violations_1pct_rGs, stuck_time_violations], axis = 0, ignore_index= True)\n",
    "    else:\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-1.0-pct-1-fCf_sCF_0.01_gS_\" +str(seed) + \"_3765/lausitz-1.0-pct-1-fCf_sCF_0.01_gS_\" + str(seed) + \"_3765.output_events.xml.gz\"\n",
    "        stuck_time_violations = countNumberOfStuckTimeViolations(path, \"1-pct\", 1, 1.0 , global_seed, 30.0)\n",
    "        stuck_time_violations_1pct_rGs = pd.concat([stuck_time_violations_1pct_rGs, stuck_time_violations], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenae and write to file\n",
    "stuck_time_violation_1pct = pd.concat([stuck_veh_1pct, stuck_time_violations_1pct_rGs])\n",
    "stuck_time_violation_1pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/stuck_time_violations_1_pct.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.05\"]\n",
    "storCapF =  [\"0.05\", \"0.10574\"]\n",
    "\n",
    "stuck_time_violations_5pct = pd.DataFrame()\n",
    "\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,11,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "\n",
    "                # declare paths \n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.05\") & (sCf == \"0.05\")):\n",
    "                    alpha = 1.0\n",
    "                    if (sampleNr == 6):\n",
    "                        path_case1  = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765/lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765-2.output_events.xml.gz\"\n",
    "                    \n",
    "                    else: \n",
    "                        path_case1 = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_3765/lausitz-5.0-pct-\"+str(sampleNr)+ \"-fCf_sCF_0.05_gS_4711_3765.output_events.xml.gz\"\n",
    "                    \n",
    "                    \n",
    "                    path_case3 =  \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-5-pct-\" + str(sampleNr) +\"-fCf_sCF_0.05_gS_4711_sT_600.0_3765.output_events.xml.gz\"\n",
    "\n",
    "                    temp_case1 = countNumberOfStuckTimeViolations(path_case1, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case3 = countNumberOfStuckTimeViolations(path_case3, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    stuck_time_violations_5pct = pd.concat([stuck_time_violations_5pct, temp_case1, temp_case3], ignore_index= True)\n",
    "                    \n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    if(sampleNr == 6):\n",
    "                        path_case2 = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_0.05_sCF_0.10574_gS_4711_3765/lausitz-5.0-pct-6-fCf_0.05_sCF_0.10574_gS_4711_3765-2.output_events.xml.gz\"\n",
    "                    else:\n",
    "                        path_case2 = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf  + \"_gS_4711_3765/lausitz-5.0-pct-\"+str(sampleNr)+ \"-fCf_0.05_sCF_0.10574_gS_4711_3765.output_events.xml.gz\"\n",
    "                    \n",
    "                    \n",
    "                    path_case4 = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str (sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-5-pct-\" + str(sampleNr) + \"-fCf_0.05_sCF_0.10574_gS_4711_sT_600.0_3765.output_events.xml.gz\"\n",
    "\n",
    "                    # calculate number of stuck time violations and attach them to the data frame\n",
    "                    temp_case2 = countNumberOfStuckTimeViolations(path_case2, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case4 = countNumberOfStuckTimeViolations(path_case4, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    stuck_time_violations_5pct = pd.concat([stuck_time_violations_5pct, temp_case2, temp_case4], ignore_index= True)\n",
    "                \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 pct random global seed, alpha = 1\n",
    "stuck_time_violations_5pct_rGs = pd.DataFrame()\n",
    "rGs = [4711, 3254, 2306, 6384,4338, 6003, 5502, 9377, 5621, 9002 ]\n",
    "\n",
    "for seed in rGs:\n",
    "    if (seed ==4711):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        stuck_time_violations = stuck_time_violations_5pct[(stuck_time_violations_5pct['alpha'] == 1.0) & (stuck_time_violations_5pct['stuck_time'] == 30.0) & (stuck_time_violations_5pct[\"sample_nr\"] == 1)][\"n_stuck_time_violations\"]\n",
    "        temp = {'n_stuck_time_violations': stuck_time_violations, 'sample_size': '5-pct','sample_nr': 1, 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': global_seed  }\n",
    "        temp = pd.DataFrame(data=temp, index=[rGs.index(4711)])\n",
    "        stuck_time_violations_5pct_rGs = pd.concat([stuck_time_violations_5pct_rGs, temp])\n",
    "    else:\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-1-fCf_sCF_0.05_gS_\" + str(seed) + \"_3765/lausitz-5.0-pct-1-fCf_sCF_0.05_gS_\"+ str(seed) +\"_3765.output_events.xml.gz\"\n",
    "        \n",
    "        stuck_time_violations = countNumberOfStuckTimeViolations(path, \"5-pct\", 1, 1.0 , global_seed, 30.0)\n",
    "        stuck_time_violations_5pct_rGs = pd.concat([stuck_time_violations_5pct_rGs, stuck_time_violations], axis = 0, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuck_time_violation_5pct = pd.concat([stuck_time_violations_5pct, stuck_time_violations_5pct_rGs])\n",
    "stuck_time_violation_5pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/stuck_time_violations_5_pct.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.1\"]\n",
    "storCapF =  [\"0.1\", \"0.17783\"]\n",
    "\n",
    "stuck_time_violations_10pct = pd.DataFrame()\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,11,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.1\") & (sCf == \"0.1\")):\n",
    "                    alpha = 1.0\n",
    "                    path_case1 = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_3765/lausitz-10.0-pct-\" + str(sampleNr) + \"-fCf_sCF_0.1_gS_4711_3765.output_events.xml.gz\"\n",
    "                    path_case3 = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-10-pct-\" + str(sampleNr) + \"-fCf_sCF_0.1_gS_4711_sT_300.0_3765.output_events.xml.gz\"\n",
    "                    \n",
    "                    temp_case1 = countNumberOfStuckTimeViolations(path_case1, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case3 = countNumberOfStuckTimeViolations(path_case3, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    stuck_time_violations_10pct = pd.concat([stuck_time_violations_10pct, temp_case1, temp_case3], ignore_index= True)     \n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    path_case2 = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\"+ str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf +\"_gS_4711_3765/lausitz-10.0-pct-\" + str(sampleNr) + \"-fCf_0.1_sCF_0.17783_gS_4711_3765.output_events.xml.gz\"\n",
    "                    path_case4 =\"/home/lola/math_cluster/output/output-lausitz-10-pct-\" + str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-10-pct-\" + str(sampleNr) + \"-fCf_0.1_sCF_0.17783_gS_4711_sT_300.0_3765.output_events.xml.gz\"\n",
    "\n",
    "                    # calculate number of stuck time violations and attach them to the data frame\n",
    "                    temp_case2 = countNumberOfStuckTimeViolations(path_case2, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case4 = countNumberOfStuckTimeViolations(path_case4, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    stuck_time_violations_10pct = pd.concat([stuck_time_violations_10pct, temp_case2, temp_case4], ignore_index= True)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "stuck_time_violations_10pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/stuck_time_violations_10_pct.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.25\"]\n",
    "storCapF =  [\"0.25\", \"0.35355\"]\n",
    "\n",
    "stuck_time_violations_25pct = pd.DataFrame()\n",
    "counter = 0\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,2,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "                \n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.25\") & (sCf == \"0.25\")):\n",
    "                    alpha = 1.0\n",
    "                    path_case1 = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_sCF_0.25_gS_4711_3765/lausitz-25.0-pct-fCf_sCF_0.25_gS_4711_3765.output_events.xml.gz\"\n",
    "                    path_case3 = \"/home/lola/math_cluster/output/output-lausitz-25-pct-1-fCf_sCF_0.25_gS_4711_sT_120.0_3765/lausitz-25-pct-1-fCf_sCF_0.25_gS_4711_sT_120.0_3765.output_events.xml.gz\"\n",
    "\n",
    "                    temp_case1 = countNumberOfStuckTimeViolations(path_case1, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case3 = countNumberOfStuckTimeViolations(path_case3, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    stuck_time_violations_25pct = pd.concat([stuck_time_violations_25pct, temp_case1, temp_case3], ignore_index= True)  \n",
    "\n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    path_case2 = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_0.25_sCF_0.35355_gS_4711_3765/lausitz-25.0-pct-fCf_0.25_sCF_0.35355_gS_4711_3765.output_events.xml.gz\"\n",
    "                    path_case4 = \"/home/lola/math_cluster/output/output-lausitz-25-pct-1-fCf_0.25_sCF_0.35355_gS_4711_sT_120.0_3765/lausitz-25-pct-1-fCf_0.25_sCF_0.35355_gS_4711_sT_120.0_3765.output_events.xml.gz\"\n",
    "\n",
    "                    # calculate number of stuck time violations and attach them to the data frame\n",
    "                    temp_case2 = countNumberOfStuckTimeViolations(path_case2, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case4 = countNumberOfStuckTimeViolations(path_case4, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    stuck_time_violations_25pct = pd.concat([stuck_time_violations_25pct, temp_case2, temp_case4], ignore_index= True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuck_time_violations_25pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/stuck_time_violations_25_pct.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.5\"]\n",
    "storCapF =  [\"0.5\", \"0.5946\"]\n",
    "\n",
    "stuck_time_violations_50pct = pd.DataFrame()\n",
    "counter = 0\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,2,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "\n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.5\") & (sCf == \"0.5\")):\n",
    "                    alpha = 1.0\n",
    "                    path_case1 = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_sCF_0.5_gS_4711_3765/lausitz-50.0-pct-fCf_sCF_0.5_gS_4711_3765.output_events.xml.gz\"\n",
    "                    path_case3 = \"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_sCF_0.5_gS_4711_sT_60.0_3765/lausitz-50-pct-1-fCf_sCF_0.5_gS_4711_sT_60.0_3765.output_events.xml.gz\"\n",
    "                    \n",
    "                    # calculate number of stuck time violations and attach them to the data frame\n",
    "                    temp_case1 = countNumberOfStuckTimeViolations(path_case1, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case3 = countNumberOfStuckTimeViolations(path_case3, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    stuck_time_violations_50pct = pd.concat([stuck_time_violations_50pct, temp_case1, temp_case3], ignore_index= True)     \n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    path_case2 = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_0.5_sCF_0.5946_gS_4711_3765/lausitz-50.0-pct-fCf_0.5_sCF_0.5946_gS_4711_3765.output_events.xml.gz\"\n",
    "                    path_case4 =\"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_0.5_sCF_0.5946_gS_4711_sT_60.0_3765/lausitz-50-pct-1-fCf_0.5_sCF_0.5946_gS_4711_sT_60.0_3765.output_events.xml.gz\"\n",
    "                    \n",
    "                    # calculate number of stuck time violations and attach them to the data frame\n",
    "                    temp_case2 = countNumberOfStuckTimeViolations(path_case2, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case4 = countNumberOfStuckTimeViolations(path_case4, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    stuck_time_violations_50pct = pd.concat([stuck_time_violations_50pct, temp_case2, temp_case4], ignore_index= True)\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25 pct doubled\n",
    "path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25-pct-doubled-fCf_0.5_sCF_0.5_gS_4711_3765/lausitz-25-pct-doubled-fCf_0.5_sCF_0.5_gS_4711__3765.output_events.xml.gz\"\n",
    "stuck_time_violations_25pct_doubled = countNumberOfStuckTimeViolations(path, \"25-pct-doubled\", 1, 1.0, \"default\", 30.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuck_time_violation_50pct = pd.concat([stuck_time_violations_50pct, stuck_time_violations_25pct_doubled])\n",
    "stuck_time_violation_50pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/stuck_time_violations_50_pct.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 pct\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765/lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765.output_events.xml.gz\"\n",
    "stuck_time_violations_100pct = countNumberOfStuckTimeViolations(path,  \"100-pct\", 1, 1.0, \"default\", 30.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25.0-pct-quadrupled-fCf_1.0_sCF_1.0_gS_4711_3765/lausitz-25-pct-quadrupled-fCf_1.0_sCF_1.0_gS_4711__3765.output_events.xml.gz\"\n",
    "stuck_time_violations_25pct_quadrupled = countNumberOfStuckTimeViolations(path,  \"25-pct-quadrupled\", 1, 1.0, \"default\", 30.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuck_time_violation_100pct = pd.concat([stuck_time_violations_100pct, stuck_time_violations_25pct_quadrupled])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pd.concat and write csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat\n",
    "stuck_time_violation_1_100 = pd.concat([stuck_time_violation_1pct, stuck_time_violation_5pct, stuck_time_violations_10pct,\n",
    "                                        stuck_time_violations_25pct, stuck_time_violations_50pct, stuck_time_violation_100pct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write csv\n",
    "stuck_time_violation_1_100.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/stuck_time_violations_1_to_100pct_July_28_2025.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. time distribution of stuck time violations and link types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def networkInfoToDf(pathToNetwork):\n",
    "    input = gzip.open(pathToNetwork, 'r')\n",
    "    tree = ET.parse(input)\n",
    "    root = tree.getroot()\n",
    "    # convert network to data frame \n",
    "    ids = []\n",
    "    length = []\n",
    "    freespeed = []\n",
    "    capacity = []\n",
    "    type_of_link = []\n",
    "\n",
    "    for links in root.findall('links'):\n",
    "        for link in links:\n",
    "            #print(link.tag, link.attrib)\n",
    "            ids.append(link.attrib['id'])\n",
    "            length.append(float(link.attrib['length']))\n",
    "            freespeed.append(float(link.attrib['freespeed'])*3.6)\n",
    "            capacity.append(float(link.attrib['capacity']))\n",
    "            type_counter = 0\n",
    "            for child in link:\n",
    "                for attr in child:\n",
    "                    if (attr.attrib['name'] == \"type\"):\n",
    "                        try:\n",
    "                            type_of_link.append(attr.text)\n",
    "                            type_counter = 1\n",
    "                        except:\n",
    "                            type_of_link.append('NA')\n",
    "            if(type_counter == 0):\n",
    "                type_of_link.append('NA')\n",
    "    network_df = pd.DataFrame({'link': ids, 'length': length, 'freespeed': freespeed, 'capacity': capacity, 'link_type': type_of_link })\n",
    "    return network_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToLausitzNetwork = '/home/lola/Downloads/lausitz-v2024.2-network.xml.gz'\n",
    "df_net = networkInfoToDf(pathToLausitzNetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countStuckTimeViolationsPerHour(pathToFile, sampleSizeString, sample_nr,  alpha, stuck_time):\n",
    "    df_sTv = pd.read_csv(pathToFile)\n",
    "    df_sTv['hour'] = np.floor(df_sTv['time']/ 3600)\n",
    "\n",
    "    n_sTv_per_hour = []\n",
    "    hour_storage = []\n",
    "    alpha_storage = []\n",
    "    sampleNr_storage = []\n",
    "    sample_size_storage = []\n",
    "    stuck_time_storage = []\n",
    "    \n",
    "    for hour in range(0,36,1): \n",
    "        hour_storage.append(hour)\n",
    "        n_sTv_per_hour.append(df_sTv[df_sTv['hour'] == hour].shape[0])\n",
    "        sample_size_storage.append(sampleSizeString)\n",
    "        alpha_storage.append(alpha)\n",
    "        sampleNr_storage.append(sample_nr)\n",
    "        stuck_time_storage.append(stuck_time)\n",
    "\n",
    "    df_sTv_perHour = pd.DataFrame({'hour': hour_storage, 'n_sTv': n_sTv_per_hour,'sample_size': sampleSizeString, 'sample_nr': sampleNr_storage, 'alpha': alpha_storage,  'stuck_time': stuck_time_storage})\n",
    "    return df_sTv_perHour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countLinkTypesOfNumberOfStuckTimeVioaltions(df_net,pathToFile, sampleSizeString, sample_nr, alpha, stuck_time):\n",
    "    df_sTv = pd.read_csv(pathToFile)\n",
    "    df_sTv = pd.merge(df_sTv, df_net, on = \"link\", how= \"left\")\n",
    "    n_sTv_per_link_type = []\n",
    "    link_type_storage = []\n",
    "    alpha_storage = []\n",
    "    sample_size_storage = []\n",
    "    sampleNr_storage = []\n",
    "    stuck_time_storage = []\n",
    "    for link_type in df_sTv['link_type'].unique(): \n",
    "        link_type_storage.append(link_type)\n",
    "        n_sTv_per_link_type.append(df_sTv[df_sTv['link_type'] == link_type].shape[0])\n",
    "        alpha_storage.append(alpha)\n",
    "        sample_size_storage.append(sampleSizeString)\n",
    "        sampleNr_storage.append(sample_nr)\n",
    "        stuck_time_storage.append(stuck_time)\n",
    "\n",
    "    df_sTv_LinkType = pd.DataFrame({'link_type': link_type_storage, 'n_sTv': n_sTv_per_link_type, 'sample_size': sample_size_storage, 'sample_nr': sampleNr_storage,'alpha': alpha_storage, 'stuck_time': stuck_time_storage})\n",
    "    return df_sTv_LinkType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countLinkTypesAndStVPerHour(df_net,pathToFile,sampleSizeString, sample_nr, alpha, stuck_time):\n",
    "\n",
    "    res1 = countStuckTimeViolationsPerHour(pathToFile, sampleSizeString, sample_nr, alpha, stuck_time)\n",
    "\n",
    "    res2 = countLinkTypesOfNumberOfStuckTimeVioaltions(df_net,pathToFile, sampleSizeString, sample_nr, alpha, stuck_time)\n",
    "    return [res1, res2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSizes = [\"0.01\", \"0.05\", \"0.1\", \"0.25\", \"0.5\", \"1.0\"]\n",
    "alpha_values = [\"1.0\", \"0.75\"]\n",
    "\n",
    "base_path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs/stuck_time_viiolations_detailed_per_run/\"\n",
    "df_nStV_per_hour = pd.DataFrame()\n",
    "df_nStV_per_link_type = pd.DataFrame()\n",
    "\n",
    "for sampleSize in sampleSizes:\n",
    "    for sampleNr in range(1,11,1):\n",
    "        # calculate adjusted stuck time\n",
    "        default_stuck_time = 30.0\n",
    "        adjusted_stuck_time = 30.0/float(sampleSize)\n",
    "        # declare sample size as str \"1-pct\"\n",
    "        sample_size_as_string = str(int(float(sampleSize)*100)) + \"-pct\"\n",
    "        if ((float(sampleSize) > 0.1) & (sampleNr >= 2)):\n",
    "            continue\n",
    "\n",
    "        elif ((sampleSize == \"1.0\") & (sampleNr == 1)):\n",
    "            path_case_1 = base_path + \"stuck_time_violations_lausitz_\" + sample_size_as_string +\"-\"+ str(sampleNr) +\"-alpha-\"+ str(1.0) + \"-gS-default-sT-\" + str(default_stuck_time) + \".csv\"\n",
    "            res_case_1 = countLinkTypesAndStVPerHour(df_net, path_case_1, sample_size_as_string, sampleNr, 1.0, default_stuck_time)\n",
    "            df_nStV_per_hour = pd.concat([df_nStV_per_hour, res_case_1[0]], ignore_index= True)\n",
    "            df_nStV_per_link_type = pd.concat([df_nStV_per_link_type, res_case_1[1]], ignore_index= True)\n",
    "            break  \n",
    "            \n",
    "\n",
    "        path_case_1 = base_path + \"stuck_time_violations_lausitz_\" + sample_size_as_string +\"-\"+ str(sampleNr) +\"-alpha-\"+ str(1.0) + \"-gS-default-sT-\" + str(default_stuck_time) + \".csv\"\n",
    "        path_case_2 = base_path + \"stuck_time_violations_lausitz_\" + sample_size_as_string +\"-\"+ str(sampleNr) +\"-alpha-\"+ str(0.75) + \"-gS-default-sT-\" + str(default_stuck_time) + \".csv\"\n",
    "        path_case_3 = base_path + \"stuck_time_violations_lausitz_\" + sample_size_as_string +\"-\"+ str(sampleNr) +\"-alpha-\"+ str(1.0) + \"-gS-default-sT-\" + str(adjusted_stuck_time) + \".csv\"\n",
    "        path_case_4 = base_path + \"stuck_time_violations_lausitz_\" + sample_size_as_string +\"-\"+ str(sampleNr) +\"-alpha-\"+ str(0.75) + \"-gS-default-sT-\" + str(adjusted_stuck_time) + \".csv\"\n",
    "        \n",
    "        # calculate number of stuck time violations and attach them to the data frame\n",
    "        res_case_1 = countLinkTypesAndStVPerHour(df_net, path_case_1, sample_size_as_string, sampleNr, 1.0, default_stuck_time)\n",
    "        res_case_2 = countLinkTypesAndStVPerHour(df_net, path_case_2, sample_size_as_string, sampleNr, 0.75, default_stuck_time)\n",
    "        res_case_3 = countLinkTypesAndStVPerHour(df_net, path_case_3, sample_size_as_string, sampleNr, 1.0, adjusted_stuck_time)\n",
    "        res_case_4 = countLinkTypesAndStVPerHour(df_net, path_case_4, sample_size_as_string, sampleNr, 0.75, adjusted_stuck_time)\n",
    "        df_nStV_per_hour = pd.concat([df_nStV_per_hour, res_case_1[0],res_case_2[0], res_case_3[0], res_case_4[0]], ignore_index= True)\n",
    "        df_nStV_per_link_type = pd.concat([df_nStV_per_link_type, res_case_1[1], res_case_2[1], res_case_3[1], res_case_4[1]], ignore_index= True)     \n",
    "\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nStV_per_hour.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/N_sTV_per_hour.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nStV_per_link_type.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/N_sTV_per_Link_Type.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Count number of link leave events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to filter output_events and count the number of link leave events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of link leave events\n",
    "def countLinkLeaveEvents(pathToEvents, sampleSize, sampleNr, alpha, globalSeed, stuckTime):\n",
    "    events_file = pathToEvents\n",
    "\n",
    "    # Read events - filter and return the listed event types only\n",
    "    events = matsim.event_reader(\n",
    "        events_file,\n",
    "        types=\"left link\",\n",
    "    )\n",
    "   \n",
    "    # initialize counter for link leave events\n",
    "    link_leave_event_counter = 0 \n",
    "\n",
    "    # stream hour of link leave event to file\n",
    "    with open(f\"/home/lola/Nextcloud/Masterarbeit/03_Outputs/Link_leave_hour/link_leave_hour_{sampleSize}_sample_nr_{str(sampleNr)}_alpha_{str(alpha)}_gS_{globalSeed}_sT_{str(stuckTime)}.csv\", 'w', newline='') as csvfile:\n",
    "        fieldnames = [\"time\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Loop on all filtered events\n",
    "        for event in events:\n",
    "            if event[\"type\"] == \"left link\":\n",
    "                if(\"pt_\" in event['link']):\n",
    "                    continue\n",
    "                link_leave_event_counter += 1\n",
    "                myDict = {key: event[key] for key in fieldnames}\n",
    "                writer.writerow(myDict)\n",
    "    \n",
    "\n",
    "    \n",
    "    res = pd.DataFrame({'n_link_leave_events': link_leave_event_counter, 'sample_size': sampleSize, 'sample_nr':sampleNr, 'alpha': alpha, 'global_seed': globalSeed, 'stuck_time': stuckTime }, index =[0])\n",
    "    \n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.01\"]\n",
    "storCapF =  [\"0.01\", \"0.03162\"]\n",
    "\n",
    "link_leave_events_1pct = pd.DataFrame()\n",
    "\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "        for sampleNr in range(1,11,1):\n",
    "            # calculate adjusted stuck time\n",
    "            default_stuck_time = 30.0\n",
    "            adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "            # declare sample size as str \"1-pct\"\n",
    "            sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "\n",
    "            if ((fCf == \"0.01\") & (sCf == '0.01')):\n",
    "                alpha = 1.0\n",
    "                # paths for case 1 and 3 \n",
    "                path_case1 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" + str(sampleNr) + \"-fCf_sCF_\"+ sCf + \"_gS_default_3765/lausitz-1pct-\"+str(sampleNr)+ \"-fCf_sCf_0.01_gS_default_3765.output_events.xml.gz\"\n",
    "                path_case3 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-1-pct-\" + str(sampleNr) + \"-fCf_sCF_0.01_gS_4711_sT_3000.0_3765.output_events.xml.gz\"\n",
    "                \n",
    "                # calculate number of stuck time violations and attach them to the data frame\n",
    "                temp_case1 = countLinkLeaveEvents(path_case1, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                temp_case3 = countLinkLeaveEvents(path_case3, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                link_leave_events_1pct = pd.concat([link_leave_events_1pct, temp_case1, temp_case3], ignore_index= True)\n",
    " \n",
    "            else:\n",
    "                alpha = 0.75\n",
    "                # paths for case 2 and 4 \n",
    "                path_case2 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" + str(sampleNr) + \"-fCf_0.01_sCF_\" + sCf + \"_gS_default_3765/lausitz-1pct-\" +str(sampleNr)+ \"-fCf_0.01_sCf_0.03162_gS_default_3765.output_events.xml.gz\"\n",
    "                path_case4 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" + str(sampleNr) + \"-fCf_0.01_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-1-pct-\" + str(sampleNr) +\"-fCf_0.01_sCF_0.03162_gS_4711_sT_3000.0_3765.output_events.xml.gz\"\n",
    "                \n",
    "                # calculate number of stuck time violations and attach them to the data frame\n",
    "                temp_case2 = countLinkLeaveEvents(path_case2, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                temp_case4 = countLinkLeaveEvents(path_case4, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                link_leave_events_1pct = pd.concat([link_leave_events_1pct, temp_case2, temp_case4], ignore_index= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1 pct random seed\n",
    "link_leave_1pct_rGs = pd.DataFrame()\n",
    "rGs = [ 4711,3254, 2306, 6384,4338, 6003, 5502, 9377, 5621, 9002 ]\n",
    "for seed in rGs:\n",
    "    global_seed = \"rnd_\" + str(seed)\n",
    "    if (seed == 4711):  \n",
    "        \n",
    "        linke_leave_events = link_leave_events_1pct[(link_leave_events_1pct['alpha'] == 1.0) & (link_leave_events_1pct['stuck_time'] == 30.0) & (link_leave_events_1pct['sample_nr'] == 1)]['n_link_leave_events']\n",
    "        temp = {'n_link_leave_events': linke_leave_events , 'sample_size': '1-pct', 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': global_seed  }\n",
    "        temp = pd.DataFrame(data=temp, index=[rGs.index(4711)])\n",
    "        link_leave_1pct_rGs = pd.concat([link_leave_1pct_rGs, temp])\n",
    "    elif (seed == 3254):\n",
    "        path =\"/home/lola/math_cluster/output/output-lausitz-1pct-1-fCf_sCF_0.01_gS_3254_3765/lausitz-1pct-1-fCf_sCf_0.01_gS_3254_3765.output_events.xml.gz\"\n",
    "        temp = countLinkLeaveEvents(path, \"1-pct\", 1, 1.0, global_seed, 30.0)\n",
    "        link_leave_1pct_rGs = pd.concat([link_leave_1pct_rGs, temp], axis = 0)\n",
    "    else:\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-1.0-pct-1-fCf_sCF_0.01_gS_\" +str(seed) + \"_3765/lausitz-1.0-pct-1-fCf_sCF_0.01_gS_\" + str(seed) + \"_3765.output_events.xml.gz\"\n",
    "        temp = countLinkLeaveEvents(path, \"1-pct\", 1, alpha, global_seed, 30.0)\n",
    "        link_leave_1pct_rGs = pd.concat([link_leave_1pct_rGs, temp], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_leave_events_1pct = pd.concat([link_leave_events_1pct,link_leave_1pct_rGs], axis = 0, ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write 1 pct to csv\n",
    "link_leave_events_1pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/link_leave_1pct_all.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.05\"]\n",
    "storCapF =  [\"0.05\", \"0.10574\"]\n",
    "\n",
    "link_leave_events_5pct = pd.DataFrame()\n",
    "\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,11,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "\n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.05\") & (sCf == \"0.05\")):\n",
    "                    alpha = 1.0\n",
    "                    if (sampleNr == 6):\n",
    "                        path_case1  = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765/lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765-2.output_events.xml.gz\"\n",
    "                    \n",
    "                    else: \n",
    "                        path_case1 = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_3765/lausitz-5.0-pct-\"+str(sampleNr)+ \"-fCf_sCF_0.05_gS_4711_3765.output_events.xml.gz\"\n",
    "                    \n",
    "                    \n",
    "                    path_case3 =  \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-5-pct-\" + str(sampleNr) +\"-fCf_sCF_0.05_gS_4711_sT_600.0_3765.output_events.xml.gz\"\n",
    "\n",
    "                    temp_case1 = countLinkLeaveEvents(path_case1, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case3 = countLinkLeaveEvents(path_case3, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    link_leave_events_5pct = pd.concat([link_leave_events_5pct, temp_case1, temp_case3], ignore_index= True)\n",
    "                    \n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    if(sampleNr == 6):\n",
    "                        path_case2 = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_0.05_sCF_0.10574_gS_4711_3765/lausitz-5.0-pct-6-fCf_0.05_sCF_0.10574_gS_4711_3765-2.output_events.xml.gz\"\n",
    "                    else:\n",
    "                        path_case2 = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf  + \"_gS_4711_3765/lausitz-5.0-pct-\"+str(sampleNr)+ \"-fCf_0.05_sCF_0.10574_gS_4711_3765.output_events.xml.gz\"\n",
    "                    \n",
    "                    \n",
    "                    path_case4 = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str (sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-5-pct-\" + str(sampleNr) + \"-fCf_0.05_sCF_0.10574_gS_4711_sT_600.0_3765.output_events.xml.gz\"\n",
    "\n",
    "                    # calculate number of stuck time violations and attach them to the data frame\n",
    "                    temp_case2 = countLinkLeaveEvents(path_case2, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case4 = countLinkLeaveEvents(path_case4, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    link_leave_events_5pct = pd.concat([link_leave_events_5pct, temp_case2, temp_case4], ignore_index= True)\n",
    "                \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 pct random global seed, alpha = 1\n",
    "link_leave_5pct_rGs = pd.DataFrame()\n",
    "rGs = [4711, 3254, 2306, 6384,4338, 6003, 5502, 9377, 5621, 9002 ]\n",
    "\n",
    "for seed in rGs:\n",
    "    if (seed ==4711):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        link_leave_events = link_leave_events_5pct[(link_leave_events_5pct['alpha'] == 1.0) & (link_leave_events_5pct['stuck_time'] == 30.0) & (link_leave_events_5pct['sample_nr'] == 1)]['n_link_leave_events']\n",
    "        temp = {'n_link_leave': link_leave_events, 'sample_size': '5-pct', 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': global_seed  }\n",
    "        temp = pd.DataFrame(data=temp, index=[rGs.index(4711)])\n",
    "        link_leave_5pct_rGs = pd.concat([link_leave_5pct_rGs, temp], ignore_index = True)\n",
    "    else:\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-1-fCf_sCF_0.05_gS_\" + str(seed) + \"_3765/lausitz-5.0-pct-1-fCf_sCF_0.05_gS_\"+ str(seed) +\"_3765.output_events.xml.gz\"\n",
    "        df = countLinkLeaveEvents(path, \"5-pct\", 1, 1.0, \"rnd_\" + str(seed), 30.0)\n",
    "\n",
    "        link_leave_5pct_rGs = pd.concat([link_leave_5pct_rGs, df], ignore_index = True,  axis = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_leave_5pct_all = pd.concat([link_leave_events_5pct, link_leave_5pct_rGs], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_leave_5pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/link_leave_5pct_all.csv', index = False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.1\"]\n",
    "storCapF =  [\"0.1\", \"0.17783\"]\n",
    "\n",
    "link_leave_events_10pct = pd.DataFrame()\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,11,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.1\") & (sCf == \"0.1\")):\n",
    "                    alpha = 1.0\n",
    "                    path_case1 = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_3765/lausitz-10.0-pct-\" + str(sampleNr) + \"-fCf_sCF_0.1_gS_4711_3765.output_events.xml.gz\"\n",
    "                    path_case3 = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-10-pct-\" + str(sampleNr) + \"-fCf_sCF_0.1_gS_4711_sT_300.0_3765.output_events.xml.gz\"\n",
    "                    \n",
    "                    temp_case1 = countLinkLeaveEvents(path_case1, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case3 = countLinkLeaveEvents(path_case3, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    link_leave_events_10pct = pd.concat([link_leave_events_10pct, temp_case1, temp_case3], ignore_index= True)     \n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    path_case2 = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\"+ str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf +\"_gS_4711_3765/lausitz-10.0-pct-\" + str(sampleNr) + \"-fCf_0.1_sCF_0.17783_gS_4711_3765.output_events.xml.gz\"\n",
    "                    path_case4 =\"/home/lola/math_cluster/output/output-lausitz-10-pct-\" + str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-10-pct-\" + str(sampleNr) + \"-fCf_0.1_sCF_0.17783_gS_4711_sT_300.0_3765.output_events.xml.gz\"\n",
    "\n",
    "                    # calculate number of stuck time violations and attach them to the data frame\n",
    "                    temp_case2 = countLinkLeaveEvents(path_case2, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case4 = countLinkLeaveEvents(path_case4, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    link_leave_events_10pct = pd.concat([link_leave_events_10pct, temp_case2, temp_case4], ignore_index= True)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write csv\n",
    "link_leave_events_10pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/link_leave_events_10pct.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25 pct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.25\"]\n",
    "storCapF =  [\"0.25\", \"0.35355\"]\n",
    "\n",
    "link_leave_events_25pct = pd.DataFrame()\n",
    "counter = 0\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,2,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "                \n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.25\") & (sCf == \"0.25\")):\n",
    "                    alpha = 1.0\n",
    "                    path_case1 = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_sCF_0.25_gS_4711_3765/lausitz-25.0-pct-fCf_sCF_0.25_gS_4711_3765.output_events.xml.gz\"\n",
    "                    path_case3 = \"/home/lola/math_cluster/output/output-lausitz-25-pct-1-fCf_sCF_0.25_gS_4711_sT_120.0_3765/lausitz-25-pct-1-fCf_sCF_0.25_gS_4711_sT_120.0_3765.output_events.xml.gz\"\n",
    "\n",
    "                    temp_case1 = countLinkLeaveEvents(path_case1, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case3 = countLinkLeaveEvents(path_case3, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    link_leave_events_25pct = pd.concat([link_leave_events_25pct, temp_case1, temp_case3], ignore_index= True)  \n",
    "\n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    path_case2 = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_0.25_sCF_0.35355_gS_4711_3765/lausitz-25.0-pct-fCf_0.25_sCF_0.35355_gS_4711_3765.output_events.xml.gz\"\n",
    "                    path_case4 = \"/home/lola/math_cluster/output/output-lausitz-25-pct-1-fCf_0.25_sCF_0.35355_gS_4711_sT_120.0_3765/lausitz-25-pct-1-fCf_0.25_sCF_0.35355_gS_4711_sT_120.0_3765.output_events.xml.gz\"\n",
    "\n",
    "                    # calculate number of stuck time violations and attach them to the data frame\n",
    "                    temp_case2 = countLinkLeaveEvents(path_case2, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case4 = countLinkLeaveEvents(path_case4, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    link_leave_events_25pct = pd.concat([link_leave_events_25pct, temp_case2, temp_case4], ignore_index= True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write csv\n",
    "link_leave_events_25pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/link_leave_events_25pct.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.5\"]\n",
    "storCapF =  [\"0.5\", \"0.5946\"]\n",
    "\n",
    "link_leave_events_50pct = pd.DataFrame()\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,2,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "\n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.5\") & (sCf == \"0.5\")):\n",
    "                    alpha = 1.0\n",
    "                    path_case1 = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_sCF_0.5_gS_4711_3765/lausitz-50.0-pct-fCf_sCF_0.5_gS_4711_3765.output_events.xml.gz\"\n",
    "                    path_case3 = \"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_sCF_0.5_gS_4711_sT_60.0_3765/lausitz-50-pct-1-fCf_sCF_0.5_gS_4711_sT_60.0_3765.output_events.xml.gz\"\n",
    "                    \n",
    "                    # calculate number of stuck time violations and attach them to the data frame\n",
    "                    temp_case1 = countLinkLeaveEvents(path_case1, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case3 = countLinkLeaveEvents(path_case3, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    link_leave_events_50pct = pd.concat([link_leave_events_50pct, temp_case1, temp_case3], ignore_index= True)     \n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    path_case2 = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_0.5_sCF_0.5946_gS_4711_3765/lausitz-50.0-pct-fCf_0.5_sCF_0.5946_gS_4711_3765.output_events.xml.gz\"\n",
    "                    path_case4 =\"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_0.5_sCF_0.5946_gS_4711_sT_60.0_3765/lausitz-50-pct-1-fCf_0.5_sCF_0.5946_gS_4711_sT_60.0_3765.output_events.xml.gz\"\n",
    "                    \n",
    "                    # calculate number of stuck time violations and attach them to the data frame\n",
    "                    temp_case2 = countLinkLeaveEvents(path_case2, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case4 = countLinkLeaveEvents(path_case4, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    link_leave_events_50pct = pd.concat([link_leave_events_50pct, temp_case2, temp_case4], ignore_index= True)\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write csv\n",
    "link_leave_events_50pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/link_leave_events_50pct.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765/lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765.output_events.xml.gz\"\n",
    "link_leave_events_100pct = countLinkLeaveEvents(path,  \"100-pct\", 1, 1.0, \"default\", 30.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write csv\n",
    "link_leave_events_100pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/link_leave_events_100pct.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_leave_1_100 = pd.concat([link_leave_events_1pct, link_leave_5pct_all, link_leave_events_10pct, link_leave_events_25pct, link_leave_events_50pct, link_leave_events_100pct], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write csv\n",
    "link_leave_1_100.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/link_leave_1_100pct.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate the number of link leave events per hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = ['1', '5', '10', '25', '50', '100']\n",
    "alpha_values = ['1.0', '0.75']\n",
    "sT_default = 30.0\n",
    "\n",
    "link_leave_per_hour = pd.DataFrame()\n",
    "base_path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs/Link_leave_hour/\"\n",
    "for sampleSize in sample_sizes:\n",
    "    # calculate adjusted stuck time\n",
    "    size_in_pct = float(sampleSize)/100.0\n",
    "    size_inverted = 1/ size_in_pct\n",
    "    sT_adjusted = 30.0*size_inverted\n",
    "    if (sampleSize == '100'):\n",
    "        stuck_times = [sT_default]\n",
    "    else: \n",
    "        stuck_times = [sT_default, sT_adjusted]\n",
    "    \n",
    "    # declare sampleSize + \"-pct\"\n",
    "    sample_size_string = sampleSize + \"-pct\"\n",
    "    factor_scale_to_100 = 100.0 / float(sampleSize)\n",
    "\n",
    "    for sT in stuck_times:\n",
    "        for alpha in alpha_values: \n",
    "            for sampleNr in range(1,11,1):\n",
    "                if((int(sampleSize) > 10) & (sampleNr > 1)):\n",
    "                    continue\n",
    "                if((sampleSize == '100') & (alpha == '0.75')):\n",
    "                    continue\n",
    "                df_link_leave = pd.read_csv(base_path + f\"link_leave_hour_{sample_size_string}_sample_nr_{str(sampleNr)}_alpha_{alpha}_gS_default_sT_{str(sT)}.csv\")\n",
    "                df_link_leave['hour'] = np.floor(df_link_leave['time'] / 3600)\n",
    "                for hour in range(0,36,1):\n",
    "                \n",
    "                    temp = df_link_leave[(df_link_leave['hour'] == hour)].shape[0]*factor_scale_to_100\n",
    "\n",
    "                    if (temp > 0):\n",
    "                        df = pd.DataFrame({'sample_size': sample_size_string, 'alpha': float(alpha), 'hour': hour, 'stuck_time': sT, 'n_Link_Leave': temp}, index = [0])\n",
    "                        link_leave_per_hour = pd.concat([link_leave_per_hour, df], ignore_index= True)\n",
    "                    else:\n",
    "                        df = pd.DataFrame({'sample_size': sample_size_string, 'alpha': float(alpha), 'hour': hour, 'stuck_time': sT, 'n_Link_Leave': temp}, index = [0])\n",
    "                        link_leave_per_hour = pd.concat([link_leave_per_hour, df], ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_leave_per_hour.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/link_leave_1_100pct_per_hour.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Determine average speed per road type and hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def networkInfoToDf(pathToNetwork):\n",
    "    input = gzip.open(pathToNetwork, 'r')\n",
    "    tree = ET.parse(input)\n",
    "    root = tree.getroot()\n",
    "    # convert network to data frame \n",
    "    ids = []\n",
    "    length = []\n",
    "    freespeed = []\n",
    "    capacity = []\n",
    "    type_of_link = []\n",
    "\n",
    "    for links in root.findall('links'):\n",
    "        for link in links:\n",
    "            #print(link.tag, link.attrib)\n",
    "            ids.append(link.attrib['id'])\n",
    "            length.append(float(link.attrib['length']))\n",
    "            freespeed.append(float(link.attrib['freespeed'])*3.6)\n",
    "            capacity.append(float(link.attrib['capacity']))\n",
    "            type_counter = 0\n",
    "            for child in link:\n",
    "                for attr in child:\n",
    "                    if (attr.attrib['name'] == \"type\"):\n",
    "                        try:\n",
    "                            type_of_link.append(attr.text)\n",
    "                            type_counter = 1\n",
    "                        except:\n",
    "                            type_of_link.append('NA')\n",
    "            if(type_counter == 0):\n",
    "                type_of_link.append('NA')\n",
    "    network_df = pd.DataFrame({'ID': ids, 'length': length, 'freespeed': freespeed, 'capacity': capacity, 'type': type_of_link })\n",
    "    return network_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def departuresTable(pathToEvents):\n",
    "    dep_events = matsim.event_reader(pathToEvents, types= \"departure\")\n",
    "\n",
    "    time_dep = []\n",
    "    id_dep= []\n",
    "    link_dep = []\n",
    "    legMode_dep = []\n",
    "\n",
    "    for event in dep_events:\n",
    "        if event['type'] == \"departure\":\n",
    "            time_dep.append(event['time'])\n",
    "            id_dep.append(event['person'])\n",
    "            link_dep.append(event['link'])\n",
    "            legMode_dep.append(event['legMode'])\n",
    "\n",
    "    departures = pd.DataFrame({'ID': id_dep, 'time_dep': time_dep, 'link_dep': link_dep, 'legMode': legMode_dep})\n",
    "    departures = departures[(departures['ID'].str.contains('pt_') == False)]\n",
    "    departures = departures[(departures['legMode'].str.contains('car') == True)]\n",
    "    return departures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrivalsTable(pathToEvents):\n",
    "    arr_events = matsim.event_reader(pathToEvents, types= \"arrival\")\n",
    "\n",
    "    time_arr = []\n",
    "    id_arr = []\n",
    "    link_arr = []\n",
    "    type_arr = []\n",
    "    legMode_arr = []\n",
    "\n",
    "    for event in arr_events:\n",
    "        if event['type'] == \"arrival\":\n",
    "            time_arr.append(event['time'])\n",
    "            link_arr.append(event['link'])\n",
    "            type_arr.append(event['type'])\n",
    "            id_arr.append(event['person'])\n",
    "            legMode_arr.append(event['legMode'])\n",
    "    arrivals = pd.DataFrame({'ID': id_arr, 'time_arr': time_arr, 'link_arr': link_arr, 'legMode': legMode_arr})\n",
    "    arrivals = arrivals[(arrivals['ID'].str.contains('pt_') == False)]\n",
    "    arrivals = arrivals[(arrivals['legMode'].str.contains('car') == True)]\n",
    "    return arrivals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcTravelTime(departures, arrivals):\n",
    "    dep_storage = []\n",
    "    arr_storage = []\n",
    "    person_storage = []\n",
    "    trip_number = []\n",
    "    depLink_storage = []\n",
    "    arrLink_storage = []\n",
    "    trip_id_storage = []\n",
    "\n",
    "    unique_person = departures['ID'].unique()\n",
    "    for person in unique_person:\n",
    "        temp_depLink = np.array(departures[departures['ID'] == person]['link_dep'])\n",
    "        temp_arrLink = np.array(arrivals[arrivals['ID'] == person]['link_arr'])\n",
    "        temp_dep = np.array(departures[departures['ID'] == person]['time_dep'])\n",
    "        temp_arr = np.array(arrivals[arrivals['ID'] == person]['time_arr'])\n",
    "        # only do this when the person has departures\n",
    "        if ((len(temp_dep) > 0) & (len(temp_arr) > 0)):\n",
    "            for element in range(0, len(temp_dep),1):\n",
    "                trip_id_storage.append(str(person) +\"_\" +str(element +1))\n",
    "                person_storage.append(person)\n",
    "                trip_number.append(element +1)\n",
    "                dep_storage.append(temp_dep[element])\n",
    "                arr_storage.append(temp_arr[element])\n",
    "                depLink_storage.append(temp_depLink[element])\n",
    "                arrLink_storage.append(temp_arrLink[element])\n",
    "    df_dep_arr = pd.DataFrame({'person_id': person_storage,'trip_number': trip_number, 'trip_id': trip_id_storage,'dep_time': dep_storage, 'dep_link': depLink_storage, 'arr_time': arr_storage, 'arr_link': arrLink_storage})\n",
    "    # calculate travel time here?\n",
    "    return df_dep_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enterEventsTable(pathToEvents, df_net):\n",
    "    enter_events = matsim.event_reader(pathToEvents, types= \"entered link\")\n",
    "\n",
    "    time_entered = []\n",
    "    vehicle_entered = []\n",
    "    link_entered = []\n",
    "    type_entered = []\n",
    "\n",
    "    for event in enter_events:\n",
    "        if event[\"type\"] == \"entered link\":\n",
    "            time_entered.append(event['time'])\n",
    "            vehicle_entered.append(event['vehicle'])\n",
    "            link_entered.append(event['link'])\n",
    "            type_entered.append(event['type'])\n",
    "            \n",
    "    df_linkEnter = pd.DataFrame({'enter_time' : time_entered, 'link_id': link_entered, 'vehicle_id': vehicle_entered, 'type_of_event': type_entered})\n",
    "    df_enter_no_pt = df_linkEnter[(df_linkEnter['link_id'].str.contains('pt_') == False)].copy()\n",
    "    df_enter_no_pt = pd.merge(df_enter_no_pt, df_net[['ID', 'type']], how='left', left_on='link_id', right_on='ID')\n",
    "    return df_enter_no_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaveEventsTable(pathToEvents, df_net):\n",
    "    leave_events = matsim.event_reader(pathToEvents, types= \"left link\")\n",
    "\n",
    "    time_left = []\n",
    "    vehicle_left = []\n",
    "    link_left = []\n",
    "    type_left = []\n",
    "\n",
    "    for event in leave_events: \n",
    "        if event[\"type\"] == \"left link\":\n",
    "            time_left.append(event['time'])\n",
    "            vehicle_left.append(event['vehicle'])\n",
    "            link_left.append(event['link'])\n",
    "            type_left.append(event['type'])\n",
    "\n",
    "\n",
    "    df_linkLeft = pd.DataFrame({'leave_time' : time_left, 'link_id': link_left, 'vehicle_id': vehicle_left, 'type_of_event': type_left})\n",
    "    df_leave_no_pt = df_linkLeft[(df_linkLeft['link_id'].str.contains('pt_') == False )]\n",
    "    df_leave_no_pt = pd.merge(df_leave_no_pt, df_net[['ID', 'type']], how='left', left_on='link_id', right_on='ID')\n",
    "    return df_leave_no_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractCarRouteFromPlans(pathToPlans):\n",
    "\n",
    "\n",
    "    input = gzip.open(pathToPlans, 'r')\n",
    "    tree = ET.parse(input)\n",
    "    root = tree.getroot()\n",
    "    # convert network to data frame \n",
    "\n",
    "\n",
    "    person_id = []\n",
    "    vehicle_id = []\n",
    "    route_storage =[]\n",
    "    trip_id_storage = []\n",
    "    start_link = []\n",
    "    end_link = []\n",
    "    first_route = []\n",
    "    last_route = []\n",
    "\n",
    "#iterate over a persons in the experienced plans file \n",
    "    for person in root.findall('person'):\n",
    "        trip_counter = 0\n",
    "        # there is only the selected plan in the experienced plans file, so it is enough to only find all plans which are children of a person\n",
    "        for plan in person.findall('plan'):\n",
    "            if(plan.attrib['selected'] != 'yes'):\n",
    "                continue\n",
    "            else:     \n",
    "                # find all legs\n",
    "                for leg in plan.findall('leg'):\n",
    "                    #print(leg.attrib['mode'])\n",
    "                    # only for car legs\n",
    "                    if (leg.attrib['mode'] != \"car\"):\n",
    "                        continue\n",
    "                    elif(leg.attrib['mode'] == \"car\"):\n",
    "                        trip_counter += 1\n",
    "                        # find the route\n",
    "                        for route in leg.findall('route'):              \n",
    "                            # get all links of the route:\n",
    "                            temp_route = route.text.split()\n",
    "                            if (len(temp_route) == 0):\n",
    "                                print(\"route of length 0 \")\n",
    "                                continue\n",
    "                            else: \n",
    "                                if((route.attrib['start_link'] == temp_route[0]) & ((route.attrib['end_link'] == temp_route[len(temp_route)-1])) ):\n",
    "                                    for element in temp_route[1:len(temp_route)-2]:\n",
    "                                        person_id.append(person.attrib['id'])\n",
    "                                        route_storage.append(element)\n",
    "                                        trip_id_storage.append(person.attrib['id'] + \"_\" + str(trip_counter))\n",
    "                                        vehicle_id.append(person.attrib['id'] + \"_car\")\n",
    "                                        start_link.append(temp_route[0])\n",
    "                                        end_link.append(temp_route[len(temp_route)-1])\n",
    "                                        first_route.append(temp_route[1])\n",
    "                                        last_route.append(temp_route[len(temp_route)-2])\n",
    "\n",
    "                                elif((route.attrib['start_link'] == temp_route[0]) & ((route.attrib['end_link'] != temp_route[len(temp_route)-1])) ):\n",
    "                                    for element in temp_route[1:len(temp_route)-1]:\n",
    "                                        person_id.append(person.attrib['id'])\n",
    "                                        route_storage.append(element)\n",
    "                                        trip_id_storage.append(person.attrib['id'] + \"_\" + str(trip_counter))\n",
    "                                        vehicle_id.append(person.attrib['id'] + \"_car\")\n",
    "                                        start_link.append(temp_route[0])\n",
    "                                        end_link.append(route.attrib['end_link'])\n",
    "                                        first_route.append(temp_route[1])\n",
    "                                        last_route.append(temp_route[len(temp_route)-1])\n",
    "\n",
    "                                elif((route.attrib['start_link'] != temp_route[0]) & ((route.attrib['end_link'] == temp_route[len(temp_route)-1])) ):\n",
    "                                    for element in temp_route[0:len(temp_route)-2]:\n",
    "                                        person_id.append(person.attrib['id'])\n",
    "                                        route_storage.append(element)\n",
    "                                        trip_id_storage.append(person.attrib['id'] + \"_\" + str(trip_counter))\n",
    "                                        vehicle_id.append(person.attrib['id'] + \"_car\")\n",
    "                                        start_link.append(route.attrib['start_link'])\n",
    "                                        end_link.append(temp_route[len(temp_route)-1])\n",
    "                                        first_route.append(temp_route[0])\n",
    "                                        last_route.append(temp_route[len(temp_route)-2])\n",
    "\n",
    "                                else: \n",
    "                                    for element in temp_route:\n",
    "                                        person_id.append(person.attrib['id'])\n",
    "                                        route_storage.append(element)\n",
    "                                        trip_id_storage.append(person.attrib['id'] + \"_\" + str(trip_counter))\n",
    "                                        vehicle_id.append(person.attrib['id'] + \"_car\")\n",
    "                                        start_link.append(route.attrib['start_link'])\n",
    "                                        end_link.append(route.attrib['end_link'])\n",
    "                                        first_route.append(temp_route[0])\n",
    "                                        last_route.append(temp_route[len(temp_route)-1])\n",
    "    df_Person_Link = pd.DataFrame({'person': person_id, 'trip_id':trip_id_storage, 'vehicle_id': vehicle_id, 'link_id':route_storage, 'start_link': start_link, 'end_link': end_link,})\n",
    "    return df_Person_Link\n",
    "                                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregateResultByRoadTypeAndHour(result):\n",
    "    result['time_on_link'] = result['time_link_left'] - result['time_link_entered']\n",
    "    result['m_per_s'] = result['length'] / result['time_on_link']\n",
    "    result['hour_link_entered'] = np.floor(result['time_link_entered'] / 3600)\n",
    "    result = result.sort_values(by=['type', 'hour_link_entered'])\n",
    "    hour_storage = []\n",
    "    type_storage = []\n",
    "    speed_storage = []\n",
    "\n",
    "    for roadType in result['type'].unique():\n",
    "        for hour in result[(result['type']== roadType)]['hour_link_entered'].unique():\n",
    "            hour_storage.append(hour)\n",
    "            type_storage.append(roadType)\n",
    "            speed_storage.append(np.mean(result[(result['type']== roadType) & (result['hour_link_entered']== hour)]['m_per_s'])*3.6)\n",
    "    res_aggr = pd.DataFrame({'type': type_storage, 'hour': hour_storage, 'speed': speed_storage})\n",
    "    return res_aggr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leftJoinEnterAndLeaveToRoute(df_enterEvents, df_leaveEvents, df_PersonAndRoute):\n",
    "\n",
    "    # from gpt \n",
    "    duckdb_conn = duckdb.connect()\n",
    "\n",
    "    # from gpt \n",
    "    duckdb_conn.register(\"df_PersonAndRoute\", df_PersonAndRoute) \n",
    "    #duckdb_conn.execute(\"CREATE TABLE PersonLink AS SELECT * FROM df_PersonAndRoute\")\n",
    "\n",
    "    # from gpt \n",
    "    duckdb_conn.register(\"df_enterEvents\", df_enterEvents) \n",
    "    #duckdb_conn.execute(\"CREATE TABLE LinkEnter AS SELECT * FROM df_enterEvents\")\n",
    "\n",
    "    duckdb_conn.register(\"df_leaveEvents\", df_leaveEvents) \n",
    "    #duckdb_conn.execute(\"CREATE TABLE LinkLeave AS SELECT * FROM df_leaveEvents\")\n",
    "\n",
    "    # query from gpt \n",
    "    result= duckdb_conn.query(\"\"\"SELECT \n",
    "        m.person,\n",
    "        m.vehicle_id,\n",
    "        m.trip_id,\n",
    "        m.link_id,\n",
    "        m.dep_link,\n",
    "        m.dep_time,\n",
    "        m.arr_link,\n",
    "        m.arr_time,\n",
    "        m.type,\n",
    "        m.length,\n",
    "        s1.enter_time AS time_link_entered,\n",
    "        t2.leave_time AS time_link_left,\n",
    "    FROM df_PersonAndRoute m\n",
    "    LEFT JOIN df_enterEvents s1 \n",
    "        ON m.link_id = s1.link_id\n",
    "        AND m.vehicle_id = s1.vehicle_id\n",
    "        AND s1.enter_time BETWEEN m.dep_time AND m.arr_time                            \n",
    "    LEFT JOIN df_leaveEvents t2 \n",
    "        ON m.link_id = t2.link_id\n",
    "        AND m.vehicle_id = t2.vehicle_id\n",
    "        AND t2.leave_time BETWEEN m.dep_time AND m.arr_time;\n",
    "                        \"\"\").to_df()\n",
    "    duckdb_conn.close()\n",
    "\n",
    "    aggr_result = aggregateResultByRoadTypeAndHour(result)\n",
    "\n",
    "\n",
    "    return aggr_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAvgSpeedPerRoadTypeAndHour(pathToCase, network_df):\n",
    "    pathToEvents = pathToCase +  \".output_events.xml.gz\"\n",
    "    pathToPlans = pathToCase + \".output_experienced_plans.xml.gz\"\n",
    "    # departures, arrivals and travel time\n",
    "    df_dep = departuresTable(pathToEvents)\n",
    "    df_arr = arrivalsTable(pathToEvents)\n",
    "    df_travTime = calcTravelTime(departures= df_dep, arrivals=  df_arr)\n",
    "    df_PersonRoute = extractCarRouteFromPlans(pathToPlans)\n",
    "\n",
    "    # left join travel time to the persons route\n",
    "    df_PersonRoute = pd.merge(df_PersonRoute, df_travTime[['trip_id', 'dep_time', 'dep_link', 'arr_time', 'arr_link']], on = \"trip_id\", how = \"left\")\n",
    "    # left join the net to the route\n",
    "    df_PersonRoute = pd.merge(df_PersonRoute, network_df[['ID', 'type', 'length']], how='left', left_on='link_id', right_on='ID')\n",
    "    # reduce to secondary, residential and tertiary\n",
    "    df_PersonRoute = df_PersonRoute[(df_PersonRoute['type'] == 'highway.secondary') | (df_PersonRoute['type'] == 'highway.residential') | (df_PersonRoute['type'] == 'highway.tertiary') ]\n",
    "    # drop column ID\n",
    "    df_PersonRoute = df_PersonRoute.drop(columns=['ID'])\n",
    "\n",
    "    # enter and leave events\n",
    "    df_enter = enterEventsTable(pathToEvents, network_df)\n",
    "    df_leave = leaveEventsTable(pathToEvents, network_df)\n",
    "\n",
    "    # calculate the avg speed per road type and hour\n",
    "    result = leftJoinEnterAndLeaveToRoute(df_enter, df_leave, df_PersonRoute)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToLausitzNetwork = '/net/ils/mersini/input/v2024.2/lausitz-v2024.2-network.xml.gz'\n",
    "df_net = networkInfoToDf(pathToLausitzNetwork)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.01\"]\n",
    "storCapF =  [\"0.01\", \"0.03162\"]\n",
    "\n",
    "avg_speed_per_RoadTypeAndHour_1pct = pd.DataFrame()\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "        for sampleNr in range(1,11,1):\n",
    "            # calculate adjusted stuck time\n",
    "            default_stuck_time = 30.0\n",
    "            adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "            # declare sample size as str \"1-pct\"\n",
    "            sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "\n",
    "            if ((fCf == \"0.01\") & (sCf == '0.01')):\n",
    "                alpha = 1.0\n",
    "                # paths for case 1 and 3 \n",
    "                path_case1 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" + str(sampleNr) + \"-fCf_sCF_\"+ sCf + \"_gS_default_3765/lausitz-1pct-\"+str(sampleNr)+ \"-fCf_sCf_0.01_gS_default_3765\"\n",
    "                path_case3 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-1-pct-\" + str(sampleNr) + \"-fCf_sCF_0.01_gS_4711_sT_3000.0_3765\"\n",
    "                \n",
    "                # calculate the average speed per road type and hour for the cases\n",
    "                temp_case1 = calcAvgSpeedPerRoadTypeAndHour(path_case1, df_net)\n",
    "                temp_case1.insert(3, 'sample_size', sample_size_as_string)\n",
    "                temp_case1.insert(4, 'alpha', alpha)\n",
    "                temp_case1.insert(5, 'stuck_time', default_stuck_time)\n",
    "                temp_case1.insert(6,'global_seed', \"default\")\n",
    "                temp_case1.insert(7,'sample_nr', sampleNr) \n",
    "\n",
    "\n",
    "                temp_case3 = calcAvgSpeedPerRoadTypeAndHour(path_case3, df_net)\n",
    "                temp_case3.insert(3, 'sample_size', sample_size_as_string)\n",
    "                temp_case3.insert(4, 'alpha', alpha)\n",
    "                temp_case3.insert(5, 'stuck_time', adjusted_stuck_time)\n",
    "                temp_case3.insert(6,'global_seed', \"default\")\n",
    "                temp_case3.insert(7,'sample_nr', sampleNr)\n",
    "\n",
    "\n",
    "                avg_speed_per_RoadTypeAndHour_1pct = pd.concat([avg_speed_per_RoadTypeAndHour_1pct, temp_case1, temp_case3], ignore_index= True)\n",
    "\n",
    "            else:\n",
    "                alpha = 0.75\n",
    "                # paths for case 2 and 4 \n",
    "                path_case2 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" + str(sampleNr) + \"-fCf_0.01_sCF_\" + sCf + \"_gS_default_3765/lausitz-1pct-\" +str(sampleNr)+ \"-fCf_0.01_sCf_0.03162_gS_default_3765\"\n",
    "                path_case4 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" + str(sampleNr) + \"-fCf_0.01_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-1-pct-\" + str(sampleNr) +\"-fCf_0.01_sCF_0.03162_gS_4711_sT_3000.0_3765\"\n",
    "                \n",
    "                # calculate the average speed per road type and hour for the cases\n",
    "                temp_case2 = calcAvgSpeedPerRoadTypeAndHour(path_case2, df_net)\n",
    "                temp_case2.insert(3, 'sample_size', sample_size_as_string)\n",
    "                temp_case2.insert(4, 'alpha', alpha)\n",
    "                temp_case2.insert(5, 'stuck_time', default_stuck_time)\n",
    "                temp_case2.insert(6,'global_seed', \"default\")\n",
    "                temp_case2.insert(7,'sample_nr', sampleNr)\n",
    "\n",
    "\n",
    "                temp_case4 = calcAvgSpeedPerRoadTypeAndHour(path_case4, df_net)\n",
    "                temp_case4.insert(3, 'sample_size', sample_size_as_string)\n",
    "                temp_case4.insert(4, 'alpha', alpha)\n",
    "                temp_case4.insert(5, 'stuck_time', adjusted_stuck_time)\n",
    "                temp_case4.insert(6,'global_seed', \"default\")\n",
    "                temp_case4.insert(7,'sample_nr', sampleNr)\n",
    "\n",
    "                avg_speed_per_RoadTypeAndHour_1pct = pd.concat([avg_speed_per_RoadTypeAndHour_1pct, temp_case2, temp_case4], ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_speed_per_RoadTypeAndHour_1pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/Avg_Speed_per_RoadTypeAndHour_1pct.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.05\"]\n",
    "storCapF =  [\"0.05\", \"0.10574\"]\n",
    "\n",
    "avg_speed_per_RoadTypeAndHour_5pct = pd.DataFrame()\n",
    "\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,11,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "\n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.05\") & (sCf == \"0.05\")):\n",
    "                    alpha = 1.0\n",
    "                    if (sampleNr == 6):\n",
    "                        path_case1  = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765/lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765-2\"\n",
    "                    \n",
    "                    else: \n",
    "                        path_case1 = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_3765/lausitz-5.0-pct-\"+str(sampleNr)+ \"-fCf_sCF_0.05_gS_4711_3765\"\n",
    "                    \n",
    "                    \n",
    "                    path_case3 =  \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-5-pct-\" + str(sampleNr) +\"-fCf_sCF_0.05_gS_4711_sT_600.0_3765\"\n",
    "\n",
    "                    # calculate the average speed per road type and hour for the cases\n",
    "                    temp_case1 = calcAvgSpeedPerRoadTypeAndHour(path_case1, df_net)\n",
    "                    temp_case1.insert(3, 'sample_size', sample_size_as_string)\n",
    "                    temp_case1.insert(4, 'alpha', alpha)\n",
    "                    temp_case1.insert(5, 'stuck_time', default_stuck_time)\n",
    "                    temp_case1.insert(6,'global_seed', \"default\")\n",
    "                    temp_case1.insert(7,'sample_nr', sampleNr) \n",
    "\n",
    "\n",
    "                    temp_case3 = calcAvgSpeedPerRoadTypeAndHour(path_case3, df_net)\n",
    "                    temp_case3.insert(3, 'sample_size', sample_size_as_string)\n",
    "                    temp_case3.insert(4, 'alpha', alpha)\n",
    "                    temp_case3.insert(5, 'stuck_time', adjusted_stuck_time)\n",
    "                    temp_case3.insert(6,'global_seed', \"default\")\n",
    "                    temp_case3.insert(7,'sample_nr', sampleNr)\n",
    "                    avg_speed_per_RoadTypeAndHour_5pct = pd.concat([avg_speed_per_RoadTypeAndHour_5pct, temp_case1, temp_case3], ignore_index= True)\n",
    "                    \n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    if(sampleNr == 6):\n",
    "                        path_case2 = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_0.05_sCF_0.10574_gS_4711_3765/lausitz-5.0-pct-6-fCf_0.05_sCF_0.10574_gS_4711_3765-2\"\n",
    "                    else:\n",
    "                        path_case2 = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf  + \"_gS_4711_3765/lausitz-5.0-pct-\"+str(sampleNr)+ \"-fCf_0.05_sCF_0.10574_gS_4711_3765\"\n",
    "                    \n",
    "                    \n",
    "                    path_case4 = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str (sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-5-pct-\" + str(sampleNr) + \"-fCf_0.05_sCF_0.10574_gS_4711_sT_600.0_3765\"\n",
    "\n",
    "                    # calculate the average speed per road type and hour for the cases\n",
    "                    temp_case2 = calcAvgSpeedPerRoadTypeAndHour(path_case2, df_net)\n",
    "                    temp_case2.insert(3, 'sample_size', sample_size_as_string)\n",
    "                    temp_case2.insert(4, 'alpha', alpha)\n",
    "                    temp_case2.insert(5, 'stuck_time', default_stuck_time)\n",
    "                    temp_case2.insert(6,'global_seed', \"default\")\n",
    "                    temp_case2.insert(7,'sample_nr', sampleNr)\n",
    "\n",
    "\n",
    "                    temp_case4 = calcAvgSpeedPerRoadTypeAndHour(path_case4, df_net)\n",
    "                    temp_case4.insert(3, 'sample_size', sample_size_as_string)\n",
    "                    temp_case4.insert(4, 'alpha', alpha)\n",
    "                    temp_case4.insert(5, 'stuck_time', adjusted_stuck_time)\n",
    "                    temp_case4.insert(6,'global_seed', \"default\")\n",
    "                    temp_case4.insert(7,'sample_nr', sampleNr)\n",
    "\n",
    "                    avg_speed_per_RoadTypeAndHour_5pct = pd.concat([avg_speed_per_RoadTypeAndHour_5pct, temp_case2, temp_case4], ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_speed_per_RoadTypeAndHour_5pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/Avg_Speed_per_RoadTypeAndHour_5pct.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgSpeedPerRoadTypeAndHour10pct():\n",
    "    flowCapF = [\"0.1\"]\n",
    "    storCapF =  [\"0.1\", \"0.17783\"]\n",
    "\n",
    "    avg_speed_per_RoadTypeAndHour_10pct = pd.DataFrame(df_net)\n",
    "    for fCf in flowCapF:\n",
    "        for sCf in storCapF:\n",
    "                for sampleNr in range(1,11,1):\n",
    "                    # calculate adjusted stuck time\n",
    "                    default_stuck_time = 30.0\n",
    "                    adj_sT = 30.0/float(flowCapF[0])\n",
    "                    # declare sample size as str \"1-pct\"\n",
    "                    sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "                    # declare path based on case \n",
    "                    if((fCf == \"0.1\") & (sCf == \"0.1\")):\n",
    "                        alpha = 1.0  \n",
    "                        path_case1 = \"/net/ils/mersini/output/output-lausitz-10.0-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_3765/output-lausitz-10.0-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_3765\"\n",
    "                        path_case3 = \"/net/ils/mersini/output/output-lausitz-10-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adj_sT) + \"_3765/output-lausitz-10-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adj_sT) + \"_3765\"\n",
    "\n",
    "                        # calculate the average speed per road type and hour for the cases\n",
    "                        temp_case1 = calcAvgSpeedPerRoadTypeAndHour(path_case1, df_net)\n",
    "                        temp_case1.insert(3, 'sample_size', sample_size_as_string)\n",
    "                        temp_case1.insert(4, 'alpha', alpha)\n",
    "                        temp_case1.insert(5, 'stuck_time', default_stuck_time)\n",
    "                        temp_case1.insert(6,'global_seed', \"default\")\n",
    "                        temp_case1.insert(7,'sample_nr', sampleNr) \n",
    "\n",
    "\n",
    "                        temp_case3 = calcAvgSpeedPerRoadTypeAndHour(path_case3, df_net)\n",
    "                        temp_case3.insert(3, 'sample_size', sample_size_as_string)\n",
    "                        temp_case3.insert(4, 'alpha', alpha)\n",
    "                        temp_case3.insert(5, 'stuck_time', adj_sT)\n",
    "                        temp_case3.insert(6,'global_seed', \"default\")\n",
    "                        temp_case3.insert(7,'sample_nr', sampleNr)\n",
    "                        avg_speed_per_RoadTypeAndHour_10pct = pd.concat([avg_speed_per_RoadTypeAndHour_10pct, temp_case1, temp_case3], ignore_index= True)\n",
    "\n",
    "                    elif((fCf == \"0.1\") & (sCf ==  \"0.17783\")):\n",
    "                        alpha = 0.75\n",
    "                        path_case2 = \"/net/ils/mersini/output/output-lausitz-10.0-pct-\" + str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_3765/output-lausitz-10.0-pct-\" + str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_3765\"\n",
    "                        path_case4 = \"/net/ils/mersini/output/output-lausitz-10-pct-\" + str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adj_sT) + \"_3765/output-lausitz-10-pct-\" + str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adj_sT) + \"_3765\"\n",
    "                        \n",
    "                        # calculate the average speed per road type and hour for the cases\n",
    "                        temp_case2 = calcAvgSpeedPerRoadTypeAndHour(path_case2, df_net)\n",
    "                        temp_case2.insert(3, 'sample_size', sample_size_as_string)\n",
    "                        temp_case2.insert(4, 'alpha', alpha)\n",
    "                        temp_case2.insert(5, 'stuck_time', default_stuck_time)\n",
    "                        temp_case2.insert(6,'global_seed', \"default\")\n",
    "                        temp_case2.insert(7,'sample_nr', sampleNr)\n",
    "\n",
    "\n",
    "                        temp_case4 = calcAvgSpeedPerRoadTypeAndHour(path_case4, df_net)\n",
    "                        temp_case4.insert(3, 'sample_size', sample_size_as_string)\n",
    "                        temp_case4.insert(4, 'alpha', alpha)\n",
    "                        temp_case4.insert(5, 'stuck_time', adj_sT)\n",
    "                        temp_case4.insert(6,'global_seed', \"default\")\n",
    "                        temp_case4.insert(7,'sample_nr', sampleNr)\n",
    "\n",
    "                        avg_speed_per_RoadTypeAndHour_10pct = pd.concat([avg_speed_per_RoadTypeAndHour_10pct, temp_case2, temp_case4], ignore_index= True)\n",
    "        avg_speed_per_RoadTypeAndHour_10pct.to_csv('/net/ils/mersini/output/Avg_Speed_per_RoadTypeAndHour_10pct.csv', index = False)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAvgSpeedPerHourAndRoadType25Pct(df_net):\n",
    "    flowCapF = [\"0.25\"]\n",
    "    storCapF =  [\"0.25\", \"0.35355\"]\n",
    "\n",
    "    avg_speed_per_RoadTypeAndHour_25pct = pd.DataFrame()\n",
    "\n",
    "    for fCf in flowCapF:\n",
    "        for sCf in storCapF:\n",
    "            for sampleNr in range(1,2,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adj_sT = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.25\") & (sCf == \"0.25\")):\n",
    "                    alpha = 1.0 \n",
    "                    path_case1 = \"/net/ils/mersini/output/output-lausitz-25.0-pct-fCf_sCF_\" + sCf + \"_gS_4711_3765/output-lausitz-25.0-pct-fCf_sCF_\" + sCf + \"_gS_4711_3765\"\n",
    "                    path_case3 = \"/net/ils/mersini/output/output-lausitz-25-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + fCf + \"_gS_4711_sT_\" + str(adj_sT) + \"_3765/output-lausitz-25-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + fCf + \"_gS_4711_sT_\" + str(adj_sT) + \"_3765\"\n",
    "\n",
    "                    # calculate the average speed per road type and hour for the cases\n",
    "                    temp_case1 = calcAvgSpeedPerRoadTypeAndHour(path_case1, df_net)\n",
    "                    temp_case1.insert(3, 'sample_size', sample_size_as_string)\n",
    "                    temp_case1.insert(4, 'alpha', alpha)\n",
    "                    temp_case1.insert(5, 'stuck_time', default_stuck_time)\n",
    "                    temp_case1.insert(6,'global_seed', \"default\")\n",
    "                    temp_case1.insert(7,'sample_nr', sampleNr) \n",
    "\n",
    "\n",
    "                    temp_case3 = calcAvgSpeedPerRoadTypeAndHour(path_case3, df_net)\n",
    "                    temp_case3.insert(3, 'sample_size', sample_size_as_string)\n",
    "                    temp_case3.insert(4, 'alpha', alpha)\n",
    "                    temp_case3.insert(5, 'stuck_time', adj_sT)\n",
    "                    temp_case3.insert(6,'global_seed', \"default\")\n",
    "                    temp_case3.insert(7,'sample_nr', sampleNr)\n",
    "                    avg_speed_per_RoadTypeAndHour_25pct = pd.concat([avg_speed_per_RoadTypeAndHour_25pct, temp_case1, temp_case3], ignore_index= True)\n",
    "                        \n",
    "                elif((fCf == \"0.25\") & (sCf ==  \"0.35355\")):\n",
    "                    alpha  = 0.75\n",
    "                    path_case2 = \"/net/ils/mersini/output/output-lausitz-25.0-pct-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_3765/output-lausitz-25.0-pct-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_3765\"\n",
    "                    path_case4 = \"/net/ils/mersini/output/output-lausitz-25-pct-\" + str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adj_sT) + \"_3765/output-lausitz-25-pct-\" + str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adj_sT) + \"_3765\"\n",
    "\n",
    "                    # calculate the average speed per road type and hour for the cases\n",
    "                    temp_case2 = calcAvgSpeedPerRoadTypeAndHour(path_case2, df_net)\n",
    "                    temp_case2.insert(3, 'sample_size', sample_size_as_string)\n",
    "                    temp_case2.insert(4, 'alpha', alpha)\n",
    "                    temp_case2.insert(5, 'stuck_time', default_stuck_time)\n",
    "                    temp_case2.insert(6,'global_seed', \"default\")\n",
    "                    temp_case2.insert(7,'sample_nr', sampleNr)\n",
    "\n",
    "\n",
    "                    temp_case4 = calcAvgSpeedPerRoadTypeAndHour(path_case4, df_net)\n",
    "                    temp_case4.insert(3, 'sample_size', sample_size_as_string)\n",
    "                    temp_case4.insert(4, 'alpha', alpha)\n",
    "                    temp_case4.insert(5, 'stuck_time', adj_sT)\n",
    "                    temp_case4.insert(6,'global_seed', \"default\")\n",
    "                    temp_case4.insert(7,'sample_nr', sampleNr)\n",
    "\n",
    "                    avg_speed_per_RoadTypeAndHour_25pct = pd.concat([avg_speed_per_RoadTypeAndHour_25pct, temp_case2, temp_case4], ignore_index= True)\n",
    "                    \n",
    "            avg_speed_per_RoadTypeAndHour_25pct.to_csv('/net/ils/mersini/output/Avg_Speed_per_RoadTypeAndHour_25pct.csv', index = False)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calvAvgSpeedPerRoadTypeANdHour50pct(df_net):\n",
    "    flowCapF = [\"0.5\"]\n",
    "    storCapF =  [\"0.5\", \"0.5946\"]\n",
    "\n",
    "    avg_speed_per_RoadTypeAndHour_50pct = pd.DataFrame()\n",
    "\n",
    "    for fCf in flowCapF:\n",
    "        for sCf in storCapF:\n",
    "            for sampleNr in range(1,2,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adj_sT = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.5\") & (sCf == \"0.5\")):\n",
    "                    alpha = 1.0\n",
    "                    path_case1 = \"/net/ils/mersini/output/output-lausitz-50.0-pct-fCf_sCF_\" + sCf + \"_gS_4711_3765/output-lausitz-50.0-pct-fCf_sCF_0.5_gS_4711_3765\"\n",
    "                    path_case3 = \"/net/ils/mersini/output/output-lausitz-50-pct-1-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" +str(adj_sT) + \"_3765/output-lausitz-50-pct-1-fCf_sCF_0.5_gS_4711_sT_60.0_3765\"\n",
    "\n",
    "                    # calculate the average speed per road type and hour for the cases\n",
    "                    temp_case1 = calcAvgSpeedPerRoadTypeAndHour(path_case1, df_net)\n",
    "                    temp_case1.insert(3, 'sample_size', sample_size_as_string)\n",
    "                    temp_case1.insert(4, 'alpha', alpha)\n",
    "                    temp_case1.insert(5, 'stuck_time', default_stuck_time)\n",
    "                    temp_case1.insert(6,'global_seed', \"default\")\n",
    "                    temp_case1.insert(7,'sample_nr', sampleNr) \n",
    "\n",
    "\n",
    "                    temp_case3 = calcAvgSpeedPerRoadTypeAndHour(path_case3, df_net)\n",
    "                    temp_case3.insert(3, 'sample_size', sample_size_as_string)\n",
    "                    temp_case3.insert(4, 'alpha', alpha)\n",
    "                    temp_case3.insert(5, 'stuck_time', adj_sT)\n",
    "                    temp_case3.insert(6,'global_seed', \"default\")\n",
    "                    temp_case3.insert(7,'sample_nr', sampleNr)\n",
    "                    avg_speed_per_RoadTypeAndHour_50pct = pd.concat([avg_speed_per_RoadTypeAndHour_50pct, temp_case1, temp_case3], ignore_index= True)\n",
    "                        \n",
    "                elif((fCf == \"0.5\") & (sCf ==  \"0.5946\") ):\n",
    "                    alpha = 0.75\n",
    "                    path_case2 = \"/net/ils/mersini/output/output-lausitz-50.0-pct-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_3765/output-lausitz-50.0-pct-fCf_0.5_sCF_0.5946_gS_4711_3765\"\n",
    "                    \n",
    "                    path_case4 = \"/net/ils/mersini/output/output-lausitz-50-pct-1-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adj_sT) + \"_3765/output-lausitz-50-pct-1-fCf_0.5_sCF_0.5946_gS_4711_sT_60.0_3765\"\n",
    "\n",
    "                    # calculate the average speed per road type and hour for the cases\n",
    "                    temp_case2 = calcAvgSpeedPerRoadTypeAndHour(path_case2, df_net)\n",
    "                    temp_case2.insert(3, 'sample_size', sample_size_as_string)\n",
    "                    temp_case2.insert(4, 'alpha', alpha)\n",
    "                    temp_case2.insert(5, 'stuck_time', default_stuck_time)\n",
    "                    temp_case2.insert(6,'global_seed', \"default\")\n",
    "                    temp_case2.insert(7,'sample_nr', sampleNr)\n",
    "\n",
    "\n",
    "                    temp_case4 = calcAvgSpeedPerRoadTypeAndHour(path_case4, df_net)\n",
    "                    temp_case4.insert(3, 'sample_size', sample_size_as_string)\n",
    "                    temp_case4.insert(4, 'alpha', alpha)\n",
    "                    temp_case4.insert(5, 'stuck_time', adj_sT)\n",
    "                    temp_case4.insert(6,'global_seed', \"default\")\n",
    "                    temp_case4.insert(7,'sample_nr', sampleNr)\n",
    "\n",
    "                    avg_speed_per_RoadTypeAndHour_50pct = pd.concat([avg_speed_per_RoadTypeAndHour_50pct, temp_case2, temp_case4], ignore_index= True)\n",
    "        avg_speed_per_RoadTypeAndHour_50pct.to_csv('/net/ils/mersini/output/Avg_Speed_per_RoadTypeAndHour_50pct.csv', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAvgSpeedPerRoadTypeAndHour(df_net):   \n",
    "    flowCapF = [\"1.0\"]\n",
    "    avg_speed_per_RoadTypeAndHour_100pct = pd.DataFrame()\n",
    "    for fCf in flowCapF:\n",
    "        for sampleNr in range(1,2,1):\n",
    "            alpha = 1.0\n",
    "            # calculate adjusted stuck time\n",
    "            default_stuck_time = 30.0\n",
    "            # declare sample size as str \"1-pct\"\n",
    "            sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "            # declare path based on case \n",
    "            path_case1 = \"/net/ils/mersini/output/output-lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765/output-lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765\"\n",
    "            # calculate the average speed per road type and hour for the cases\n",
    "            temp_case1 = calcAvgSpeedPerRoadTypeAndHour(path_case1, df_net)\n",
    "            temp_case1.insert(3, 'sample_size', sample_size_as_string)\n",
    "            temp_case1.insert(4, 'alpha', alpha)\n",
    "            temp_case1.insert(5, 'stuck_time', default_stuck_time)\n",
    "            temp_case1.insert(6,'global_seed', \"default\")\n",
    "            temp_case1.insert(7,'sample_nr', sampleNr)\n",
    "            avg_speed_per_RoadTypeAndHour_100pct = pd.concat([avg_speed_per_RoadTypeAndHour_100pct, temp_case1], ignore_index= True)\n",
    "            avg_speed_per_RoadTypeAndHour_100pct.to_csv('/net/ils/mersini/output/Avg_Speed_per_RoadTypeAndHour_100pct.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Executed Scores at iteration 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all necessary values from experienced plans\n",
    "pathToExecutedPlans = \"/home/lola/math_cluster/output/output-lausitz-1pct-1-fCf_sCF_0.01_gS_default_3765/lausitz-1pct-1-fCf_sCf_0.01_gS_default_3765.output_experienced_plans.xml.gz\"\n",
    "#def exectuedPlansToDf(pathToExecutedPlans):\n",
    "input = gzip.open(pathToExecutedPlans, 'r')\n",
    "tree = ET.parse(input)\n",
    "root = tree.getroot()\n",
    "\n",
    "\n",
    "id_sc_storage = []\n",
    "score_storage = []\n",
    "\n",
    "\n",
    "id_trip_storage = []\n",
    "trip_counter_storage = []\n",
    "trav_time_storage = []\n",
    "trav_dist_storage = []\n",
    "\n",
    "for person in root.findall('person'):\n",
    "    trip_counter = 0\n",
    "    for plan in person.findall('plan'):\n",
    "        if (plan.attrib['selected'] == \"yes\"):\n",
    "            id_sc_storage.append(person.attrib['id'])\n",
    "            score_storage.append(plan.attrib['score'])\n",
    "\n",
    "            for leg in plan.findall('leg'):\n",
    "                if(leg.attrib['mode'] == \"car\"):\n",
    "                    trip_counter += 1\n",
    "                    for route in leg.findall('route'):\n",
    "                        id_trip_storage.append(person.attrib['id'])\n",
    "                        trip_counter_storage.append(trip_counter)\n",
    "                        trav_time_storage.append(route.attrib['trav_time'])\n",
    "                        trav_dist_storage.append(route.attrib['distance'])\n",
    "                else:\n",
    "                    continue\n",
    "        else: \n",
    "            continue\n",
    "\n",
    "df_trips = pd.DataFrame({'person_id': id_trip_storage, 'trip_number': trip_counter_storage, 'trav_time': trav_time_storage, 'distance': trav_dist_storage })\n",
    "df_trips['trav_time_in_seconds'] = df_trips.trav_time.astype('timedelta64[s]')/  pd.Timedelta(seconds=1)\n",
    "\n",
    "avg_trav_time_min = np.mean(df_trips['trav_time_in_seconds'])/60\n",
    "\n",
    "df_score = pd.DataFrame({'person_id': id_sc_storage, 'exp_score': score_storage })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### scores ####\n",
    "# 1pct, alpha = 1\n",
    "scores_1pct =pd.DataFrame()\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" + str(elem) + \"-fCf_sCF_0.01_gS_default_3765/lausitz-1pct-\" + str(elem) + \"-fCf_sCf_0.01_gS_default_3765.scorestats.csv\"\n",
    "    temp = pd.read_csv(path, sep=\";\")\n",
    "    df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"1-pct\", \"sample_nr\": elem, \"alpha\": 1.0, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "    scores_1pct = pd.concat([scores_1pct, df], axis = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 pct, alpha = 0.75\n",
    "scores_1pct_sCf = pd.DataFrame()\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" + str(elem) + \"-fCf_0.01_sCF_0.03162_gS_default_3765/lausitz-1pct-\" + str(elem) + \"-fCf_0.01_sCf_0.03162_gS_default_3765.scorestats.csv\"\n",
    "    temp = pd.read_csv(path, sep=\";\")\n",
    "    df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"1-pct\", \"sample_nr\": elem, \"alpha\": 0.75, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "    scores_1pct_sCf = pd.concat([scores_1pct_sCf, df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 pct, alpha 1, random global seed\n",
    "#  1 pct random seed\n",
    "scores_1pct_rGs = pd.DataFrame()\n",
    "rGs = [ 4711,3254, 2306, 6384,4338, 6003, 5502, 9377, 5621, 9002 ]\n",
    "for seed in rGs:\n",
    "    if (seed == 4711):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        # insert number of stuck time violations from the first 1 pct sample\n",
    "        df = pd.DataFrame({\"avg_executed_it_500\":scores_1pct[\"avg_executed_it_500\"].iloc[0], \"sample_size\": \"1-pct\", \"sample_nr\": 1, \"alpha\": 1.0, \"stuck_time\": 30.0, \"global_seed\": global_seed }, index = [rGs.index(seed)])\n",
    "        scores_1pct_rGs = pd.concat([scores_1pct_rGs, df])\n",
    "    elif (seed == 3254):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        path =\"/home/lola/math_cluster/output/output-lausitz-1pct-1-fCf_sCF_0.01_gS_3254_3765/lausitz-1pct-1-fCf_sCf_0.01_gS_3254_3765.scorestats.csv\"\n",
    "        temp = pd.read_csv(path, sep = \";\")\n",
    "        df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"1-pct\", \"sample_nr\": 1, \"alpha\":1.0, \"stuck_time\": 30.0, \"global_seed\": global_seed }, index = [rGs.index(seed)])\n",
    "        scores_1pct_rGs = pd.concat([scores_1pct_rGs, df], axis = 0)\n",
    "    else:\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-1.0-pct-1-fCf_sCF_0.01_gS_\" + str(seed) + \"_3765/lausitz-1.0-pct-1-fCf_sCF_0.01_gS_\" + str(seed) + \"_3765.scorestats.csv\"\n",
    "        temp = pd.read_csv(path, sep = \";\")\n",
    "        df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"1-pct\", \"sample_nr\": 1, \"alpha\":1.0, \"stuck_time\": 30.0, \"global_seed\": global_seed }, index = [rGs.index(seed)])\n",
    "        scores_1pct_rGs = pd.concat([scores_1pct_rGs, df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 pct, alpha = 1, sT scaled\n",
    "scores_1pct_sT =pd.DataFrame()\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" + str(elem) + \"-fCf_sCF_0.01_gS_4711_sT_3000.0_3765/lausitz-1-pct-\" + str(elem) + \"-fCf_sCF_0.01_gS_4711_sT_3000.0_3765.scorestats.csv\"\n",
    "    temp = pd.read_csv(path, sep=\";\")\n",
    "    df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"1-pct\", \"sample_nr\": elem, \"alpha\": 1.0, \"stuck_time\": 3000.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "    scores_1pct_sT = pd.concat([scores_1pct_sT, df], axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 pct, alpha = 0.75, sT scaled\n",
    "scores_1pct_sT_sCf =pd.DataFrame()\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\"+ str(elem) + \"-fCf_0.01_sCF_0.03162_gS_4711_sT_3000.0_3765/lausitz-1-pct-\" + str(elem) + \"-fCf_0.01_sCF_0.03162_gS_4711_sT_3000.0_3765.scorestats.csv\"\n",
    "    temp = pd.read_csv(path, sep=\";\")\n",
    "    df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"1-pct\", \"sample_nr\": elem, \"alpha\": 0.75, \"stuck_time\": 3000.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "    scores_1pct_sT_sCf = pd.concat([scores_1pct_sT_sCf, df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_1pct_all = pd.concat([scores_1pct, scores_1pct_sCf, scores_1pct_rGs, scores_1pct_sT, scores_1pct_sT_sCf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_1pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/scores_all_1pct_samples.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 pct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### scores, 5 pct ####\n",
    "# 5pct, alpha = 1\n",
    "scores_5pct = pd.DataFrame()\n",
    "\n",
    "for elem in range(1,11,1):\n",
    "    if (elem==6):\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765/lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765-2.scorestats.csv\"\n",
    "        temp = pd.read_csv(path, sep=\";\")\n",
    "        df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"5-pct\", \"sample_nr\": elem, \"alpha\": 1.0, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "        scores_5pct = pd.concat([scores_5pct, df], axis = 0)\n",
    "    else:\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(elem) + \"-fCf_sCF_0.05_gS_4711_3765/lausitz-5.0-pct-\" + str(elem) + \"-fCf_sCF_0.05_gS_4711_3765.scorestats.csv\"\n",
    "        temp = pd.read_csv(path, sep=\";\")\n",
    "        df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"5-pct\", \"sample_nr\": elem, \"alpha\": 1.0, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "        scores_5pct = pd.concat([scores_5pct, df], axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 pct, alpha 0.75 \n",
    "scores_5pct_sCf = pd.DataFrame()\n",
    "\n",
    "for elem in range(1,11,1):\n",
    "    if (elem==6):\n",
    "        path =\"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_0.05_sCF_0.10574_gS_4711_3765/lausitz-5.0-pct-6-fCf_0.05_sCF_0.10574_gS_4711_3765-2.scorestats.csv\"\n",
    "        temp = pd.read_csv(path, sep=\";\")\n",
    "        df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"5-pct\", \"sample_nr\": elem, \"alpha\": 0.75, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "        scores_5pct_sCf = pd.concat([scores_5pct_sCf, df], axis = 0)\n",
    "\n",
    "    else:\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(elem) + \"-fCf_0.05_sCF_0.10574_gS_4711_3765/lausitz-5.0-pct-\" + str(elem) + \"-fCf_0.05_sCF_0.10574_gS_4711_3765.scorestats.csv\"\n",
    "        temp = pd.read_csv(path, sep=\";\")\n",
    "        df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"5-pct\", \"sample_nr\": elem, \"alpha\":0.75, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "        scores_5pct_sCf = pd.concat([scores_5pct_sCf, df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_5pct_rGs = pd.DataFrame()\n",
    "rGs = [4711, 3254, 2306, 6384,4338, 6003, 5502, 9377, 5621, 9002 ]\n",
    "\n",
    "for seed in rGs:\n",
    "    if (seed ==4711):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        temp = {'avg_executed_it_500': scores_5pct[\"avg_executed_it_500\"].iloc[0], 'sample_size': '5-pct', \"sample_nr\": 1, 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': global_seed  }\n",
    "        temp = pd.DataFrame(data=temp, index=[rGs.index(4711)])\n",
    "        scores_5pct_rGs = pd.concat([scores_5pct_rGs, temp])\n",
    "    else:\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-1-fCf_sCF_0.05_gS_\" + str(seed) + \"_3765/lausitz-5.0-pct-1-fCf_sCF_0.05_gS_\" + str(seed) + \"_3765.scorestats.csv\"\n",
    "        temp = pd.read_csv(path, sep = \";\")\n",
    "        temp = {'avg_executed_it_500': temp[\"avg_executed\"].iloc[500], 'sample_size': '5-pct', \"sample_nr\": 1, 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': global_seed  }\n",
    "        df = pd.DataFrame(data=temp, index=[rGs.index(seed)])\n",
    "        scores_5pct_rGs = pd.concat([scores_5pct_rGs, df], axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 pct, alpha 1.0, sT scaled\n",
    "scores_5pct_sT =pd.DataFrame()\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str(elem) + \"-fCf_sCF_0.05_gS_4711_sT_600.0_3765/lausitz-5-pct-\" + str(elem) + \"-fCf_sCF_0.05_gS_4711_sT_600.0_3765.scorestats.csv\"\n",
    "    temp = pd.read_csv(path, sep=\";\")\n",
    "    df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"5-pct\", \"sample_nr\": elem, \"alpha\": 1.0, \"stuck_time\": 600.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "    scores_5pct_sT = pd.concat([scores_5pct_sT, df], axis = 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 pct, alpha 0.75, sT scaled\n",
    "scores_5pct_sT_sCf =pd.DataFrame()\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str(elem) + \"-fCf_0.05_sCF_0.10574_gS_4711_sT_600.0_3765/lausitz-5-pct-\" + str(elem) + \"-fCf_0.05_sCF_0.10574_gS_4711_sT_600.0_3765.scorestats.csv\"\n",
    "    temp = pd.read_csv(path, sep=\";\")\n",
    "    df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"5-pct\", \"sample_nr\": elem, \"alpha\": 0.75, \"stuck_time\": 600.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "    scores_5pct_sT_sCf = pd.concat([scores_5pct_sT_sCf, df], axis = 0)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_5pct_all = pd.concat([scores_5pct, scores_5pct_sCf, scores_5pct_rGs, scores_5pct_sT, scores_5pct_sT_sCf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_5pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/scores_all_5pct_samples.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### scores, 10 pct ####\n",
    "# 10 pct, alpha = 1.0\n",
    "scores_10pct = pd.DataFrame()\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" + str(elem) + \"-fCf_sCF_0.1_gS_4711_3765/lausitz-10.0-pct-\" + str(elem) + \"-fCf_sCF_0.1_gS_4711_3765.scorestats.csv\"\n",
    "    temp = pd.read_csv(path, sep=\";\")\n",
    "    df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"10-pct\", \"sample_nr\": elem, \"alpha\": 1.0, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "    scores_10pct = pd.concat([scores_10pct, df], axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 pct, alpha = 0.75\n",
    "scores_10pct_sCf = pd.DataFrame()\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" + str(elem) + \"-fCf_0.1_sCF_0.17783_gS_4711_3765/lausitz-10.0-pct-\" + str(elem) + \"-fCf_0.1_sCF_0.17783_gS_4711_3765.scorestats.csv\"\n",
    "    temp = pd.read_csv(path, sep=\";\")\n",
    "    df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"10-pct\", \"sample_nr\": elem, \"alpha\": 0.75, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "    scores_10pct = pd.concat([scores_10pct, df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 pct, alpha = 1, sT scaled\n",
    "scores_10pct_sT = pd.DataFrame()\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\" + str(elem) + \"-fCf_sCF_0.1_gS_4711_sT_300.0_3765/lausitz-10-pct-\" + str(elem) + \"-fCf_sCF_0.1_gS_4711_sT_300.0_3765.scorestats.csv\"\n",
    "    temp = pd.read_csv(path, sep=\";\")\n",
    "    df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"10-pct\", \"sample_nr\": elem, \"alpha\": 1.0, \"stuck_time\": 300.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "    scores_10pct_sT = pd.concat([scores_10pct_sT, df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 pct, alpha = 0.75, sT scaled\n",
    "# 10 pct, alpha = 1, sT scaled\n",
    "scores_10pct_sT_sCf = pd.DataFrame()\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\" + str(elem) + \"-fCf_0.1_sCF_0.17783_gS_4711_sT_300.0_3765/lausitz-10-pct-\" + str(elem) + \"-fCf_0.1_sCF_0.17783_gS_4711_sT_300.0_3765.scorestats.csv\"\n",
    "    temp = pd.read_csv(path, sep=\";\")\n",
    "    df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"10-pct\", \"sample_nr\": elem, \"alpha\": 0.75, \"stuck_time\": 300.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "    scores_10pct_sT_sCf = pd.concat([scores_10pct_sT_sCf, df], axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_10pct_all = pd.concat([scores_10pct, scores_10pct_sCf, scores_10pct_sT, scores_10pct_sT_sCf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_10pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/scores_all_10pct_samples.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### scores, 25 pct ####\n",
    "scores_25pct = pd.DataFrame()\n",
    " # 25 pct, alpha = 1\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_sCF_0.25_gS_4711_3765/lausitz-25.0-pct-fCf_sCF_0.25_gS_4711_3765.scorestats.csv\"\n",
    "temp = pd.read_csv(path, sep = \";\")\n",
    "df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"25-pct\", \"sample_nr\": 1, \"alpha\": 1.0, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [0])\n",
    "scores_25pct = pd.concat([scores_25pct, df], axis = 0)\n",
    "\n",
    "# 25 pct, alpha = 0.75\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_0.25_sCF_0.35355_gS_4711_3765/lausitz-25.0-pct-fCf_0.25_sCF_0.35355_gS_4711_3765.scorestats.csv\"\n",
    "temp = pd.read_csv(path, sep = \";\")\n",
    "df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"25-pct\", \"sample_nr\": 1, \"alpha\": 0.75, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [1])\n",
    "scores_25pct = pd.concat([scores_25pct, df], axis = 0)\n",
    "\n",
    " # 25 pct, alpha = 1, sT scaled\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-25-pct-1-fCf_sCF_0.25_gS_4711_sT_120.0_3765/lausitz-25-pct-1-fCf_sCF_0.25_gS_4711_sT_120.0_3765.scorestats.csv\"\n",
    "temp = pd.read_csv(path, sep = \";\")\n",
    "df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"25-pct\", \"sample_nr\": 1, \"alpha\": 1.0, \"stuck_time\": 120.0, \"global_seed\": \"default\" }, index = [2])\n",
    "scores_25pct = pd.concat([scores_25pct, df], axis = 0)\n",
    "\n",
    " # 25 pct, alpha = 0.75, sT scaled\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-25-pct-1-fCf_0.25_sCF_0.35355_gS_4711_sT_120.0_3765/lausitz-25-pct-1-fCf_0.25_sCF_0.35355_gS_4711_sT_120.0_3765.scorestats.csv\"\n",
    "temp = pd.read_csv(path, sep = \";\")\n",
    "df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"25-pct\", \"sample_nr\": 1, \"alpha\": 0.75, \"stuck_time\": 120.0, \"global_seed\": \"default\" }, index = [0])\n",
    "scores_25pct = pd.concat([scores_25pct, df], axis = 0)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_25pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/scores_all_25pct_samples.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### scores, 50 pct #### \n",
    "scores_50pct = pd.DataFrame()\n",
    "# 50 pct, alpha = 1\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_sCF_0.5_gS_4711_3765/lausitz-50.0-pct-fCf_sCF_0.5_gS_4711_3765.scorestats.csv\"\n",
    "temp = pd.read_csv(path, sep = \";\")\n",
    "df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"50-pct\", \"sample_nr\": 1, \"alpha\": 1.0, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [0])\n",
    "scores_50pct = pd.concat([scores_50pct, df], axis = 0)\n",
    "\n",
    "# 50 pct, alpha = 0.75\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_0.5_sCF_0.5946_gS_4711_3765/lausitz-50.0-pct-fCf_0.5_sCF_0.5946_gS_4711_3765.scorestats.csv\"\n",
    "temp = pd.read_csv(path, sep = \";\")\n",
    "df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"50-pct\", \"sample_nr\": 1, \"alpha\": 0.75, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [1])\n",
    "scores_50pct = pd.concat([scores_50pct, df], axis = 0)\n",
    "\n",
    "# 50 pct/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_sCF_0.5_gS_4711_sT_60.0_3765/lausitz-50-pct-1-fCf_sCF_0.5_gS_4711_sT_60.0_3765.scorestats.csv\"\n",
    "temp = pd.read_csv(path, sep = \";\")\n",
    "df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"50-pct\", \"sample_nr\": 1, \"alpha\": 1.0, \"stuck_time\": 60.0, \"global_seed\": \"default\" }, index = [2])\n",
    "scores_50pct = pd.concat([scores_50pct, df], axis = 0)\n",
    "\n",
    "# 50 pct, alpha = 0.75, sT scaled\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_0.5_sCF_0.5946_gS_4711_sT_60.0_3765/lausitz-50-pct-1-fCf_0.5_sCF_0.5946_gS_4711_sT_60.0_3765.scorestats.csv\"\n",
    "temp = pd.read_csv(path, sep = \";\")\n",
    "df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"50-pct\", \"sample_nr\": 1, \"alpha\": 0.75, \"stuck_time\": 60.0, \"global_seed\": \"default\" }, index = [0])\n",
    "scores_50pct = pd.concat([scores_50pct, df], axis = 0)\n",
    "\n",
    "\n",
    "# 25 pct doubled, alpha = 1.0, sT 30.0\n",
    "path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25-pct-doubled-fCf_0.5_sCF_0.5_gS_4711_3765/lausitz-25-pct-doubled-fCf_0.5_sCF_0.5_gS_4711__3765.scorestats.csv\"\n",
    "temp = pd.read_csv(path, sep = \";\")\n",
    "df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"25-pct-doubled\", \"sample_nr\": 1, \"alpha\": 1.0 , \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [0])\n",
    "scores_50pct = pd.concat([scores_50pct, df], axis = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_50pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/scores_all_50pct_samples.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### scores , 100 pct ####\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765/lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765.scorestats.csv\"\n",
    "temp = pd.read_csv(path, sep = \";\")\n",
    "scores_100pct = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"100-pct\", \"sample_nr\": 1, \"alpha\": 1.0, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [0])\n",
    "\n",
    "path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25.0-pct-quadrupled-fCf_1.0_sCF_1.0_gS_4711_3765/lausitz-25-pct-quadrupled-fCf_1.0_sCF_1.0_gS_4711__3765.scorestats.csv\"\n",
    "temp = pd.read_csv(path, sep = \";\")\n",
    "scores_25_pct_quadrupled = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"25-pct-quadrupled\", \"sample_nr\": 1, \"alpha\": 1.0, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_1_100 = pd.concat([scores_1pct_all, scores_5pct_all, scores_10pct_all, scores_25pct, scores_50pct, scores_100pct, scores_25_pct_quadrupled], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_1_100.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/scores_all_1_100pct_samples.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5./6. Average travel time and traveled distance & N departures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAvgTravelTimeAndDist(pathToFile, sampleSize, sampleNr, alpha, stuckTime, globalSeed):\n",
    "    temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
    "    temp['trav_time_seconds'] = temp.trav_time.astype('timedelta64[s]')/  pd.Timedelta(seconds=1)\n",
    "    temp['dep_hour'] = np.floor(temp.dep_time.astype('timedelta64[s]')/  pd.Timedelta(minutes=60))\n",
    "    aTt = np.mean(temp['trav_time_seconds'])/60\n",
    "    aTt_car = np.mean(temp[temp['main_mode'] == \"car\"]['trav_time_seconds'])/60\n",
    "    aTt_ride = np.mean(temp[temp['main_mode'] == \"ride\"]['trav_time_seconds'])/60\n",
    "    aTd =  np.mean(temp['traveled_distance'])/1000\n",
    "    aTd_car =  np.mean(temp[temp['main_mode'] == \"car\"]['traveled_distance'])/1000\n",
    "    aTd_ride =  np.mean(temp[temp['main_mode'] == \"ride\"]['traveled_distance'])/1000\n",
    "\n",
    "    df_aTt_pH = pd.DataFrame()\n",
    "    for h in range(0,36,1):\n",
    "        aTt_pH_min =  np.mean(temp[temp['dep_hour'] == h]['trav_time_seconds'])/ 60\n",
    "        aTd_pH_km =  np.mean(temp[temp['dep_hour'] == h]['traveled_distance'])/ 1000\n",
    "        dict = {'hour': h, 'aTt_pH_min': aTt_pH_min, 'aTd_pH_km':aTd_pH_km,  \"sample_size\": sampleSize, \"sample_nr\": sampleNr, \"alpha\" : alpha, \"stuck_time\": stuckTime, \"global_seed\": globalSeed}\n",
    "        df_temp = pd.DataFrame(dict, index=[0])\n",
    "        df_aTt_pH = pd.concat([df_aTt_pH, df_temp], axis = 0, ignore_index= True)\n",
    "\n",
    "    df_avg_trav_time = pd.DataFrame({\"aTt_min\": aTt,\"aTt_car_min\": aTt_car, \"aTt_ride_min\": aTt_ride, \"aTd_km\": aTd,\"aTd_car_km\": aTd_car, \"aTd_ride_km\": aTd_ride,  \"sample_size\": sampleSize, \"sample_nr\": sampleNr, \"alpha\" : alpha, \"stuck_time\": stuckTime, \"global_seed\": globalSeed}, index=[sampleNr])\n",
    "    return [df_avg_trav_time, df_aTt_pH]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def calcAvgTravelDistance(pathToFile, sampleSize, sampleNr, alpha, stuckTime, globalSeed):\n",
    "    temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
    "    avg_trav_dist = np.mean(temp[\"traveled_distance\"])\n",
    "    df_avg_trav_dist = pd.DataFrame({\"avg_trav_dist\": avg_trav_dist, \"sample_size\": sampleSize, \"sample_nr\": sampleNr, \"alpha\" : alpha, \"stuck_time\": stuckTime, \"global_seed\": globalSeed}, index=[sampleNr])\n",
    "    return df_avg_trav_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcDepPerHour(pathToFile, sampleSize, sampleNr, alpha, stuckTime, globalSeed):\n",
    "    temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
    "    departure_hours = []\n",
    "    for element in temp['dep_time']:\n",
    "        departure_hours.append(element[:2])\n",
    "        \n",
    "    df_all_dep = pd.DataFrame({'dep_hour': departure_hours})\n",
    "\n",
    "    df_n_of_dep_per_hour = pd.DataFrame()\n",
    "    for element in df_all_dep['dep_hour'].unique():\n",
    "        temp_dep_hour = pd.DataFrame({'hour': element, 'n_departues': df_all_dep[(df_all_dep['dep_hour']== element)].shape[0] }, index = [0])\n",
    "        df_n_of_dep_per_hour = pd.concat([df_n_of_dep_per_hour, temp_dep_hour], axis = 0, ignore_index= True)\n",
    "\n",
    "    df_n_of_dep_per_hour = df_n_of_dep_per_hour.sort_values(by=['hour'])\n",
    "    df_n_of_dep_per_hour.insert(2,\"sample_size\", sampleSize)\n",
    "    df_n_of_dep_per_hour.insert(3,\"sample_nr\", sampleNr)\n",
    "    df_n_of_dep_per_hour.insert(4,'alpha', alpha)\n",
    "    df_n_of_dep_per_hour.insert(5,'stuck_time', stuckTime)\n",
    "    df_n_of_dep_per_hour.insert(6,'global_seed', globalSeed)\n",
    "    return df_n_of_dep_per_hour\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.01\"]\n",
    "storCapF =  [\"0.01\", \"0.03162\"]\n",
    "\n",
    "# initialize empty data frames\n",
    "avg_travel_times_and_dist_1pct = pd.DataFrame()\n",
    "#avg_travel_distances_1pct = pd.DataFrame()\n",
    "dep_per_hour_1pct = pd.DataFrame()\n",
    "aTt_perHour_1pct = pd.DataFrame()\n",
    "\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "        for sampleNr in range(1,11,1):\n",
    "            # calculate adjusted stuck time\n",
    "            default_stuck_time = 30.0\n",
    "            adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "            # declare sample size as str \"1-pct\"\n",
    "            sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "\n",
    "            if ((fCf == \"0.01\") & (sCf == '0.01')):\n",
    "                # declare alpha\n",
    "                alpha = 1.0\n",
    "                # paths for case 1 and 3 \n",
    "                path_case1 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\"+ str(sampleNr) +\"-fCf_sCF_\" + sCf + \"_gS_default_3765/lausitz-1pct-\"+ str(sampleNr) +\"-fCf_sCf_0.01_gS_default_3765.output_trips.csv.gz\"\n",
    "                path_case3 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" +str(sampleNr) +\"-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-1-pct-\" +str(sampleNr) + \"-fCf_sCF_0.01_gS_4711_sT_3000.0_3765.output_trips.csv.gz\"\n",
    "                \n",
    "                # average travel time\n",
    "                aTt_case1= calcAvgTravelTimeAndDist(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                aTt_case3= calcAvgTravelTimeAndDist(path_case3, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                # concatenate with existing values\n",
    "                avg_travel_times_and_dist_1pct = pd.concat([avg_travel_times_and_dist_1pct, aTt_case1[0], aTt_case3[0]], ignore_index= True)\n",
    "                aTt_perHour_1pct = pd.concat([aTt_perHour_1pct, aTt_case1[1], aTt_case3[1]], ignore_index= True)\n",
    "\n",
    "                # number of departures per hour \n",
    "                dPh_case1 = calcDepPerHour(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                dPh_case3 = calcDepPerHour(path_case3, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                # concatenate results\n",
    "                dep_per_hour_1pct = pd.concat([dep_per_hour_1pct, dPh_case1, dPh_case3], ignore_index= True)\n",
    "\n",
    "\n",
    "            else:\n",
    "                alpha = 0.75\n",
    "                path_case2 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" +str(sampleNr) + \"-fCf_0.01_sCF_\" + sCf + \"_gS_default_3765/lausitz-1pct-\" + str(sampleNr) + \"-fCf_0.01_sCf_0.03162_gS_default_3765.output_trips.csv.gz\"\n",
    "                path_case4 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" + str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-1-pct-\" + str(sampleNr) + \"-fCf_0.01_sCF_0.03162_gS_4711_sT_3000.0_3765.output_trips.csv.gz\"\n",
    "                \n",
    "                # average travel time\n",
    "                aTt_case2= calcAvgTravelTimeAndDist(path_case2, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                aTt_case4= calcAvgTravelTimeAndDist(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                # concatenate with existing values\n",
    "                avg_travel_times_and_dist_1pct = pd.concat([avg_travel_times_and_dist_1pct, aTt_case2[0], aTt_case4[0]], ignore_index= True)\n",
    "                aTt_perHour_1pct = pd.concat([aTt_perHour_1pct, aTt_case2[1], aTt_case4[1]], ignore_index= True)\n",
    "\n",
    "                # number of departures per hour \n",
    "                dPh_case2 = calcDepPerHour(path_case2, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                dPh_case4 = calcDepPerHour(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                # concatenate results\n",
    "                dep_per_hour_1pct = pd.concat([dep_per_hour_1pct, dPh_case2, dPh_case4], ignore_index= True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 pct rGs\n",
    "avg_trav_time_and_dist_1pct_rGs = pd.DataFrame()\n",
    "aTt_perHour_1pct_rgs = pd.DataFrame()\n",
    "rGs = [ 4711,3254, 2306, 6384,4338, 6003, 5502, 9377, 5621, 9002 ]\n",
    "for seed in rGs:\n",
    "    if (seed == 4711):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        # Create Data frame and insert first value from avg_trav_time_1pct\n",
    "        temp = avg_travel_times_and_dist_1pct[(avg_travel_times_and_dist_1pct['alpha']== 1.0) & (avg_travel_times_and_dist_1pct['stuck_time']== 30.0)& (avg_travel_times_and_dist_1pct['sample_nr'] == 1)]\n",
    "        df1 = pd.DataFrame({\"aTt_min\": temp[\"aTt_min\"],\"aTt_car_min\": temp['aTt_car_min'], \"aTt_ride_min\": temp['aTt_ride_min'], \"aTd_km\": temp['aTd_km'],\"aTd_car_km\": temp['aTd_car_km'], \"aTd_ride_km\": temp['aTd_car_km'], \"sample_size\": \"1-pct\", \"sample_nr\": 1, \"alpha\": 1.0, \"stuck_time\": 30.0, \"global_seed\": global_seed }, index = [rGs.index(seed)])\n",
    "        avg_trav_time_and_dist_1pct_rGs = pd.concat([avg_trav_time_and_dist_1pct_rGs, df1])\n",
    "        \n",
    "        df2 = aTt_perHour_1pct[(aTt_perHour_1pct['sample_size'] == \"1-pct\")  & (aTt_perHour_1pct['alpha'] == 1) & (aTt_perHour_1pct['sample_nr'] == 1) & (aTt_perHour_1pct['stuck_time'] == 30.0)].copy()\n",
    "        df2['global_seed'] = df2['global_seed'].str.replace(\"default\", global_seed)\n",
    "        aTt_perHour_1pct_rgs = pd.concat([aTt_perHour_1pct_rgs, df2], axis = 0, ignore_index = True)\n",
    "\n",
    "    elif (seed == 3254):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        path =\"/home/lola/math_cluster/output/output-lausitz-1pct-1-fCf_sCF_0.01_gS_3254_3765/lausitz-1pct-1-fCf_sCf_0.01_gS_3254_3765.output_trips.csv.gz\"\n",
    "\n",
    "        # average travel time and distance\n",
    "        aTt_case5= calcAvgTravelTimeAndDist(path, sample_size_as_string, 1,  alpha, default_stuck_time, global_seed)\n",
    "        avg_trav_time_and_dist_1pct_rGs = pd.concat([avg_trav_time_and_dist_1pct_rGs, aTt_case5[0]], axis = 0, ignore_index = True)\n",
    "        aTt_perHour_1pct_rgs = pd.concat([aTt_perHour_1pct_rgs, aTt_case5[1]], axis = 0, ignore_index = True)\n",
    "\n",
    "\n",
    "    else:\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        path =\"/home/lola/math_cluster/output/output-lausitz-1.0-pct-1-fCf_sCF_0.01_gS_\"+ str(seed) + \"_3765/lausitz-1.0-pct-1-fCf_sCF_0.01_gS_\" +str(seed) +\"_3765.output_trips.csv.gz\"\n",
    "        # average travel time and distance\n",
    "        aTt_case5= calcAvgTravelTimeAndDist(path, sample_size_as_string, 1,  alpha, default_stuck_time, global_seed)\n",
    "        avg_trav_time_and_dist_1pct_rGs = pd.concat([avg_trav_time_and_dist_1pct_rGs, aTt_case5[0]], axis = 0, ignore_index = True)\n",
    "        aTt_perHour_1pct_rgs = pd.concat([aTt_perHour_1pct_rgs, aTt_case5[1]], axis = 0, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_travel_times_and_dist_1pct_all = pd.concat([avg_travel_times_and_dist_1pct, avg_trav_time_and_dist_1pct_rGs], axis = 0, ignore_index= True)\n",
    "aTt_perHour_1pct = pd.concat([aTt_perHour_1pct, aTt_perHour_1pct_rgs], ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_travel_times_and_dist_1pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/avg_trav_time_and_dist_all_1pct_samples.csv', index = False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.05\"]\n",
    "storCapF =  [\"0.05\", \"0.10574\"]\n",
    "\n",
    "# initialize empty data frames\n",
    "avg_travel_times_and_dist_5pct = pd.DataFrame()\n",
    "aTt_perHour_5pct = pd.DataFrame()\n",
    "dep_per_hour_5pct = pd.DataFrame()\n",
    "\n",
    "\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,11,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "\n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.05\") & (sCf == \"0.05\")):\n",
    "                    alpha = 1.0\n",
    "                    if (sampleNr == 6):\n",
    "                        path_case1  = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_sCF_\" + sCf + \"_gS_4711_3765/lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765-2.output_trips.csv.gz\"\n",
    "                    \n",
    "                    else: \n",
    "                        path_case1 = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(sampleNr) + \"-fCf_sCF_0.05_gS_4711_3765/lausitz-5.0-pct-\" + str(sampleNr) + \"-fCf_sCF_0.05_gS_4711_3765.output_trips.csv.gz\"\n",
    "                    \n",
    "                    \n",
    "                    path_case3 =  \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" +str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_sT_600.0_3765/lausitz-5-pct-\" + str(sampleNr) + \"-fCf_sCF_0.05_gS_4711_sT_600.0_3765.output_trips.csv.gz\"\n",
    "\n",
    "                    # average travel time\n",
    "                    aTt_case1= calcAvgTravelTimeAndDist(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    aTt_case3= calcAvgTravelTimeAndDist(path_case3, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    avg_travel_times_and_dist_5pct = pd.concat([avg_travel_times_and_dist_5pct, aTt_case1[0], aTt_case3[0]], ignore_index= True)\n",
    "                    aTt_perHour_5pct = pd.concat([aTt_perHour_5pct, aTt_case1[1], aTt_case3[1]], ignore_index= True)\n",
    "\n",
    "                    # number of departures per hour \n",
    "                    dPh_case1 = calcDepPerHour(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    dPh_case3 = calcDepPerHour(path_case3, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate results\n",
    "                    dep_per_hour_5pct = pd.concat([dep_per_hour_5pct, dPh_case1, dPh_case3], ignore_index= True)\n",
    "                        \n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    if(sampleNr == 6):\n",
    "                        path_case2 =  \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_0.05_sCF_0.10574_gS_4711_3765/lausitz-5.0-pct-6-fCf_0.05_sCF_0.10574_gS_4711_3765-2.output_trips.csv.gz\"\n",
    "                    else:\n",
    "                        path_case2 = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" +str(sampleNr) + \"-fCf_0.05_sCF_0.10574_gS_4711_3765/lausitz-5.0-pct-\" + str(sampleNr) + \"-fCf_0.05_sCF_0.10574_gS_4711_3765.output_trips.csv.gz\"\n",
    "                    \n",
    "                    \n",
    "                    path_case4 = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str(sampleNr) + \"-fCf_0.05_sCF_0.10574_gS_4711_sT_600.0_3765/lausitz-5-pct-\" + str(sampleNr) + \"-fCf_0.05_sCF_0.10574_gS_4711_sT_600.0_3765.output_trips.csv.gz\"\n",
    "\n",
    "                    # average travel time\n",
    "                    aTt_case2= calcAvgTravelTimeAndDist(path_case2, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    aTt_case4= calcAvgTravelTimeAndDist(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    avg_travel_times_and_dist_5pct = pd.concat([avg_travel_times_and_dist_5pct, aTt_case2[0], aTt_case4[0]], ignore_index= True)\n",
    "                    aTt_perHour_5pct = pd.concat([aTt_perHour_5pct, aTt_case2[1], aTt_case4[1]], ignore_index= True)\n",
    "\n",
    "                    # number of departures per hour \n",
    "                    dPh_case2 = calcDepPerHour(path_case2, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    dPh_case4 = calcDepPerHour(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate results\n",
    "                    dep_per_hour_5pct = pd.concat([dep_per_hour_5pct, dPh_case2, dPh_case4], ignore_index= True)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 pct random global seed\n",
    "avg_trav_time_and_dist_5pct_rGs = pd.DataFrame()\n",
    "aTt_perHour_5pct_rgs = pd.DataFrame()\n",
    "rGs = [ 4711,3254, 2306, 6384,4338, 6003, 5502, 9377, 5621, 9002 ]\n",
    "for seed in rGs:\n",
    "    if (seed == 4711):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        # Create Data frame and insert first value from avg_trav_time_5pct\n",
    "        temp = avg_travel_times_and_dist_5pct[(avg_travel_times_and_dist_5pct['alpha']== 1.0) & (avg_travel_times_and_dist_5pct['stuck_time']== 30.0)& (avg_travel_times_and_dist_5pct['sample_nr'] == 1)]\n",
    "        df1 = pd.DataFrame({\"aTt_min\": temp[\"aTt_min\"],\"aTt_car_min\": temp['aTt_car_min'], \"aTt_ride_min\": temp['aTt_ride_min'], \"aTd_km\": temp['aTd_km'],\"aTd_car_km\": temp['aTd_car_km'], \"aTd_ride_km\": temp['aTd_car_km'], \"sample_size\": \"1-pct\", \"sample_nr\": 1, \"alpha\": 1.0, \"stuck_time\": 30.0, \"global_seed\": global_seed }, index = [rGs.index(seed)])\n",
    "        avg_trav_time_and_dist_5pct_rGs = pd.concat([avg_trav_time_and_dist_5pct_rGs, df1])\n",
    "\n",
    "        df2 = aTt_perHour_5pct[(aTt_perHour_5pct['sample_size'] == \"5-pct\")  & (aTt_perHour_5pct['alpha'] == 1) & (aTt_perHour_5pct['sample_nr'] == 1) & (aTt_perHour_5pct['stuck_time'] == 30.0)].copy()\n",
    "        df2['global_seed'] = df2['global_seed'].str.replace(\"default\", global_seed)\n",
    "        aTt_perHour_5pct_rgs = pd.concat([aTt_perHour_5pct_rgs, df2], axis = 0, ignore_index = True)\n",
    "        \n",
    "    elif (seed == 3254):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        path =\"/home/lola/math_cluster/output/output-lausitz-5.0-pct-1-fCf_sCF_0.05_gS_3254_3765/lausitz-5.0-pct-1-fCf_sCF_0.05_gS_3254_3765.output_trips.csv.gz\"\n",
    "        aTt_case5= calcAvgTravelTimeAndDist(path, \"5-pct\", 1,  1.0, 30.0, global_seed)\n",
    "\n",
    "        # average travel time and distance\n",
    "        avg_trav_time_and_dist_5pct_rGs = pd.concat([avg_trav_time_and_dist_5pct_rGs, aTt_case5[0]], axis = 0)\n",
    "        aTt_perHour_5pct_rgs = pd.concat([aTt_perHour_5pct_rgs, aTt_case5[1]], axis = 0, ignore_index = True)\n",
    "\n",
    "    else:\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        path =\"/home/lola/math_cluster/output/output-lausitz-5.0-pct-1-fCf_sCF_0.05_gS_\" + str(seed) + \"_3765/lausitz-5.0-pct-1-fCf_sCF_0.05_gS_\" + str(seed) + \"_3765.output_trips.csv.gz\"\n",
    "        # average travel time and dist\n",
    "        aTt_case5= calcAvgTravelTimeAndDist(path, \"5-pct\", 1,  1.0, 30.0, global_seed)\n",
    "\n",
    "        avg_trav_time_and_dist_5pct_rGs = pd.concat([avg_trav_time_and_dist_5pct_rGs, aTt_case5[0]], axis = 0)\n",
    "        aTt_perHour_5pct_rgs = pd.concat([aTt_perHour_5pct_rgs, aTt_case5[1]], axis = 0, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_travel_times_and_dist_5pct_all = pd.concat([avg_travel_times_and_dist_5pct, avg_trav_time_and_dist_5pct_rGs], axis = 0)\n",
    "aTt_perHour_5pct = pd.concat([aTt_perHour_5pct, aTt_perHour_5pct_rgs], ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_travel_times_and_dist_5pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/avg_trav_time_and_dist_all_5pct_samples.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6504/3460482934.py:2: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
      "/tmp/ipykernel_6504/3460482934.py:2: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
      "/tmp/ipykernel_6504/951122156.py:2: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
      "/tmp/ipykernel_6504/951122156.py:2: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
      "/tmp/ipykernel_6504/3460482934.py:2: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
      "/tmp/ipykernel_6504/3460482934.py:2: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
      "/tmp/ipykernel_6504/951122156.py:2: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
      "/tmp/ipykernel_6504/951122156.py:2: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n"
     ]
    }
   ],
   "source": [
    "flowCapF = [\"0.1\"]\n",
    "storCapF =  [\"0.1\", \"0.17783\"]\n",
    "\n",
    "# initialize empty data frames\n",
    "avg_travel_times_and_dist_10pct = pd.DataFrame()\n",
    "aTt_perHour_10pct = pd.DataFrame()\n",
    "dep_per_hour_10pct = pd.DataFrame()\n",
    "\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,11,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.1\") & (sCf == \"0.1\")):\n",
    "                    alpha = 1.0\n",
    "                    path_case1 = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" +str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_3765/lausitz-10.0-pct-\" +str(sampleNr) + \"-fCf_sCF_0.1_gS_4711_3765.output_trips.csv.gz\"\n",
    "                    path_case3 = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\" +str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-10-pct-\" +str(sampleNr) + \"-fCf_sCF_0.1_gS_4711_sT_300.0_3765.output_trips.csv.gz\"\n",
    "                    \n",
    "                    # average travel time\n",
    "                    aTt_case1= calcAvgTravelTimeAndDist(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    aTt_case3= calcAvgTravelTimeAndDist(path_case3, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    avg_travel_times_and_dist_10pct = pd.concat([avg_travel_times_and_dist_10pct, aTt_case1[0], aTt_case3[0]], ignore_index= True)\n",
    "                    aTt_perHour_10pct = pd.concat([aTt_perHour_10pct, aTt_case1[1], aTt_case3[1]], ignore_index= True)\n",
    "\n",
    "\n",
    "                    # number of departures per hour \n",
    "                    dPh_case1 = calcDepPerHour(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    dPh_case3 = calcDepPerHour(path_case3, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate results\n",
    "                    dep_per_hour_10pct = pd.concat([dep_per_hour_10pct, dPh_case1, dPh_case3], ignore_index= True)\n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    path_case2 = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" + str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf +  \"_gS_4711_3765/lausitz-10.0-pct-\" + str(sampleNr) +\"-fCf_0.1_sCF_0.17783_gS_4711_3765.output_trips.csv.gz\"\n",
    "                    path_case4 = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\" +str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\"+ str(adjusted_stuck_time) + \"_3765/lausitz-10-pct-\" + str(sampleNr) + \"-fCf_0.1_sCF_0.17783_gS_4711_sT_300.0_3765.output_trips.csv.gz\"\n",
    "\n",
    "                    # average travel time\n",
    "                    aTt_case2= calcAvgTravelTimeAndDist(path_case2, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    aTt_case4= calcAvgTravelTimeAndDist(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    avg_travel_times_and_dist_10pct = pd.concat([avg_travel_times_and_dist_10pct, aTt_case2[0], aTt_case4[0]], ignore_index= True)\n",
    "                    aTt_perHour_10pct = pd.concat([aTt_perHour_10pct, aTt_case2[1], aTt_case4[1]], ignore_index= True)\n",
    "\n",
    "\n",
    "\n",
    "                    # number of departures per hour \n",
    "                    dPh_case2 = calcDepPerHour(path_case2, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    dPh_case4 = calcDepPerHour(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate results\n",
    "                    dep_per_hour_10pct = pd.concat([dep_per_hour_10pct, dPh_case2, dPh_case4], ignore_index= True)\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_travel_times_and_dist_10pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/avg_travel_times_and_dist_10pct_samples.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.25\"]\n",
    "storCapF =  [\"0.25\", \"0.35355\"]\n",
    "\n",
    "# initialize empty data frames\n",
    "avg_travel_times_and_dist_25pct = pd.DataFrame()\n",
    "aTt_perHour_25pct = pd.DataFrame()\n",
    "dep_per_hour_25pct = pd.DataFrame()\n",
    "\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,2,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "                \n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.25\") & (sCf == \"0.25\")):\n",
    "                    alpha = 1.0\n",
    "                    path_case1 = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_sCF_\" + sCf + \"_gS_4711_3765/lausitz-25.0-pct-fCf_sCF_0.25_gS_4711_3765.output_trips.csv.gz\"\n",
    "                    path_case3 = \"/home/lola/math_cluster/output/output-lausitz-25-pct-1-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-25-pct-1-fCf_sCF_0.25_gS_4711_sT_120.0_3765.output_trips.csv.gz\"\n",
    "\n",
    "                    # average travel time and distances\n",
    "                    aTt_case1= calcAvgTravelTimeAndDist(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    aTt_case3= calcAvgTravelTimeAndDist(path_case3, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    avg_travel_times_and_dist_25pct = pd.concat([avg_travel_times_and_dist_25pct, aTt_case1[0], aTt_case3[0]], ignore_index= True)\n",
    "                    aTt_perHour_25pct = pd.concat([aTt_perHour_25pct, aTt_case1[1], aTt_case3[1]], ignore_index= True)\n",
    "\n",
    "                    # number of departures per hour \n",
    "                    dPh_case1 = calcDepPerHour(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    dPh_case3 = calcDepPerHour(path_case3, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate results\n",
    "                    dep_per_hour_25pct = pd.concat([dep_per_hour_25pct, dPh_case1, dPh_case3], ignore_index= True)\n",
    "\n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    path_case2 = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_3765/lausitz-25.0-pct-fCf_0.25_sCF_0.35355_gS_4711_3765.output_trips.csv.gz\"\n",
    "                    path_case4 = \"/home/lola/math_cluster/output/output-lausitz-25-pct-1-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-25-pct-1-fCf_0.25_sCF_0.35355_gS_4711_sT_120.0_3765.output_trips.csv.gz\"\n",
    "                    \n",
    "                    # average travel time and distances\n",
    "                    aTt_case2= calcAvgTravelTimeAndDist(path_case2, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    aTt_case4= calcAvgTravelTimeAndDist(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    avg_travel_times_and_dist_25pct = pd.concat([avg_travel_times_and_dist_25pct, aTt_case2[0], aTt_case4[0]], ignore_index= True)\n",
    "                    aTt_perHour_25pct = pd.concat([aTt_perHour_25pct, aTt_case2[1], aTt_case4[1]], ignore_index= True)\n",
    "\n",
    "                    # number of departures per hour \n",
    "                    dPh_case2 = calcDepPerHour(path_case2, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    dPh_case4 = calcDepPerHour(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate results\n",
    "                    dep_per_hour_25pct = pd.concat([dep_per_hour_25pct, dPh_case2, dPh_case4], ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_travel_times_and_dist_25pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/avg_travel_times_and_dist_25pct_samples.csv', index = False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.5\"]\n",
    "storCapF =  [\"0.5\", \"0.5946\"]\n",
    "\n",
    "# initialize empty data frames\n",
    "avg_travel_times_and_dist_50pct = pd.DataFrame()\n",
    "aTt_perHour_50pct = pd.DataFrame()\n",
    "dep_per_hour_50pct = pd.DataFrame()\n",
    "\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,2,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "\n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.5\") & (sCf == \"0.5\")):\n",
    "                    alpha = 1.0\n",
    "                    path_case1 = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_sCF_\" + sCf + \"_gS_4711_3765/lausitz-50.0-pct-fCf_sCF_0.5_gS_4711_3765.output_trips.csv.gz\"\n",
    "                    path_case3 = \"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-50-pct-1-fCf_sCF_0.5_gS_4711_sT_60.0_3765.output_trips.csv.gz\"\n",
    "                    \n",
    "                    # average travel time\n",
    "                    aTt_case1= calcAvgTravelTimeAndDist(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    aTt_case3= calcAvgTravelTimeAndDist(path_case3, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    avg_travel_times_and_dist_50pct = pd.concat([avg_travel_times_and_dist_50pct, aTt_case1[0], aTt_case3[0]], ignore_index= True)\n",
    "                    aTt_perHour_50pct = pd.concat([aTt_perHour_50pct, aTt_case1[1], aTt_case3[1]], ignore_index= True)\n",
    "\n",
    "\n",
    "                    # number of departures per hour \n",
    "                    dPh_case1 = calcDepPerHour(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    dPh_case3 = calcDepPerHour(path_case3, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate results\n",
    "                    dep_per_hour_50pct = pd.concat([dep_per_hour_50pct, dPh_case1, dPh_case3], ignore_index= True)\n",
    "\n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    path_case2 = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_3765/lausitz-50.0-pct-fCf_0.5_sCF_0.5946_gS_4711_3765.output_trips.csv.gz\"\n",
    "                    path_case4 = \"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-50-pct-1-fCf_0.5_sCF_0.5946_gS_4711_sT_60.0_3765.output_trips.csv.gz\"\n",
    "                    \n",
    "                    # average travel time\n",
    "                    aTt_case2= calcAvgTravelTimeAndDist(path_case2, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    aTt_case4= calcAvgTravelTimeAndDist(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    avg_travel_times_and_dist_50pct = pd.concat([avg_travel_times_and_dist_50pct, aTt_case2[0], aTt_case4[0]], ignore_index= True)\n",
    "                    aTt_perHour_50pct = pd.concat([aTt_perHour_50pct, aTt_case2[1], aTt_case4[1]], ignore_index= True)\n",
    "\n",
    "\n",
    "                    # number of departures per hour \n",
    "                    dPh_case2 = calcDepPerHour(path_case2, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    dPh_case4 = calcDepPerHour(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate results\n",
    "                    dep_per_hour_50pct = pd.concat([dep_per_hour_50pct, dPh_case2, dPh_case4], ignore_index= True)\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25 pct doubled\n",
    "\n",
    "path_case1 = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25-pct-doubled-fCf_0.5_sCF_0.5_gS_4711_3765/lausitz-25-pct-doubled-fCf_0.5_sCF_0.5_gS_4711__3765.output_trips.csv.gz\"\n",
    "# average travel time\n",
    "aTt_case1= calcAvgTravelTimeAndDist(path_case1, \"25-pct-doubled\", 1,  1.0, 30.0, 'default')\n",
    "\n",
    "# concatenate with existing values\n",
    "avg_travel_times_and_dist_50pct = pd.concat([avg_travel_times_and_dist_50pct, aTt_case1[0]], ignore_index= True)\n",
    "\n",
    "\n",
    "# number of departures per hour \n",
    "dPh_case1 = calcDepPerHour(path_case1, \"25-pct-doubled\", 1,  1.0, 30.0, 'default')\n",
    "\n",
    "# concatenate results\n",
    "dep_per_hour_50pct = pd.concat([dep_per_hour_50pct, dPh_case1], ignore_index= True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_travel_times_and_dist_50pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/avg_travel_times_and_dist_50pct_samples.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 pct\n",
    "dep_per_hour_100pct = pd.DataFrame()\n",
    "avg_travel_times_and_dist_100pct = pd.DataFrame()\n",
    "avg_travel_distances_100pct = pd.DataFrame()\n",
    "path_case1 = \"/home/lola/math_cluster/output/output-lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765/lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765.output_trips.csv.gz\"\n",
    "aTt_case1= calcAvgTravelTimeAndDist(path_case1, \"100-pct\", 1,  1.0, 30.0, 'default')\n",
    "\n",
    "# concatenate with existing values\n",
    "avg_travel_times_and_dist_100pct = pd.concat([avg_travel_times_and_dist_100pct, aTt_case1[0]], ignore_index= True)\n",
    "aTt_perHour_100pct = aTt_case1[1]\n",
    "# number of departures per hour \n",
    "dPh_case1 = calcDepPerHour(path_case1, \"100-pct\", 1,  1.0, 30.0, 'default')\n",
    "\n",
    "# concatenate results\n",
    "dep_per_hour_100pct = pd.concat([dep_per_hour_100pct, dPh_case1], ignore_index= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6504/3460482934.py:2: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
      "/tmp/ipykernel_6504/951122156.py:2: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n"
     ]
    }
   ],
   "source": [
    "# 25 pct quadrupled\n",
    "path_case1 = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25.0-pct-quadrupled-fCf_1.0_sCF_1.0_gS_4711_3765/lausitz-25-pct-quadrupled-fCf_1.0_sCF_1.0_gS_4711__3765.output_trips.csv.gz\"\n",
    "# average travel time\n",
    "aTt_case1= calcAvgTravelTimeAndDist(path_case1, \"25-pct-quadrupled\", 1,  1.0, 30.0, 'default')\n",
    "\n",
    "# concatenate with existing values\n",
    "avg_travel_times_and_dist_100pct = pd.concat([avg_travel_times_and_dist_100pct, aTt_case1[0]], ignore_index= True)\n",
    "\n",
    "\n",
    "# number of departures per hour \n",
    "dPh_case1 = calcDepPerHour(path_case1, \"25-pct-quadrupled\", 1,  1.0, 30.0, 'default')\n",
    "\n",
    "# concatenate results\n",
    "dep_per_hour_100pct = pd.concat([dep_per_hour_100pct, dPh_case1], ignore_index= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat \n",
    "avg_trav_time_and_dist_all = pd.concat([avg_travel_times_and_dist_1pct_all, avg_travel_times_and_dist_5pct_all, avg_travel_times_and_dist_10pct,\n",
    "                                avg_travel_times_and_dist_25pct, avg_travel_times_and_dist_50pct, avg_travel_times_and_dist_100pct], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "aTt_perHour_1_100 = pd.concat([aTt_perHour_1pct, aTt_perHour_5pct, aTt_perHour_10pct, aTt_perHour_25pct, aTt_perHour_50pct, aTt_perHour_100pct], ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_per_hour_all = pd.concat([dep_per_hour_1pct, dep_per_hour_5pct, dep_per_hour_10pct, dep_per_hour_25pct, dep_per_hour_50pct, dep_per_hour_100pct], axis = 0, ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_departures = pd.DataFrame()\n",
    "list_adj_Factors = []\n",
    "\n",
    "for sampleSize in dep_per_hour_all['sample_size'].unique():\n",
    "    for alpha in dep_per_hour_all[(dep_per_hour_all['sample_size'] == sampleSize)]['alpha'].unique():\n",
    "        for stuckTime in dep_per_hour_all[(dep_per_hour_all['sample_size'] == sampleSize) & (dep_per_hour_all['alpha'] == alpha)]['stuck_time'].unique():\n",
    "            for hour in dep_per_hour_all[(dep_per_hour_all['sample_size'] == sampleSize) & (dep_per_hour_all['alpha'] == alpha) & (dep_per_hour_all['stuck_time'] == stuckTime)]['hour'].unique():\n",
    "                # calculate adjustment factor\n",
    "                if (sampleSize.find('doubled') > -1):\n",
    "                    adj_Factor = 2.0\n",
    "                elif(sampleSize.find('quadrupled') > -1):\n",
    "                    adj_Factor = 1.0\n",
    "                else:\n",
    "                    sZ = sampleSize.replace(\"-pct\", \"\")\n",
    "                    adj_Factor = 100.0 / float(sZ)\n",
    "                list_adj_Factors.append(adj_Factor)\n",
    "\n",
    "                avg_departures_scaled = np.mean(dep_per_hour_all[(dep_per_hour_all['sample_size'] == sampleSize) & (dep_per_hour_all['alpha'] == alpha) & (dep_per_hour_all['stuck_time'] == stuckTime) \n",
    "                                                          & (dep_per_hour_all['hour'] == hour)]['n_departues'])*adj_Factor\n",
    "                temp = pd.DataFrame({'sample_size': sampleSize, 'alpha': alpha, 'stuck_time': stuckTime, 'hour': hour, 'avg_dep_scaled': avg_departures_scaled}, index = [0])\n",
    "                aggregated_departures = pd.concat([aggregated_departures, temp], axis = 0, ignore_index= True)\n",
    "                \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_trav_time_and_dist_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/avg_trav_time_and_dist_1_to_100_pct_samples.csv', index = False)\n",
    "aggregated_departures.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/aggregated_departures_1_to_100_pct_samples_already_scaled.csv', index = False)\n",
    "aTt_perHour_1_100.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/avg_trav_time_per_hour_1_to_100_pct.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Travel Time distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Travel time categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sixMinuteCategoriesUpToTwoHours(df):\n",
    "    categories = []\n",
    "    for t in df[\"trav_time2\"]:\n",
    "\n",
    "\n",
    "        if (0.0 <= t and t < 0.10):\n",
    "            categories.append(\"0:00_0:06\")\n",
    "        elif (0.10 <= t and t < 0.20):\n",
    "            categories.append(\"0:06_0:12\") \n",
    "        elif (0.20 <= t and t < 0.30):\n",
    "            categories.append(\"0:12_0:18\")\n",
    "        elif (0.30 <= t and t < 0.40):\n",
    "            categories.append(\"0:18_0:24\")\n",
    "        elif (0.40 <= t and t < 0.50):\n",
    "            categories.append(\"0:24_0:30\")\n",
    "        elif (0.50 <= t and t < 0.60):\n",
    "            categories.append(\"0:30_0:36\")  \n",
    "        elif (0.60 <= t and t < 0.70):\n",
    "            categories.append(\"0:36_0:42\")\n",
    "        elif (0.70 <= t and t < 0.80):\n",
    "            categories.append(\"0:42_0:48\")\n",
    "        elif (0.80 <= t and t < 0.90):\n",
    "            categories.append(\"0:48_0:54\")\n",
    "        elif (0.90 <= t and t < 1.00):\n",
    "            categories.append(\"0:54_1:00\")\n",
    "\n",
    "\n",
    "        elif (1.0 <= t and t < 1.10):\n",
    "            categories.append(\"1:00_1:06\")\n",
    "        elif (1.10 <= t and t < 1.20):\n",
    "            categories.append(\"1:06_1:12\") \n",
    "        elif (1.20 <= t and t < 1.30):\n",
    "            categories.append(\"1:12_1:18\")\n",
    "        elif (1.30 <= t and t < 1.40):\n",
    "            categories.append(\"1:18_1:24\")\n",
    "        elif (1.40 <= t and t < 1.50):\n",
    "            categories.append(\"1:24_1:30\")\n",
    "        elif (1.50 <= t and t < 1.60):\n",
    "            categories.append(\"1:30_1:36\")  \n",
    "        elif (1.60 <= t and t < 1.70):\n",
    "            categories.append(\"1:36_1:42\")\n",
    "        elif (1.70 <= t and t < 1.80):\n",
    "            categories.append(\"1:42_1:48\")\n",
    "        elif (1.80 <= t and t < 1.90):\n",
    "            categories.append(\"1:48_1:54\")\n",
    "        elif (1.90 <= t and t < 2.00):\n",
    "            categories.append(\"1:54_2:00\")\n",
    "\n",
    "        else: categories.append(\">2h\")\n",
    "\n",
    "    temp = pd.DataFrame({ \"categories\": categories})\n",
    "    temp = temp.sort_values(by=['categories'])\n",
    "\n",
    "    freq_6_min_cat = []\n",
    "    for cat in temp.categories.unique():\n",
    "        f = temp[(temp[\"categories\"]==cat)].shape[0]\n",
    "        freq_6_min_cat.append(f)\n",
    "    \n",
    "    trav_times_6min_cat = pd.DataFrame({\"category\":temp.categories.unique(), \"freq\": freq_6_min_cat })\n",
    "    return trav_times_6min_cat \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateSixMinCat(pathToFile, SampleNr):\n",
    "    temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
    "    temp[\"trav_time\"] = pd.to_timedelta(temp.trav_time)\n",
    "    temp[\"trav_time2\"] = temp.trav_time.astype('timedelta64[s]')/  pd.Timedelta(minutes=60)\n",
    "    trav_time_cat = sixMinuteCategoriesUpToTwoHours(temp)\n",
    "    sampleNrAsString = str(SampleNr)\n",
    "    trav_time_cat.rename(columns={\"freq\": \"freq_\" + sampleNrAsString}, inplace = True )\n",
    "    return trav_time_cat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.01\"]\n",
    "storCapF =  [\"0.01\", \"0.03162\"]\n",
    "\n",
    "# initialize empty data frames\n",
    "avg_travel_time_dist_1pct_case1 = []\n",
    "avg_travel_time_dist_1pct_case2 = []\n",
    "avg_travel_time_dist_1pct_case3 = []\n",
    "avg_travel_time_dist_1pct_case4 = []\n",
    "\n",
    "\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "        for sampleNr in range(1,11,1):\n",
    "            # calculate adjusted stuck time\n",
    "            default_stuck_time = 30.0\n",
    "            adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "            # declare sample size as str \"1-pct\"\n",
    "            sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "\n",
    "            if ((fCf == \"0.01\") & (sCf == '0.01')):\n",
    "                # paths for case 1 and 3 \n",
    "                path_case1 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\"+ str(sampleNr) +\"-fCf_sCF_\" + sCf + \"_gS_default_3765/lausitz-1pct-\"+ str(sampleNr) +\"-fCf_sCf_0.01_gS_default_3765.output_trips.csv.gz\"\n",
    "                path_case3 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" +str(sampleNr) +\"-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-1-pct-\" +str(sampleNr) + \"-fCf_sCF_0.01_gS_4711_sT_3000.0_3765.output_trips.csv.gz\"\n",
    "                \n",
    "                # calculate the six minute categories and append them\n",
    "                avg_travel_time_dist_1pct_case1.append(calculateSixMinCat(path_case1, sampleNr))\n",
    "                avg_travel_time_dist_1pct_case3.append(calculateSixMinCat(path_case3, sampleNr))\n",
    "\n",
    "            else:\n",
    "                \n",
    "                # calculate the six minute categories and append them\n",
    "                path_case2 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" +str(sampleNr) + \"-fCf_0.01_sCF_\" + sCf + \"_gS_default_3765/lausitz-1pct-\" + str(sampleNr) + \"-fCf_0.01_sCf_0.03162_gS_default_3765.output_trips.csv.gz\"\n",
    "                path_case4 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" + str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-1-pct-\" + str(sampleNr) + \"-fCf_0.01_sCF_0.03162_gS_4711_sT_3000.0_3765.output_trips.csv.gz\"\n",
    "                \n",
    "                avg_travel_time_dist_1pct_case2.append(calculateSixMinCat(path_case2, sampleNr))\n",
    "                avg_travel_time_dist_1pct_case4.append(calculateSixMinCat(path_case4, sampleNr))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_travel_time_dist_1pct_case1 = pd.DataFrame({'sixMinCategories': avg_travel_time_dist_1pct_case1[0]['category'], 'freq_1': avg_travel_time_dist_1pct_case1[0]['freq_1'] })\n",
    "df_avg_travel_time_dist_1pct_case2 = pd.DataFrame({'sixMinCategories': avg_travel_time_dist_1pct_case2[0]['category'], 'freq_1': avg_travel_time_dist_1pct_case2[0]['freq_1'] })\n",
    "df_avg_travel_time_dist_1pct_case3 = pd.DataFrame({'sixMinCategories': avg_travel_time_dist_1pct_case3[0]['category'], 'freq_1': avg_travel_time_dist_1pct_case3[0]['freq_1'] })\n",
    "df_avg_travel_time_dist_1pct_case4 = pd.DataFrame({'sixMinCategories': avg_travel_time_dist_1pct_case4[0]['category'], 'freq_1': avg_travel_time_dist_1pct_case4[0]['freq_1'] })\n",
    "\n",
    "for sampleNr in range(1,10,1):\n",
    "    colname = \"freq_\" + str(sampleNr + 1)\n",
    "    df_avg_travel_time_dist_1pct_case1.insert(sampleNr + 1, colname,avg_travel_time_dist_1pct_case1[sampleNr][colname] )\n",
    "    df_avg_travel_time_dist_1pct_case2.insert(sampleNr + 1, colname,avg_travel_time_dist_1pct_case2[sampleNr][colname] )\n",
    "    df_avg_travel_time_dist_1pct_case3.insert(sampleNr + 1, colname,avg_travel_time_dist_1pct_case3[sampleNr][colname] )\n",
    "    df_avg_travel_time_dist_1pct_case4.insert(sampleNr + 1, colname,avg_travel_time_dist_1pct_case4[sampleNr][colname] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_case1 = []\n",
    "mean_case2 = []\n",
    "mean_case3 = []\n",
    "mean_case4 = []\n",
    "for category in range(0,21,1):\n",
    "    mean_case1.append(np.mean(df_avg_travel_time_dist_1pct_case1.iloc[category,1:11]))\n",
    "    mean_case2.append(np.mean(df_avg_travel_time_dist_1pct_case2.iloc[category,1:11]))\n",
    "    mean_case3.append(np.mean(df_avg_travel_time_dist_1pct_case3.iloc[category,1:11]))\n",
    "    mean_case4.append(np.mean(df_avg_travel_time_dist_1pct_case4.iloc[category,1:11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_travel_time_dist_1pct_case1.insert(11, 'mean', mean_case1)\n",
    "df_avg_travel_time_dist_1pct_case2.insert(11, 'mean', mean_case2)\n",
    "df_avg_travel_time_dist_1pct_case3.insert(11, 'mean', mean_case3)\n",
    "df_avg_travel_time_dist_1pct_case4.insert(11, 'mean', mean_case4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_travel_time_dist_1pct_case1.insert(12, 'sample_size', \"1-pct\")\n",
    "df_avg_travel_time_dist_1pct_case1.insert(13, 'alpha', 1.0)\n",
    "df_avg_travel_time_dist_1pct_case1.insert(14, 'stuck_time', 30.0)\n",
    "df_avg_travel_time_dist_1pct_case1.insert(15,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_travel_time_dist_1pct_case2.insert(12, 'sample_size', \"1-pct\")\n",
    "df_avg_travel_time_dist_1pct_case2.insert(13, 'alpha', 0.75)\n",
    "df_avg_travel_time_dist_1pct_case2.insert(14, 'stuck_time', 30.0)\n",
    "df_avg_travel_time_dist_1pct_case2.insert(15,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_travel_time_dist_1pct_case3.insert(12, 'sample_size', \"1-pct\")\n",
    "df_avg_travel_time_dist_1pct_case3.insert(13, 'alpha', 1.0)\n",
    "df_avg_travel_time_dist_1pct_case3.insert(14, 'stuck_time', 3000.0)\n",
    "df_avg_travel_time_dist_1pct_case3.insert(15,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_travel_time_dist_1pct_case4.insert(12, 'sample_size', \"1-pct\")\n",
    "df_avg_travel_time_dist_1pct_case4.insert(13, 'alpha', 0.75)\n",
    "df_avg_travel_time_dist_1pct_case4.insert(14, 'stuck_time', 3000.0)\n",
    "df_avg_travel_time_dist_1pct_case4.insert(15,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_travel_time_dist_1pct_all = pd.concat([df_avg_travel_time_dist_1pct_case1, df_avg_travel_time_dist_1pct_case2, df_avg_travel_time_dist_1pct_case3, df_avg_travel_time_dist_1pct_case4], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_travel_time_dist_1pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/trav_time_categories_1pct_cases_1_4.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trav_time_5pct_categories = []\n",
    "for elem in range(1,11,1):\n",
    "    if (elem == 6):\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765/lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765-2.output_trips.csv.gz\"\n",
    "        temp = pd.read_csv(path, compression = \"gzip\", sep = \";\")\n",
    "        temp[\"trav_time\"] = pd.to_timedelta(temp.trav_time)\n",
    "        temp[\"trav_time2\"] = temp.trav_time.astype('timedelta64[s]')/  pd.Timedelta(minutes=60)\n",
    "        trav_time_cat = sixMinuteCategoriesUpToTwoHours(temp)\n",
    "        trav_time_cat.rename(columns={\"freq\": \"freq_\" + str(elem)}, inplace = True )\n",
    "        trav_time_5pct_categories.append(trav_time_cat)\n",
    "    else: \n",
    "        # ERROR WRONG PATH, Now corrected\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(elem) + \"-fCf_sCF_0.05_gS_4711_3765/lausitz-5.0-pct-\" + str(elem) + \"-fCf_sCF_0.05_gS_4711_3765.output_trips.csv.gz\"\n",
    "        temp = pd.read_csv(path, compression= \"gzip\", sep=\";\")\n",
    "        temp[\"trav_time\"] = pd.to_timedelta(temp.trav_time)\n",
    "        temp[\"trav_time2\"] = temp.trav_time.astype('timedelta64[s]')/  pd.Timedelta(minutes=60)\n",
    "        trav_time_cat = sixMinuteCategoriesUpToTwoHours(temp)\n",
    "        trav_time_cat.rename(columns={\"freq\": \"freq_\" + str(elem)}, inplace = True )\n",
    "        trav_time_5pct_categories.append(trav_time_cat)\n",
    "# left join and calculate mean\n",
    "trav_time_5pct_cat_all = pd.merge(trav_time_5pct_categories[0], trav_time_5pct_categories[1], on = ['category'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    trav_time_5pct_cat_all = pd.merge(trav_time_5pct_cat_all, trav_time_5pct_categories[elem], on = ['category'], how='left')\n",
    "\n",
    "trav_time_5pct_cat_all['mean'] = trav_time_5pct_cat_all.iloc[:,1:11].mean(axis = 1)\n",
    "trav_time_5pct_cat_all.insert(12, 'sample_size', \"5-pct\")\n",
    "trav_time_5pct_cat_all.insert(13, 'alpha', 1.0)\n",
    "trav_time_5pct_cat_all.insert(14, 'stuck_time', 30.0)\n",
    "trav_time_5pct_cat_all.insert(15,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14311/1743431341.py:4: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(path, compression= \"gzip\", sep=\";\")\n"
     ]
    }
   ],
   "source": [
    "trav_time_10pct_categories = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" +str(elem) + \"-fCf_sCF_0.1_gS_4711_3765/lausitz-10.0-pct-\" +str(elem) + \"-fCf_sCF_0.1_gS_4711_3765.output_trips.csv.gz\"\n",
    "    temp = pd.read_csv(path, compression= \"gzip\", sep=\";\")\n",
    "    temp[\"trav_time\"] = pd.to_timedelta(temp.trav_time)\n",
    "    temp[\"trav_time2\"] = temp.trav_time.astype('timedelta64[s]')/  pd.Timedelta(minutes=60)\n",
    "    trav_time_cat = sixMinuteCategoriesUpToTwoHours(temp)\n",
    "    trav_time_cat.rename(columns={\"freq\": \"freq_\" + str(elem)}, inplace = True )\n",
    "    trav_time_10pct_categories.append(trav_time_cat)\n",
    "# left join and calculate mean\n",
    "trav_time_10pct_cat_all = pd.merge(trav_time_10pct_categories[0], trav_time_10pct_categories[1], on = ['category'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    trav_time_10pct_cat_all = pd.merge(trav_time_10pct_cat_all, trav_time_10pct_categories[elem], on = ['category'], how='left')\n",
    "\n",
    "trav_time_10pct_cat_all['mean'] = trav_time_10pct_cat_all.iloc[:,1:11].mean(axis = 1)\n",
    "trav_time_10pct_cat_all.insert(12, 'sample_size', \"10-pct\")\n",
    "trav_time_10pct_cat_all.insert(13, 'alpha', 1.0)\n",
    "trav_time_10pct_cat_all.insert(14, 'stuck_time', 30.0)\n",
    "trav_time_10pct_cat_all.insert(15,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_sCF_0.25_gS_4711_3765/lausitz-25.0-pct-fCf_sCF_0.25_gS_4711_3765.output_trips.csv.gz\"\n",
    "t_25pct = pd.read_csv(path, compression= \"gzip\", sep=\";\")\n",
    "t_25pct[\"trav_time\"] = pd.to_timedelta(t_25pct.trav_time)\n",
    "t_25pct[\"trav_time2\"] = t_25pct.trav_time.astype('timedelta64[s]')/  pd.Timedelta(minutes=60)\n",
    "trav_time_25pct_categories = sixMinuteCategoriesUpToTwoHours(t_25pct)\n",
    "trav_time_25pct_categories.rename(columns={\"category\": \"sixMinCategories\"}, inplace = True )\n",
    "trav_time_25pct_categories.rename(columns={\"freq\": \"mean\"}, inplace = True )\n",
    "trav_time_25pct_categories.insert(2, 'sample_size', \"25-pct\")\n",
    "trav_time_25pct_categories.insert(3, 'alpha', 1.0)\n",
    "trav_time_25pct_categories.insert(4, 'stuck_time', 30.0)\n",
    "trav_time_25pct_categories.insert(5,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_sCF_0.5_gS_4711_3765/lausitz-50.0-pct-fCf_sCF_0.5_gS_4711_3765.output_trips.csv.gz\"\n",
    "t_50pct = pd.read_csv(path, compression= \"gzip\", sep=\";\")\n",
    "t_50pct[\"trav_time\"] = pd.to_timedelta(t_50pct.trav_time)\n",
    "t_50pct[\"trav_time2\"] = t_50pct.trav_time.astype('timedelta64[s]')/  pd.Timedelta(minutes=60)\n",
    "trav_time_50pct_categories = sixMinuteCategoriesUpToTwoHours(t_50pct)\n",
    "trav_time_50pct_categories.rename(columns={\"category\": \"sixMinCategories\"}, inplace = True )\n",
    "trav_time_50pct_categories.rename(columns={\"freq\": \"mean\"}, inplace = True )\n",
    "trav_time_50pct_categories.insert(2, 'sample_size', \"50-pct\")\n",
    "trav_time_50pct_categories.insert(3, 'alpha', 1.0)\n",
    "trav_time_50pct_categories.insert(4, 'stuck_time', 30.0)\n",
    "trav_time_50pct_categories.insert(5,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25 pct doubled\n",
    "path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25-pct-doubled-fCf_0.5_sCF_0.5_gS_4711_3765/lausitz-25-pct-doubled-fCf_0.5_sCF_0.5_gS_4711__3765.output_trips.csv.gz\"\n",
    "t_25pct_doubled = pd.read_csv(path, compression= \"gzip\", sep=\";\")\n",
    "t_25pct_doubled[\"trav_time\"] = pd.to_timedelta(t_25pct_doubled.trav_time)\n",
    "t_25pct_doubled[\"trav_time2\"] = t_25pct_doubled.trav_time.astype('timedelta64[s]')/  pd.Timedelta(minutes=60)\n",
    "trav_time_25pct_doubled_categories = sixMinuteCategoriesUpToTwoHours(t_25pct_doubled)\n",
    "trav_time_25pct_doubled_categories.rename(columns={\"category\": \"sixMinCategories\"}, inplace = True )\n",
    "trav_time_25pct_doubled_categories.rename(columns={\"freq\": \"mean\"}, inplace = True )\n",
    "trav_time_25pct_doubled_categories.insert(2, 'sample_size', \"25-pct-doubled\")\n",
    "trav_time_25pct_doubled_categories.insert(3, 'alpha', 1.0)\n",
    "trav_time_25pct_doubled_categories.insert(4, 'stuck_time', 30.0)\n",
    "trav_time_25pct_doubled_categories.insert(5,'global_seed', \"default\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765/lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765.output_trips.csv.gz\"\n",
    "t_100pct = pd.read_csv(path, compression= \"gzip\", sep=\";\")\n",
    "t_100pct[\"trav_time\"] = pd.to_timedelta(t_100pct.trav_time)\n",
    "t_100pct[\"trav_time2\"] = t_100pct.trav_time.astype('timedelta64[s]')/  pd.Timedelta(minutes=60)\n",
    "trav_time_100pct_categories = sixMinuteCategoriesUpToTwoHours(t_100pct)\n",
    "trav_time_100pct_categories.rename(columns={\"category\": \"sixMinCategories\"}, inplace = True )\n",
    "trav_time_100pct_categories.rename(columns={\"freq\": \"mean\"}, inplace = True )\n",
    "trav_time_100pct_categories.insert(2, 'sample_size', \"100-pct\")\n",
    "trav_time_100pct_categories.insert(3, 'alpha', 1.0)\n",
    "trav_time_100pct_categories.insert(4, 'stuck_time', 30.0)\n",
    "trav_time_100pct_categories.insert(5,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14311/3221003958.py:3: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  t_25pct_quadrupled = pd.read_csv(path, compression= \"gzip\", sep=\";\")\n"
     ]
    }
   ],
   "source": [
    "# 25 pct quadrupled\n",
    "path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25.0-pct-quadrupled-fCf_1.0_sCF_1.0_gS_4711_3765/lausitz-25-pct-quadrupled-fCf_1.0_sCF_1.0_gS_4711__3765.output_trips.csv.gz\"\n",
    "t_25pct_quadrupled = pd.read_csv(path, compression= \"gzip\", sep=\";\")\n",
    "t_25pct_quadrupled[\"trav_time\"] = pd.to_timedelta(t_25pct_quadrupled.trav_time)\n",
    "t_25pct_quadrupled[\"trav_time2\"] = t_25pct_quadrupled.trav_time.astype('timedelta64[s]')/  pd.Timedelta(minutes=60)\n",
    "trav_time_25pct_quadrupled_categories = sixMinuteCategoriesUpToTwoHours(t_25pct_quadrupled)\n",
    "trav_time_25pct_quadrupled_categories.rename(columns={\"category\": \"sixMinCategories\"}, inplace = True )\n",
    "trav_time_25pct_quadrupled_categories.rename(columns={\"freq\": \"mean\"}, inplace = True )\n",
    "trav_time_25pct_quadrupled_categories.insert(2, 'sample_size', \"25-pct-quadrupled\")\n",
    "trav_time_25pct_quadrupled_categories.insert(3, 'alpha', 1.0)\n",
    "trav_time_25pct_quadrupled_categories.insert(4, 'stuck_time', 30.0)\n",
    "trav_time_25pct_quadrupled_categories.insert(5,'global_seed', \"default\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concat and write csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trav_time_categories_all = pd.concat([df_avg_travel_time_dist_1pct_all, trav_time_5pct_cat_all, trav_time_10pct_cat_all,  trav_time_25pct_categories, trav_time_50pct_categories,trav_time_25pct_doubled_categories, trav_time_100pct_categories, trav_time_25pct_quadrupled_categories], axis = 0)\n",
    "trav_time_categories_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/trav_time_categories_1_100.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Network Congestion Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 pct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nci_1pct = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-1pct-\"+ str(elem) + \"-fCf_sCF_0.01_gS_default_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\" \n",
    "    temp = pd.read_csv(path, sep=\",\")\n",
    "    temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "    nci_1pct.append(temp)\n",
    "\n",
    "nci_1pct_df = pd.merge(nci_1pct[0], nci_1pct[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    nci_1pct_df = pd.merge(nci_1pct_df, nci_1pct[elem], on = ['road_type', 'hour'], how='left')\n",
    "\n",
    "nci_1pct_df['congestion_index_mean'] = nci_1pct_df.iloc[: ,2:11].mean(axis=1)\n",
    "nci_1pct_df.insert(13, 'sample_size', \"1-pct\")\n",
    "nci_1pct_df.insert(14, 'alpha', 1.0)\n",
    "nci_1pct_df.insert(15, 'stuck_time', 30.0)\n",
    "nci_1pct_df.insert(16,'global_seed', \"default\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nci_1pct_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "    #       /home/lola/math_cluster/output/output-lausitz-1pct-1-fCf_0.01_sCF_0.03162_gS_default_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-1pct-\"+ str(elem) + \"-fCf_0.01_sCF_0.03162_gS_default_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\" \n",
    "    temp = pd.read_csv(path, sep=\",\")\n",
    "    temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "    nci_1pct_sCf.append(temp)\n",
    "\n",
    "nci_1pct_sCf_df = pd.merge(nci_1pct_sCf[0], nci_1pct_sCf[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    nci_1pct_sCf_df = pd.merge(nci_1pct_sCf_df, nci_1pct_sCf[elem], on = ['road_type', 'hour'], how='left')\n",
    "\n",
    "nci_1pct_sCf_df['congestion_index_mean'] = nci_1pct_sCf_df.iloc[: ,2:11].mean(axis=1)\n",
    "nci_1pct_sCf_df.insert(13, 'sample_size', \"1-pct\")\n",
    "nci_1pct_sCf_df.insert(14, 'alpha', 0.75)\n",
    "nci_1pct_sCf_df.insert(15, 'stuck_time', 30.0)\n",
    "nci_1pct_sCf_df.insert(16,'global_seed', \"default\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random global seed\n",
    "nci_1pct_rGs = []\n",
    "rGs = [ 4711,3254, 2306, 6384,4338, 6003, 5502, 9377, 5621, 9002 ]\n",
    "for seed in rGs:\n",
    "    if (seed == 4711):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        temp = nci_1pct_df[[\"road_type\", \"hour\", \"congestion_index_1\"]].copy()\n",
    "        nci_1pct_rGs.append(temp)\n",
    "    elif (seed == 3254):\n",
    "        global_seed  = \"rnd_\" + str(seed)\n",
    "        path =\"/home/lola/math_cluster/output/output-lausitz-1pct-1-fCf_sCF_0.01_gS_3254_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "        temp = pd.read_csv(path, sep = \",\")\n",
    "        temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(rGs.index(seed) + 1)}, inplace = True)\n",
    "        nci_1pct_rGs.append(temp)\n",
    "    else:\n",
    "        global_seed  = \"rnd_\" + str(seed)\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-1.0-pct-1-fCf_sCF_0.01_gS_\"+str(seed) + \"_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "        temp = pd.read_csv(path, sep = \",\")\n",
    "        temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(rGs.index(seed) + 1 )}, inplace = True)\n",
    "        nci_1pct_rGs.append(temp)\n",
    "\n",
    "nci_1pct_rGs_df = pd.merge(nci_1pct_rGs[0], nci_1pct_rGs[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    nci_1pct_rGs_df = pd.merge(nci_1pct_rGs_df, nci_1pct_rGs[elem], on = ['road_type', 'hour'], how='left')\n",
    "\n",
    "nci_1pct_rGs_df['congestion_index_mean'] = nci_1pct_rGs_df.iloc[: ,2:11].mean(axis=1)\n",
    "nci_1pct_rGs_df.insert(13, 'sample_size', \"1-pct\")\n",
    "nci_1pct_rGs_df.insert(14, 'alpha', 1.0)\n",
    "nci_1pct_rGs_df.insert(15, 'stuck_time', 30.0)\n",
    "nci_1pct_rGs_df.insert(16,'global_seed', \"rnd\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nci_1pct_sT = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" + str(elem) + \"-fCf_sCF_0.01_gS_4711_sT_3000.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\" \n",
    "    temp = pd.read_csv(path, sep=\",\")\n",
    "    temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "    nci_1pct_sT.append(temp)\n",
    "    \n",
    "nci_1pct_sT_df = pd.merge(nci_1pct_sT[0], nci_1pct_sT[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    nci_1pct_sT_df = pd.merge(nci_1pct_sT_df, nci_1pct_sT[elem], on = ['road_type', 'hour'], how='left')    \n",
    "    \n",
    "nci_1pct_sT_df['congestion_index_mean'] = nci_1pct_sT_df.iloc[: ,2:11].mean(axis=1)\n",
    "nci_1pct_sT_df.insert(13, 'sample_size', \"1-pct\")\n",
    "nci_1pct_sT_df.insert(14, 'alpha', 1.0)\n",
    "nci_1pct_sT_df.insert(15, 'stuck_time', 3000.0)\n",
    "nci_1pct_sT_df.insert(16,'global_seed', \"default\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nci_1pct_sT_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" + str(elem) + \"-fCf_0.01_sCF_0.03162_gS_4711_sT_3000.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\" \n",
    "    temp = pd.read_csv(path, sep=\",\")\n",
    "    temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "    nci_1pct_sT_sCf.append(temp)\n",
    "    \n",
    "nci_1pct_sT_sCf_df = pd.merge(nci_1pct_sT_sCf[0], nci_1pct_sT_sCf[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    nci_1pct_sT_sCf_df = pd.merge(nci_1pct_sT_sCf_df, nci_1pct_sT_sCf[elem], on = ['road_type', 'hour'], how='left')    \n",
    "    \n",
    "nci_1pct_sT_sCf_df['congestion_index_mean'] = nci_1pct_sT_sCf_df.iloc[: ,2:11].mean(axis=1)\n",
    "nci_1pct_sT_sCf_df.insert(13, 'sample_size', \"1-pct\")\n",
    "nci_1pct_sT_sCf_df.insert(14, 'alpha', 0.75)\n",
    "nci_1pct_sT_sCf_df.insert(15, 'stuck_time', 3000.0)\n",
    "nci_1pct_sT_sCf_df.insert(16,'global_seed', \"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nci_1pct_all = pd.concat([nci_1pct_df, nci_1pct_sCf_df, nci_1pct_rGs_df,  nci_1pct_sT_df, nci_1pct_sT_sCf_df ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nci_1pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/nci_all_1pct_samples.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 pct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nci_5pct = []\n",
    "for elem in range(1,11,1):\n",
    "    if (elem ==6): \n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "        temp = pd.read_csv(path, sep=\",\")\n",
    "        temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "        nci_5pct.append(temp)\n",
    "\n",
    "\n",
    "    else:\n",
    "    #       /home/lola/math_cluster/output/output-lausitz-5.0-pct-1-fCf_sCF_0.05_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\"+ str(elem) + \"-fCf_sCF_0.05_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\" \n",
    "        temp = pd.read_csv(path, sep=\",\")\n",
    "        temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "        nci_5pct.append(temp)\n",
    "\n",
    "\n",
    "\n",
    "nci_5pct_df = pd.merge(nci_5pct[0], nci_5pct[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    nci_5pct_df = pd.merge(nci_5pct_df, nci_5pct[elem], on = ['road_type', 'hour'], how='left')    \n",
    "    \n",
    "nci_5pct_df['congestion_index_mean'] = nci_5pct_df.iloc[: ,2:11].mean(axis=1)\n",
    "nci_5pct_df.insert(13, 'sample_size', \"5-pct\")\n",
    "nci_5pct_df.insert(14, 'alpha', 1.0)\n",
    "nci_5pct_df.insert(15, 'stuck_time', 30.0)\n",
    "nci_5pct_df.insert(16,'global_seed', \"default\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nci_5pct_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "    if (elem == 6):\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_0.05_sCF_0.10574_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\" \n",
    "        temp = pd.read_csv(path, sep=\",\")\n",
    "        temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "        nci_5pct_sCf.append(temp)\n",
    "\n",
    "    else: \n",
    "    #           /home/lola/math_cluster/output/output-lausitz-5.0-pct-7-fCf_0.05_sCF_0.10574_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\"+ str(elem) + \"-fCf_0.05_sCF_0.10574_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\" \n",
    "        temp = pd.read_csv(path, sep=\",\")\n",
    "        temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "        nci_5pct_sCf.append(temp)\n",
    "\n",
    "nci_5pct_sCf_df = pd.merge(nci_5pct_sCf[0], nci_5pct_sCf[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    nci_5pct_sCf_df = pd.merge(nci_5pct_sCf_df, nci_5pct_sCf[elem], on = ['road_type', 'hour'], how='left')    \n",
    "    \n",
    "nci_5pct_sCf_df['congestion_index_mean'] = nci_5pct_sCf_df.iloc[: ,2:11].mean(axis=1)\n",
    "nci_5pct_sCf_df.insert(13, 'sample_size', \"5-pct\")\n",
    "nci_5pct_sCf_df.insert(14, 'alpha',0.75)\n",
    "nci_5pct_sCf_df.insert(15, 'stuck_time', 30.0)\n",
    "nci_5pct_sCf_df.insert(16,'global_seed', \"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random global seed\n",
    "nci_5pct_rGs = []\n",
    "\n",
    "rGs = [ 4711,3254, 2306, 6384,4338, 6003, 5502, 9377, 5621, 9002 ]\n",
    "for seed in rGs:\n",
    "    if (seed == 4711):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        temp = nci_5pct_df[[\"road_type\", \"hour\", \"congestion_index_1\"]].copy()\n",
    "        nci_5pct_rGs.append(temp)\n",
    "    elif (seed == 3254):\n",
    "        global_seed  = \"rnd_\" + str(seed)\n",
    "        path =\"/home/lola/math_cluster/output/output-lausitz-5.0-pct-1-fCf_sCF_0.05_gS_3254_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "        temp = pd.read_csv(path, sep = \",\")\n",
    "        temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(rGs.index(seed) + 1)}, inplace = True)\n",
    "        nci_5pct_rGs.append(temp)\n",
    "        \n",
    "    else:\n",
    "        if (seed == 6384 or seed == 6003):\n",
    "            continue\n",
    "        global_seed  = \"rnd_\" + str(seed)\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-1-fCf_sCF_0.05_gS_\" + str(seed) + \"_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "        temp = pd.read_csv(path, sep = \",\")\n",
    "        temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(rGs.index(seed) + 1)}, inplace = True)\n",
    "        nci_5pct_rGs.append(temp)\n",
    "\n",
    "\n",
    "nci_5pct_rGs_df = pd.merge(nci_5pct_rGs[0], nci_5pct_rGs[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,8,1):\n",
    "    nci_5pct_rGs_df = pd.merge(nci_5pct_rGs_df, nci_5pct_rGs[elem], on = ['road_type', 'hour'], how='left')\n",
    "\n",
    "nci_5pct_rGs_df['congestion_index_mean'] = nci_5pct_rGs_df.iloc[: ,2:9].mean(axis=1)\n",
    "nci_5pct_rGs_df.insert(11, 'sample_size', \"5-pct\")\n",
    "nci_5pct_rGs_df.insert(12, 'alpha', 1.0)\n",
    "nci_5pct_rGs_df.insert(13, 'stuck_time', 30.0)\n",
    "nci_5pct_rGs_df.insert(14,'global_seed', \"rnd\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 pct, alpha 1.0, sT scaled\n",
    "nci_5pct_sT = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str(elem) + \"-fCf_sCF_0.05_gS_4711_sT_600.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "    temp = pd.read_csv(path)\n",
    "    temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "    nci_5pct_sT.append(temp)\n",
    "\n",
    "nci_5pct_sT_df = pd.merge(nci_5pct_sT[0], nci_5pct_sT[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    nci_5pct_sT_df = pd.merge(nci_5pct_sT_df, nci_5pct_sT[elem], on = ['road_type', 'hour'], how='left')\n",
    "\n",
    "\n",
    "nci_5pct_sT_df['congestion_index_mean'] = nci_5pct_sT_df.iloc[: ,2:11].mean(axis=1)\n",
    "nci_5pct_sT_df.insert(13, 'sample_size', \"5-pct\")\n",
    "nci_5pct_sT_df.insert(14, 'alpha',1.0)\n",
    "nci_5pct_sT_df.insert(15, 'stuck_time', 600.0)\n",
    "nci_5pct_sT_df.insert(16,'global_seed', \"default\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nci_5pct_sT_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str(elem) + \"-fCf_0.05_sCF_0.10574_gS_4711_sT_600.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "    temp = pd.read_csv(path)\n",
    "    temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "    nci_5pct_sT_sCf.append(temp)\n",
    "\n",
    "nci_5pct_sT_sCf_df = pd.merge(nci_5pct_sT_sCf[0], nci_5pct_sT_sCf[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    nci_5pct_sT_sCf_df = pd.merge(nci_5pct_sT_sCf_df, nci_5pct_sT_sCf[elem], on = ['road_type', 'hour'], how='left')\n",
    "    \n",
    "nci_5pct_sT_sCf_df['congestion_index_mean'] = nci_5pct_sT_sCf_df.iloc[: ,2:11].mean(axis=1)\n",
    "nci_5pct_sT_sCf_df.insert(13, 'sample_size', \"5-pct\")\n",
    "nci_5pct_sT_sCf_df.insert(14, 'alpha',0.75)\n",
    "nci_5pct_sT_sCf_df.insert(15, 'stuck_time', 600.0)\n",
    "nci_5pct_sT_sCf_df.insert(16,'global_seed', \"default\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nci_5pct_all = pd.concat([nci_5pct_df, nci_5pct_sCf_df,nci_5pct_rGs_df, nci_5pct_sT_df, nci_5pct_sT_sCf_df], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nci_5pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/nci_all_5pct_samples.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 pct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flowCapF = [\"0.1\"]\n",
    "storCapF =  [\"0.1\", \"0.17783\"]\n",
    "stuckTimes = [\"30.0\", \"300.0\"]\n",
    "\n",
    "nci_10pct = pd.DataFrame()\n",
    "\n",
    "counter = 0\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "        for sT in stuckTimes:\n",
    "            for sampleNr in range(1,11,1):\n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.1\") & (sCf == \"0.1\") & (sT == \"30.0\")):\n",
    "                    path = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" +str(sampleNr) + \"-fCf_sCF_0.1_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"       \n",
    "                elif((fCf == \"0.1\") & (sCf ==  \"0.17783\") & (sT == \"30.0\")):\n",
    "                    path = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" + str(sampleNr) + \"-fCf_0.1_sCF_0.17783_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "                elif((fCf == \"0.1\") & (sCf == \"0.1\") & (sT == \"300.0\")):\n",
    "                    path = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\" + str(sampleNr) + \"-fCf_sCF_0.1_gS_4711_sT_300.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "                elif((fCf == \"0.1\") & (sCf ==  \"0.17783\") & (sT == \"300.0\")):\n",
    "                    path = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\" + str(sampleNr) + \"-fCf_0.1_sCF_0.17783_gS_4711_sT_300.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"  \n",
    "                else: \n",
    "                    print(\"case not found\")\n",
    "                    break\n",
    "                temp = pd.read_csv(path, compression= \"gzip\", sep=\";\")\n",
    "                temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nci_10pct = []\n",
    "for elem in range(1,11,1):\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\"+ str(elem) + \"-fCf_sCF_0.1_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\" \n",
    "        temp = pd.read_csv(path)\n",
    "        temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "        nci_10pct.append(temp)\n",
    "\n",
    "nci_10pct_df = pd.merge(nci_10pct[0], nci_10pct[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    nci_10pct_df = pd.merge(nci_10pct_df, nci_10pct[elem], on = ['road_type', 'hour'], how='left')\n",
    "    \n",
    "nci_10pct_df['congestion_index_mean'] = nci_10pct_df.iloc[: ,2:11].mean(axis=1)        \n",
    "nci_10pct_df.insert(13, 'sample_size', \"10-pct\")\n",
    "nci_10pct_df.insert(14, 'alpha',1.0)\n",
    "nci_10pct_df.insert(15, 'stuck_time', 30.0)\n",
    "nci_10pct_df.insert(16,'global_seed', \"default\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nci_10pct_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\"+ str(elem) + \"-fCf_0.1_sCF_0.17783_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\" \n",
    "        temp = pd.read_csv(path)\n",
    "        temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "        nci_10pct_sCf.append(temp)\n",
    "\n",
    "nci_10pct_sCf_df = pd.merge(nci_10pct_sCf[0], nci_10pct_sCf[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    nci_10pct_sCf_df = pd.merge(nci_10pct_sCf_df, nci_10pct_sCf[elem], on = ['road_type', 'hour'], how='left')\n",
    "\n",
    "\n",
    "nci_10pct_sCf_df['congestion_index_mean'] = nci_10pct_sCf_df.iloc[: ,2:11].mean(axis=1)  \n",
    "nci_10pct_sCf_df.insert(13, 'sample_size', \"10-pct\")\n",
    "nci_10pct_sCf_df.insert(14, 'alpha',0.75)\n",
    "nci_10pct_sCf_df.insert(15, 'stuck_time', 30.0)\n",
    "nci_10pct_sCf_df.insert(16,'global_seed', \"default\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nci_10pct_sT = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\"+ str(elem) + \"-fCf_sCF_0.1_gS_4711_sT_300.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "    temp = pd.read_csv(path, sep=\",\")\n",
    "    temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "    nci_10pct_sT.append(temp)\n",
    "\n",
    "nci_10pct_sT_df = pd.merge(nci_10pct_sT[0], nci_10pct_sT[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    nci_10pct_sT_df = pd.merge(nci_10pct_sT_df, nci_10pct_sT[elem], on = ['road_type', 'hour'], how='left')\n",
    "\n",
    "\n",
    "nci_10pct_sT_df['congestion_index_mean'] = nci_10pct_sT_df.iloc[: ,2:11].mean(axis=1)  \n",
    "\n",
    "\n",
    "nci_10pct_sT_df.insert(13, 'sample_size', \"10-pct\")\n",
    "nci_10pct_sT_df.insert(14, 'alpha',1.0)\n",
    "nci_10pct_sT_df.insert(15, 'stuck_time', 300.0)\n",
    "nci_10pct_sT_df.insert(16,'global_seed', \"default\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nci_10pct_sT_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "    if (elem == 2):\n",
    "        path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-10-pct-2-fCf_0.1_sCF_0.17783_gS_4711_sT_300.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "        temp = pd.read_csv(path, sep=\",\")\n",
    "        temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "        nci_10pct_sT_sCf.append(temp)\n",
    "    else:\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\" +str(elem) + \"-fCf_0.1_sCF_0.17783_gS_4711_sT_300.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "        temp = pd.read_csv(path, sep=\",\")\n",
    "        temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "        nci_10pct_sT_sCf.append(temp)\n",
    "\n",
    "\n",
    "nci_10pct_sT_sCf_df = pd.merge(nci_10pct_sT_sCf[0], nci_10pct_sT_sCf[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,9,1):\n",
    "    nci_10pct_sT_sCf_df = pd.merge(nci_10pct_sT_sCf_df, nci_10pct_sT_sCf[elem], on = ['road_type', 'hour'], how='left')\n",
    "\n",
    "\n",
    "nci_10pct_sT_sCf_df['congestion_index_mean'] = nci_10pct_sT_sCf_df.iloc[: ,2:10].mean(axis=1) \n",
    "nci_10pct_sT_sCf_df.insert(12, 'sample_size', \"10-pct\")\n",
    "nci_10pct_sT_sCf_df.insert(13, 'alpha',0.75)\n",
    "nci_10pct_sT_sCf_df.insert(14, 'stuck_time', 300.0)\n",
    "nci_10pct_sT_sCf_df.insert(15,'global_seed', \"default\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nci_10pct_all = pd.concat([nci_10pct_df, nci_10pct_sCf_df, nci_10pct_sT_df, nci_10pct_sT_sCf_df], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nci_10pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/nci_10pct_samples_some_missing.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25 pct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_sCF_0.25_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "nci_25pct = pd.DataFrame(pd.read_csv(path, sep = \",\"))\n",
    "nci_25pct.rename(columns={\"congestion_index\": \"congestion_index_mean\"}, inplace = True)\n",
    "nci_25pct.insert(3, 'sample_size', \"25-pct\")\n",
    "nci_25pct.insert(4, 'alpha',1.0)\n",
    "nci_25pct.insert(5, 'stuck_time', 30.0)\n",
    "nci_25pct.insert(6,'global_seed', \"default\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_0.25_sCF_0.35355_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "nci_25pct_sCf = pd.DataFrame(pd.read_csv(path, sep = \",\"))\n",
    "nci_25pct_sCf.rename(columns={\"congestion_index\": \"congestion_index_mean\"}, inplace = True)\n",
    "nci_25pct_sCf.insert(3, 'sample_size', \"25-pct\")\n",
    "nci_25pct_sCf.insert(4, 'alpha',0.75)\n",
    "nci_25pct_sCf.insert(5, 'stuck_time', 30.0)\n",
    "nci_25pct_sCf.insert(6,'global_seed', \"default\")\n",
    "\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-25-pct-1-fCf_sCF_0.25_gS_4711_sT_120.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "nci_25pct_sT = pd.DataFrame(pd.read_csv(path, sep = \",\"))\n",
    "nci_25pct_sT.rename(columns={\"congestion_index\": \"congestion_index_mean\"}, inplace = True)\n",
    "nci_25pct_sT.insert(3, 'sample_size', \"25-pct\")\n",
    "nci_25pct_sT.insert(4, 'alpha',1.0)\n",
    "nci_25pct_sT.insert(5, 'stuck_time',120.0)\n",
    "nci_25pct_sT.insert(6,'global_seed', \"default\")\n",
    "\n",
    "\n",
    "path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25-pct-1-fCf_0.25_sCF_0.35355_gS_4711_sT_120.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "nci_25pct_sT_sCf = pd.DataFrame(pd.read_csv(path, sep = \",\"))\n",
    "nci_25pct_sT_sCf.insert(3, 'sample_size', \"25-pct\")\n",
    "nci_25pct_sT_sCf.insert(4, 'sample_nr', 1)\n",
    "nci_25pct_sT_sCf.insert(5, 'alpha',0.75)\n",
    "nci_25pct_sT_sCf.insert(6, 'stuck_time',120.0)\n",
    "nci_25pct_sT_sCf.insert(7,'global_seed', \"default\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nci_25pct_all = pd.concat([nci_25pct, nci_25pct_sCf, nci_25pct_sT, nci_25pct_sT_sCf], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nci_25pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/nci_25pct_samples_sCf_sT_missing.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50 pct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_sCF_0.5_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "nci_50pct = pd.DataFrame(pd.read_csv(path, sep = \",\"))\n",
    "nci_50pct.rename(columns={\"congestion_index\": \"congestion_index_mean\"}, inplace = True)\n",
    "nci_50pct.insert(3, 'sample_size', \"50-pct\")\n",
    "nci_50pct.insert(4, 'alpha',1.0)\n",
    "nci_50pct.insert(5, 'stuck_time', 30.0)\n",
    "nci_50pct.insert(6,'global_seed', \"default\")\n",
    "\n",
    "\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_0.5_sCF_0.5946_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "nci_50pct_sCf = pd.DataFrame(pd.read_csv(path, sep = \",\"))\n",
    "nci_25pct_sCf.rename(columns={\"congestion_index\": \"congestion_index_mean\"}, inplace = True)\n",
    "nci_50pct_sCf.insert(3, 'sample_size', \"50-pct\")\n",
    "nci_50pct_sCf.insert(4, 'alpha',0.75)\n",
    "nci_50pct_sCf.insert(5, 'stuck_time', 30.0)\n",
    "nci_50pct_sCf.insert(6,'global_seed', \"default\")\n",
    "\n",
    "\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_sCF_0.5_gS_4711_sT_60.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "nci_50pct_sT = pd.DataFrame(pd.read_csv(path, sep = \",\"))\n",
    "nci_50pct_sT.rename(columns={\"congestion_index\": \"congestion_index_mean\"}, inplace = True)\n",
    "nci_50pct_sT.insert(3, 'sample_size', \"50-pct\")\n",
    "nci_50pct_sT.insert(4, 'alpha',1.0)\n",
    "nci_50pct_sT.insert(5, 'stuck_time', 60.0)\n",
    "nci_50pct_sT.insert(6,'global_seed', \"default\")\n",
    "\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_0.5_sCF_0.5946_gS_4711_sT_60.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "nci_50pct_sT_sCf = pd.DataFrame(pd.read_csv(path, sep = \",\"))\n",
    "nci_50pct_sT_sCf.rename(columns={\"congestion_index\": \"congestion_index_mean\"}, inplace = True)\n",
    "nci_50pct_sT_sCf.insert(3, 'sample_size', \"50-pct\")\n",
    "nci_50pct_sT_sCf.insert(4, 'alpha',0.75)\n",
    "nci_50pct_sT_sCf.insert(5, 'stuck_time', 60.0)\n",
    "nci_50pct_sT_sCf.insert(6,'global_seed', \"default\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25-pct-doubled-fCf_0.5_sCF_0.5_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "nci_25pct_doubled = pd.DataFrame(pd.read_csv(path, sep = \",\"))\n",
    "nci_25pct_doubled.rename(columns={\"congestion_index\": \"congestion_index_mean\"}, inplace = True)\n",
    "nci_25pct_doubled.insert(3, 'sample_size', \"25-pct-doubled\")\n",
    "nci_25pct_doubled.insert(4, 'alpha',1.0)\n",
    "nci_25pct_doubled.insert(5, 'stuck_time', 30.0)\n",
    "nci_25pct_doubled.insert(6,'global_seed', \"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nci_50pct_all = pd.concat([nci_50pct, nci_50pct_sCf, nci_50pct_sT, nci_50pct_sT_sCf, nci_25pct_doubled], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nci_50pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/nci_50pct_samples.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100 pct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "nci_100pct = pd.DataFrame(pd.read_csv(path, sep = \",\"))\n",
    "nci_100pct.rename(columns={\"congestion_index\": \"congestion_index_mean\"}, inplace = True)\n",
    "nci_100pct.insert(3, 'sample_size', \"100-pct\")\n",
    "nci_100pct.insert(4, 'alpha',1.0)\n",
    "nci_100pct.insert(5, 'stuck_time', 30.0)\n",
    "nci_100pct.insert(6,'global_seed', \"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25.0-pct-quadrupled-fCf_1.0_sCF_1.0_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "nci_25pct_quadrupled = pd.DataFrame(pd.read_csv(path, sep = \",\"))\n",
    "nci_25pct_quadrupled.rename(columns={\"congestion_index\": \"congestion_index_mean\"}, inplace = True)\n",
    "nci_25pct_quadrupled.insert(3, 'sample_size', \"25-pct-quadrupled\")\n",
    "nci_25pct_quadrupled.insert(4, 'alpha',1.0)\n",
    "nci_25pct_quadrupled.insert(5, 'stuck_time', 30.0)\n",
    "nci_25pct_quadrupled.insert(6,'global_seed', \"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concat and write csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nci_all = pd.concat([nci_1pct_all, nci_5pct_all, nci_10pct_all, nci_25pct_all, nci_50pct_all, nci_100pct, nci_25pct_quadrupled], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nci_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/nci_100pct_only_5pct_rGs_6384_6003_missing.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data on one motorway, primary and residential link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readStatsByLinkAndReturnValue(pathToFile, LinkId, sampleSize, sampleNr, alpha, stuckTime ):\n",
    "    scaling_factor_to_100pct = 100.0/ sampleSize\n",
    "    df = pd.read_csv(pathToFile)\n",
    "    temp = df[df['link_id'] == LinkId].copy()\n",
    "    temp['road_capacity_utilization'] = temp['road_capacity_utilization']*scaling_factor_to_100pct\n",
    "    temp['simulated_traffic_volume'] = temp['simulated_traffic_volume']*scaling_factor_to_100pct\n",
    "    temp['vol_car'] = temp['vol_car']*scaling_factor_to_100pct\n",
    "    temp.insert(9,'sample_nr', sampleNr)\n",
    "    temp.insert(10, 'sample_size', str(sampleSize) + \"-pct\")\n",
    "    temp.insert(11, 'alpha', alpha)\n",
    "    temp.insert(12, 'stuck_time', stuckTime)\n",
    "    return temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readStatsByLinkAndReturnShortCongLinks(pathToFile, sampleSize, sampleNr, alpha, stuckTime, cI_value, link_length):\n",
    "    scaling_factor_to_100pct = 100.0/ sampleSize\n",
    "    df = pd.read_csv(pathToFile)\n",
    "    temp = df[(df['congestion_index']<= cI_value) & (df['lane_km'] <= link_length)].copy()\n",
    "    temp['road_capacity_utilization'] = temp['road_capacity_utilization']*scaling_factor_to_100pct\n",
    "    temp['simulated_traffic_volume'] = temp['simulated_traffic_volume']*scaling_factor_to_100pct\n",
    "    temp['vol_car'] = temp['vol_car']*scaling_factor_to_100pct\n",
    "    temp.insert(9,'sample_nr', sampleNr)\n",
    "    temp.insert(10, 'sample_size', str(sampleSize) + \"-pct\")\n",
    "    temp.insert(11, 'alpha', alpha)\n",
    "    temp.insert(12, 'stuck_time', stuckTime)\n",
    "    return temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.01\"]\n",
    "storCapF =  [\"0.01\", \"0.03162\"]\n",
    "\n",
    "# initialize empty data frames\n",
    "df_3_links_mw_pr_res_1pct = pd.DataFrame()\n",
    "df_link_cong_len_leq_100m_1pct = pd.DataFrame()\n",
    "\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "        for sampleNr in range(1,11,1):\n",
    "            # calculate adjusted stuck time\n",
    "            default_stuck_time = 30.0\n",
    "            adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "            # declare sample size as str \"1-pct\"\n",
    "            sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "            sample_size = float(fCf)*100\n",
    "\n",
    "            if ((fCf == \"0.01\") & (sCf == '0.01')):\n",
    "                # declare alpha\n",
    "                alpha = 1.0\n",
    "                # paths for case 1 and 3 \n",
    "                path_case1 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\"+ str(sampleNr) +\"-fCf_sCF_\" + sCf + \"_gS_default_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                path_case3 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" +str(sampleNr) +\"-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                \n",
    "                temp_mw_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                temp_mw_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                temp_pr_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                temp_pr_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                temp_rs_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                temp_rs_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                # concatenate with existing values\n",
    "                df_3_links_mw_pr_res_1pct = pd.concat([df_3_links_mw_pr_res_1pct, temp_mw_case1, temp_mw_case3, temp_pr_case1, temp_pr_case3, temp_rs_case1, temp_rs_case3 ], ignore_index= True)\n",
    "\n",
    "                temp_case1_C100m = readStatsByLinkAndReturnShortCongLinks(path_case1, sample_size, sampleNr, alpha, default_stuck_time, 0.5, 0.1)\n",
    "                temp_case3_C100m = readStatsByLinkAndReturnShortCongLinks(path_case3, sample_size, sampleNr, alpha, adjusted_stuck_time, 0.5, 0.1)\n",
    "\n",
    "                df_link_cong_len_leq_100m_1pct = pd.concat([df_link_cong_len_leq_100m_1pct, temp_case1_C100m, temp_case3_C100m ], ignore_index= True)\n",
    "\n",
    "\n",
    "            else:\n",
    "                alpha = 0.75\n",
    "                path_case2 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" +str(sampleNr) + \"-fCf_0.01_sCF_\" + sCf + \"_gS_default_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                path_case4 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" + str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                \n",
    "                temp_mw_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                temp_mw_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                temp_pr_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                temp_pr_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                temp_rs_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                temp_rs_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                # concatenate with existing values\n",
    "                df_3_links_mw_pr_res_1pct = pd.concat([df_3_links_mw_pr_res_1pct, temp_mw_case2, temp_mw_case4, temp_pr_case2, temp_pr_case4, temp_rs_case2, temp_rs_case4 ], ignore_index= True)\n",
    "\n",
    "                temp_case2_C100m = readStatsByLinkAndReturnShortCongLinks(path_case2, sample_size, sampleNr, alpha, default_stuck_time, 0.5, 0.1)\n",
    "                temp_case4_C100m = readStatsByLinkAndReturnShortCongLinks(path_case4, sample_size, sampleNr, alpha, adjusted_stuck_time, 0.5, 0.1)\n",
    "\n",
    "                df_link_cong_len_leq_100m_1pct = pd.concat([df_link_cong_len_leq_100m_1pct, temp_case2_C100m, temp_case4_C100m ], ignore_index= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.05\"]\n",
    "storCapF =  [\"0.05\", \"0.10574\"]\n",
    "\n",
    "\n",
    "df_3_links_mw_pr_res_5pct = pd.DataFrame()\n",
    "df_link_cong_len_leq_100m_5pct = pd.DataFrame()\n",
    "\n",
    "\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,11,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "                sample_size = float(fCf)*100\n",
    "                # declare path based on case \n",
    "\n",
    "                \n",
    "                if((fCf == \"0.05\") & (sCf == \"0.05\")):\n",
    "                    alpha = 1.0\n",
    "                    if (sampleNr == 6):\n",
    "                        path_case1  = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                        path_case3 =  path = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str(sampleNr) + \"-fCf_sCF_0.05_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "\n",
    "                    else: \n",
    "                        path_case1 = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(sampleNr) + \"-fCf_sCF_0.05_gS_4711_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                        path_case3 =  path = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str(sampleNr) + \"-fCf_sCF_0.05_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    \n",
    "                    temp_mw_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_mw_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_pr_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_pr_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_rs_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_rs_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    df_3_links_mw_pr_res_5pct = pd.concat([df_3_links_mw_pr_res_5pct, temp_mw_case1, temp_mw_case3, temp_pr_case1, temp_pr_case3, temp_rs_case1, temp_rs_case3 ], ignore_index= True)\n",
    "\n",
    "                    # count how many links are congested with a length of less thann 100m\n",
    "\n",
    "                    temp_case1_C100m = readStatsByLinkAndReturnShortCongLinks(path_case1, sample_size, sampleNr, alpha, default_stuck_time, 0.5, 0.1)\n",
    "                    temp_case3_C100m = readStatsByLinkAndReturnShortCongLinks(path_case3, sample_size, sampleNr, alpha, adjusted_stuck_time, 0.5, 0.1)\n",
    "\n",
    "                    df_link_cong_len_leq_100m_5pct = pd.concat([df_link_cong_len_leq_100m_5pct, temp_case1_C100m, temp_case3_C100m ], ignore_index= True)\n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    # case 2\n",
    "                    if(sampleNr == 6):\n",
    "                        path_case2 = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_0.05_sCF_0.10574_gS_4711_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                        path_case4 = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str(sampleNr) + \"-fCf_0.05_sCF_0.10574_gS_4711_sT_600.0_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    else:\n",
    "                        path_case2 = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(sampleNr) + \"-fCf_0.05_sCF_0.10574_gS_4711_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                        path_case4 = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str(sampleNr) + \"-fCf_0.05_sCF_0.10574_gS_4711_sT_600.0_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    temp_mw_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_mw_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_pr_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_pr_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_rs_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_rs_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    df_3_links_mw_pr_res_5pct = pd.concat([df_3_links_mw_pr_res_5pct, temp_mw_case2, temp_mw_case4, temp_pr_case2, temp_pr_case4, temp_rs_case2, temp_rs_case4 ], ignore_index= True)\n",
    "\n",
    "                    # count how many links are congested with a length of less thann 100m\n",
    "                    temp_case2_C100m = readStatsByLinkAndReturnShortCongLinks(path_case2, sample_size, sampleNr, alpha, default_stuck_time, 0.5, 0.1)\n",
    "                    temp_case4_C100m = readStatsByLinkAndReturnShortCongLinks(path_case4, sample_size, sampleNr, alpha, adjusted_stuck_time, 0.5, 0.1)\n",
    "\n",
    "                    df_link_cong_len_leq_100m_5pct = pd.concat([df_link_cong_len_leq_100m_5pct, temp_case2_C100m, temp_case4_C100m ], ignore_index= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.1\"]\n",
    "storCapF =  [\"0.1\", \"0.17783\"]\n",
    "\n",
    "df_3_links_mw_pr_res_10pct = pd.DataFrame()\n",
    "df_link_cong_len_leq_100m_10pct = pd.DataFrame()\n",
    "\n",
    "counter = 0\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,11,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "                sample_size = float(fCf)*100\n",
    "                # declare path based on case\n",
    "                if((fCf == \"0.1\") & (sCf == \"0.1\")):\n",
    "                    alpha = 1.0\n",
    "                    path_case1 = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" +str(sampleNr) + \"-fCf_sCF_0.1_gS_4711_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    path_case3 = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\" + str(sampleNr) + \"-fCf_sCF_0.1_gS_4711_sT_300.0_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    \n",
    "                    temp_mw_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_mw_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_pr_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_pr_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_rs_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_rs_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    df_3_links_mw_pr_res_10pct = pd.concat([df_3_links_mw_pr_res_10pct, temp_mw_case1, temp_mw_case3, temp_pr_case1, temp_pr_case3, temp_rs_case1, temp_rs_case3 ], ignore_index= True)\n",
    "\n",
    "                    #count how many links are congested with a length of less than 100m\n",
    "                    temp_case1_C100m = readStatsByLinkAndReturnShortCongLinks(path_case1, sample_size, sampleNr, alpha, default_stuck_time, 0.5, 0.1)\n",
    "                    temp_case3_C100m = readStatsByLinkAndReturnShortCongLinks(path_case3, sample_size, sampleNr, alpha, adjusted_stuck_time, 0.5, 0.1)\n",
    "\n",
    "                    df_link_cong_len_leq_100m_10pct = pd.concat([df_link_cong_len_leq_100m_10pct, temp_case1_C100m, temp_case3_C100m ], ignore_index= True)\n",
    "               \n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    if(sampleNr==2):\n",
    "                        path_case4 = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-10-pct-2-fCf_0.1_sCF_0.17783_gS_4711_sT_300.0_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                        temp_mw_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "                        temp_pr_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "                        temp_rs_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "                        df_3_links_mw_pr_res_10pct = pd.concat([df_3_links_mw_pr_res_10pct, temp_mw_case4, temp_pr_case4, temp_rs_case4 ], ignore_index= True)\n",
    "\n",
    "                        temp_case4_C100m = readStatsByLinkAndReturnShortCongLinks(path_case4, sample_size, sampleNr, alpha, adjusted_stuck_time, 0.5, 0.1)\n",
    "                        df_link_cong_len_leq_100m_10pct = pd.concat([df_link_cong_len_leq_100m_10pct, temp_case4_C100m ], ignore_index= True)\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        path_case4 = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\" + str(sampleNr) + \"-fCf_0.1_sCF_0.17783_gS_4711_sT_300.0_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                        temp_mw_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "                        temp_pr_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "                        temp_rs_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "                        df_3_links_mw_pr_res_10pct = pd.concat([df_3_links_mw_pr_res_10pct, temp_mw_case4, temp_pr_case4, temp_rs_case4 ], ignore_index= True)\n",
    "\n",
    "                        temp_case4_C100m = readStatsByLinkAndReturnShortCongLinks(path_case4, sample_size, sampleNr, alpha, adjusted_stuck_time, 0.5, 0.1)\n",
    "                        df_link_cong_len_leq_100m_10pct = pd.concat([df_link_cong_len_leq_100m_10pct, temp_case4_C100m ], ignore_index= True)\n",
    "                          \n",
    "                    \n",
    "                    path_case2 = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" + str(sampleNr) + \"-fCf_0.1_sCF_0.17783_gS_4711_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    temp_mw_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_pr_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_rs_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    \n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    df_3_links_mw_pr_res_10pct = pd.concat([df_3_links_mw_pr_res_10pct, temp_mw_case2,  temp_pr_case2,  temp_rs_case2 ], ignore_index= True)\n",
    "\n",
    "                    # count how many links are congested with a length of less than 100m\n",
    "\n",
    "                    temp_case2_C100m = readStatsByLinkAndReturnShortCongLinks(path_case2, sample_size, sampleNr, alpha, default_stuck_time, 0.5, 0.1)\n",
    "                    \n",
    "\n",
    "                    df_link_cong_len_leq_100m_10pct = pd.concat([df_link_cong_len_leq_100m_10pct, temp_case2_C100m, temp_case4_C100m ], ignore_index= True)\n",
    "\n",
    "\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.25\"]\n",
    "storCapF =  [\"0.25\", \"0.35355\"]\n",
    "\n",
    "df_3_links_mw_pr_res_25pct = pd.DataFrame()\n",
    "df_link_cong_len_leq_100m_25pct = pd.DataFrame()\n",
    "counter = 0\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,2,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "                sample_size = float(fCf)*100\n",
    "                # declare path based on case\n",
    "                if((fCf == \"0.25\") & (sCf == \"0.25\")):\n",
    "                    alpha = 1.0\n",
    "                    path_case1 = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_sCF_0.25_gS_4711_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    path_case3 = \"/home/lola/math_cluster/output/output-lausitz-25-pct-1-fCf_sCF_0.25_gS_4711_sT_120.0_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    \n",
    "                    temp_mw_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_mw_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_pr_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_pr_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_rs_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_rs_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    df_3_links_mw_pr_res_25pct = pd.concat([df_3_links_mw_pr_res_25pct, temp_mw_case1, temp_mw_case3, temp_pr_case1, temp_pr_case3, temp_rs_case1, temp_rs_case3 ], ignore_index= True)\n",
    "\n",
    "                    #count how many links are congested with a length of less than 100m\n",
    "                    temp_case1_C100m = readStatsByLinkAndReturnShortCongLinks(path_case1, sample_size, sampleNr, alpha, default_stuck_time, 0.5, 0.1)\n",
    "                    temp_case3_C100m = readStatsByLinkAndReturnShortCongLinks(path_case3, sample_size, sampleNr, alpha, adjusted_stuck_time, 0.5, 0.1)\n",
    "\n",
    "                    df_link_cong_len_leq_100m_25pct = pd.concat([df_link_cong_len_leq_100m_25pct, temp_case1_C100m, temp_case3_C100m ], ignore_index= True)\n",
    "\n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    path_case2 = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_0.25_sCF_0.35355_gS_4711_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    path_case4 = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25-pct-1-fCf_0.25_sCF_0.35355_gS_4711_sT_120.0_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    temp_mw_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_mw_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_pr_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_pr_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_rs_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_rs_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    df_3_links_mw_pr_res_25pct = pd.concat([df_3_links_mw_pr_res_25pct, temp_mw_case2, temp_mw_case4, temp_pr_case2, temp_pr_case4, temp_rs_case2, temp_rs_case4 ], ignore_index= True)\n",
    "\n",
    "                    # count how many links are congested with a length of less thann 100m\n",
    "                    temp_case2_C100m = readStatsByLinkAndReturnShortCongLinks(path_case2, sample_size, sampleNr, alpha, default_stuck_time, 0.5, 0.1)\n",
    "                    temp_case4_C100m = readStatsByLinkAndReturnShortCongLinks(path_case4, sample_size, sampleNr, alpha, adjusted_stuck_time, 0.5, 0.1)\n",
    "\n",
    "                    df_link_cong_len_leq_100m_25pct = pd.concat([df_link_cong_len_leq_100m_25pct, temp_case2_C100m, temp_case4_C100m ], ignore_index= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 50 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.5\"]\n",
    "storCapF =  [\"0.5\", \"0.5946\"]\n",
    "\n",
    "df_3_links_mw_pr_res_50pct = pd.DataFrame()\n",
    "df_link_cong_len_leq_100m_50pct = pd.DataFrame()\n",
    "\n",
    "counter = 0\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,2,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "                sample_size = float(fCf)*100\n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.5\") & (sCf == \"0.5\")):\n",
    "                    alpha = 1.0\n",
    "                    path_case1 = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_sCF_0.5_gS_4711_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    path_case3 = \"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_sCF_0.5_gS_4711_sT_60.0_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    temp_mw_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_mw_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_pr_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_pr_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_rs_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_rs_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    df_3_links_mw_pr_res_50pct = pd.concat([df_3_links_mw_pr_res_50pct, temp_mw_case1, temp_mw_case3, temp_pr_case1, temp_pr_case3, temp_rs_case1, temp_rs_case3 ], ignore_index= True)\n",
    "\n",
    "                    #count how many links are congested with a length of less than 100m\n",
    "                    temp_case1_C100m = readStatsByLinkAndReturnShortCongLinks(path_case1, sample_size, sampleNr, alpha, default_stuck_time, 0.5, 0.1)\n",
    "                    temp_case3_C100m = readStatsByLinkAndReturnShortCongLinks(path_case3, sample_size, sampleNr, alpha, adjusted_stuck_time, 0.5, 0.1)\n",
    "\n",
    "                    df_link_cong_len_leq_100m_50pct = pd.concat([df_link_cong_len_leq_100m_50pct, temp_case1_C100m, temp_case3_C100m ], ignore_index= True)\n",
    "\n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    path_case2 = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_0.5_sCF_0.5946_gS_4711_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    path_case4 = \"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_0.5_sCF_0.5946_gS_4711_sT_60.0_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    \n",
    "                    temp_mw_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_mw_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_pr_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_pr_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_rs_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_rs_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    df_3_links_mw_pr_res_50pct = pd.concat([df_3_links_mw_pr_res_50pct, temp_mw_case2, temp_mw_case4, temp_pr_case2, temp_pr_case4, temp_rs_case2, temp_rs_case4 ], ignore_index= True)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 100 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"1.0\"]\n",
    "storCapF =  [\"1.0\"]\n",
    "\n",
    "\n",
    "df_3_links_mw_pr_res_100pct = pd.DataFrame()\n",
    "df_link_cong_len_leq_100m_100pct = pd.DataFrame()\n",
    "\n",
    "counter = 0\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,2,1):\n",
    "                # declare path based on case \n",
    "                alpha = 1.0\n",
    "                sample_size = float(fCf)*100\n",
    "                path = \"/home/lola/math_cluster/output/output-lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                temp_mw_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=1.0, stuckTime=30.0)\n",
    "                temp_pr_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=1.0, stuckTime=30.0)\n",
    "                temp_rs_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=1.0, stuckTime=30.0)\n",
    "\n",
    "                df_3_links_mw_pr_res_100pct = pd.concat([df_3_links_mw_pr_res_100pct, temp_mw_case1, temp_pr_case1, temp_rs_case1 ], ignore_index= True)\n",
    "\n",
    "                #count how many links are congested with a length of less than 100m\n",
    "                temp_case1_C100m = readStatsByLinkAndReturnShortCongLinks(path_case1, sample_size, sampleNr, alpha, default_stuck_time, 0.5, 0.1)\n",
    "\n",
    "                df_link_cong_len_leq_100m_100pct = pd.concat([df_link_cong_len_leq_100m_100pct, temp_case1_C100m ], ignore_index= True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### concat and write output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3_links_mw_pr_res_1_100 = pd.concat([df_3_links_mw_pr_res_1pct, df_3_links_mw_pr_res_5pct, df_3_links_mw_pr_res_10pct, df_3_links_mw_pr_res_25pct, df_3_links_mw_pr_res_50pct, df_3_links_mw_pr_res_100pct], ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3_links_mw_pr_res_1_100.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/traffic_stats_by_link_314328566_314040202_130268155.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_link_cong_len_leq_100m_1_100 = pd.concat([df_link_cong_len_leq_100m_1pct, df_link_cong_len_leq_100m_5pct, df_link_cong_len_leq_100m_10pct, df_link_cong_len_leq_100m_25pct, df_link_cong_len_leq_100m_50pct, df_link_cong_len_leq_100m_100pct], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_link_cong_len_leq_100m_1pct[(df_link_cong_len_leq_100m_1pct[\"alpha\"] == 1.0) & (df_link_cong_len_leq_100m_1pct[\"stuck_time\"] == 30.0) & (df_link_cong_len_leq_100m_1pct[\"sample_nr\"] == 10.0)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_link_cong_len_leq_100m_1_100.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/traffic_stats_by_link_with_cI_leq0.5_and_length_leq100m_1_100pct.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
