{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate all necessary output files into reference datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matsim\n",
    "import os \n",
    "import pandas as pd\n",
    "#import plotly.express as px \n",
    "# AttributeError: module 'numpy' has no attribute 'bool8'\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "from collections import defaultdict\n",
    "import plotly\n",
    "import kaleido\n",
    "%matplotlib inline\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.etree.ElementTree as ET\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Runtime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Import run_info.csv for all sample sizes, alpha and stuck-time values and create data frames with relevent specifications:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 pct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateRuntimeAndReturnDfRow(pathToFile,sampleSize, sampleNr, alpha_value, stuckTime, globalSeed):\n",
    "    temp = pd.read_csv(pathToFile, sep=\",\")\n",
    "    start= pd.Timestamp(temp.iloc[6,1])\n",
    "    end= pd.Timestamp(temp.iloc[7,1])\n",
    "    diff = end - start\n",
    "    rt = pd.DataFrame({'runtime': diff, 'sample_size': sampleSize, 'sample_nr': sampleNr, 'alpha': alpha_value, 'stuck_time': stuckTime, 'global_seed': globalSeed}, index = [0])\n",
    "    return rt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-pct\n"
     ]
    }
   ],
   "source": [
    "flowCapF = [\"0.01\"]\n",
    "sample_size_as_string = str(int(float(flowCapF[0])*100)) + \"-pct\"\n",
    "print(sample_size_as_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.01\"]\n",
    "storCapF =  [\"0.01\", \"0.03162\"]\n",
    "\n",
    "runtimes_1pct = pd.DataFrame()\n",
    "\n",
    "counter = 0\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "        for sampleNr in range(1,11,1):\n",
    "            # calculate adjusted stuck time\n",
    "            default_stuck_time = 30.0\n",
    "            adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "            # declare sample size as str \"1-pct\"\n",
    "            sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "\n",
    "            if ((fCf == \"0.01\") & (sCf == '0.01')):\n",
    "                alpha = 1.0\n",
    "                path_case1 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_default_3765/analysis/general/run_info.csv\"\n",
    "                rt_case1= calculateRuntimeAndReturnDfRow(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "\n",
    "                path_case3 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\"+str(sampleNr)+\"-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/analysis/general/run_info.csv\"\n",
    "                rt_case3 = calculateRuntimeAndReturnDfRow(path_case3, sample_size_as_string, sampleNr, alpha, adjusted_stuck_time, 'default')\n",
    "                runtimes_1pct = pd.concat([runtimes_1pct, rt_case1, rt_case3], ignore_index = True)\n",
    "            else:\n",
    "                alpha = 0.75\n",
    "                path_case2 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" + str(sampleNr) + \"-fCf_0.01_sCF_\" +sCf + \"_gS_default_3765/analysis/general/run_info.csv\"\n",
    "                rt_case2 = calculateRuntimeAndReturnDfRow(path_case2, sample_size_as_string, sampleNr, alpha, default_stuck_time, 'default')\n",
    "\n",
    "                path_case4 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\"+str(sampleNr)+\"-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/analysis/general/run_info.csv\"\n",
    "                rt_case4 = calculateRuntimeAndReturnDfRow(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "                runtimes_1pct = pd.concat([runtimes_1pct, rt_case2, rt_case4], ignore_index = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time_1pct = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" + str(elem) + \"-fCf_sCF_0.01_gS_default_3765/analysis/general/run_info.csv\"\n",
    "    temp = pd.read_csv(path, sep=\",\")\n",
    "    start= pd.Timestamp(temp.iloc[6,1])\n",
    "    end= pd.Timestamp(temp.iloc[7,1])\n",
    "    diff = end - start\n",
    "    run_time_1pct.append(diff)\n",
    "run_time_1pct = pd.DataFrame({'runtime': run_time_1pct, 'sample_size': '1-pct',\n",
    "                               'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': 'default'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time_1pct_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" + str(elem) + \"-fCf_0.01_sCF_0.03162_gS_default_3765/analysis/general/run_info.csv\"\n",
    "    temp = pd.read_csv(path, sep=\",\")\n",
    "    start= pd.Timestamp(temp.iloc[6,1])\n",
    "    end= pd.Timestamp(temp.iloc[7,1])\n",
    "    diff = end - start\n",
    "    run_time_1pct_sCf.append(diff)\n",
    "run_time_1pct_sCf = pd.DataFrame({'runtime': run_time_1pct_sCf, 'sample_size': '1-pct', 'alpha' : 0.75, 'stuck_time': 30.0, 'global_seed': 'default'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time_1pct_sT = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\"+str(elem)+\"-fCf_sCF_0.01_gS_4711_sT_3000.0_3765/analysis/general/run_info.csv\"\n",
    "    temp = pd.read_csv(path, sep = \",\")\n",
    "    start= pd.Timestamp(temp.iloc[6,1])\n",
    "    end= pd.Timestamp(temp.iloc[7,1])\n",
    "    diff = end - start\n",
    "    run_time_1pct_sT.append(diff)\n",
    "run_time_1pct_sT = pd.DataFrame({'runtime': run_time_1pct_sT, 'sample_size': \"1-pct\", 'alpha': 1.0,  'stuck_time': 3000.0, 'global_seed': \"default\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time_1pct_sT_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\"+str(elem)+\"-fCf_0.01_sCF_0.03162_gS_4711_sT_3000.0_3765/analysis/general/run_info.csv\"\n",
    "    temp = pd.read_csv(path, sep = \",\")\n",
    "    start= pd.Timestamp(temp.iloc[6,1])\n",
    "    end= pd.Timestamp(temp.iloc[7,1])\n",
    "    diff = end - start\n",
    "    run_time_1pct_sT_sCf.append(diff)\n",
    "run_time_1pct_sT_sCf = pd.DataFrame({'runtime': run_time_1pct_sT_sCf, 'sample_size': \"1-pct\", 'alpha': 0.75, 'stuck_time': 3000.0, 'global_seed': \"default\" })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time_1pct_rGs = pd.DataFrame()\n",
    "\n",
    "rGs = [ 4711,3254, 2306, 6384,4338, 6003, 5502, 9377, 5621, 9002 ]\n",
    "for seed in rGs:\n",
    "    if (seed == 4711):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        temp = {'runtime': run_time_1pct[\"runtime\"].iloc[0], 'sample_size': '1-pct', 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': global_seed  }\n",
    "        temp = pd.DataFrame(data=temp, index=[rGs.index(4711)])\n",
    "        run_time_1pct_rGs = pd.concat([run_time_1pct_rGs, temp])\n",
    "    elif (seed == 3254):\n",
    "        path =\"/home/lola/math_cluster/output/output-lausitz-1pct-1-fCf_sCF_0.01_gS_3254_3765/analysis/general/run_info.csv\"\n",
    "        temp = pd.read_csv(path, sep = \",\")\n",
    "        start= pd.Timestamp(temp.iloc[6,1])\n",
    "        end= pd.Timestamp(temp.iloc[7,1])\n",
    "        diff = end - start\n",
    "        temp2 = pd.DataFrame({'runtime': diff, 'sample_size': '1-pct', 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': \"rnd_\" + str(seed)  }, index = [rGs.index(3254)])\n",
    "        run_time_1pct_rGs = pd.concat([run_time_1pct_rGs, temp2], axis = 0)\n",
    "    else:\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-1.0-pct-1-fCf_sCF_0.01_gS_\"+str(seed) + \"_3765/analysis/general/run_info.csv\"\n",
    "        temp = pd.read_csv(path, sep = \",\")\n",
    "        start= pd.Timestamp(temp.iloc[6,1])\n",
    "        end= pd.Timestamp(temp.iloc[7,1])\n",
    "        diff = end - start\n",
    "        temp2 = pd.DataFrame({'runtime': diff, 'sample_size': '1-pct', 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': \"rnd_\" + str(seed)  }, index = [rGs.index(seed)] )\n",
    "        run_time_1pct_rGs = pd.concat([run_time_1pct_rGs, temp2], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time_5pct = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(elem) + \"-fCf_sCF_0.05_gS_4711_3765/analysis/general/run_info.csv\"\n",
    "    temp = pd.read_csv(path, sep=\",\")\n",
    "    start= pd.Timestamp(temp.iloc[6,1])\n",
    "    end= pd.Timestamp(temp.iloc[7,1])\n",
    "    diff = end - start\n",
    "    run_time_5pct.append(diff)\n",
    "run_time_5pct = pd.DataFrame({'runtime': run_time_5pct})\n",
    "run_time_5pct.insert(1, 'sample_size', \"5-pct\")\n",
    "run_time_5pct.insert(2, 'alpha', 1.0)\n",
    "run_time_5pct.insert(3, 'stuck_time', 30.0)\n",
    "run_time_5pct.insert(4,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time_5pct_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(elem) + \"-fCf_0.05_sCF_0.10574_gS_4711_3765/analysis/general/run_info.csv\"\n",
    "    temp = pd.read_csv(path, sep=\",\")\n",
    "    start= pd.Timestamp(temp.iloc[6,1])\n",
    "    end= pd.Timestamp(temp.iloc[7,1])\n",
    "    diff = end - start\n",
    "    run_time_5pct_sCf.append(diff)\n",
    "run_time_5pct_sCf = pd.DataFrame({'runtime': run_time_5pct_sCf})\n",
    "run_time_5pct_sCf.insert(1, 'sample_size', \"5-pct\")\n",
    "run_time_5pct_sCf.insert(2, 'alpha', 0.75)\n",
    "run_time_5pct_sCf.insert(3, 'stuck_time', 30.0)\n",
    "run_time_5pct_sCf.insert(4,'global_seed', \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time_5pct_rGs = pd.DataFrame()\n",
    "rGs = [4711, 3254, 2306, 6384,4338, 6003, 5502, 9377, 5621, 9002 ]\n",
    "\n",
    "for seed in rGs:\n",
    "    if (seed ==4711):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        temp = {'runtime': run_time_5pct[\"runtime\"].iloc[0], 'sample_size': '5-pct', 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': global_seed  }\n",
    "        temp = pd.DataFrame(data=temp, index=[rGs.index(4711)])\n",
    "        run_time_5pct_rGs = pd.concat([run_time_5pct_rGs, temp])\n",
    "    else:\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-1-fCf_sCF_0.05_gS_\"+ str(seed) +\"_3765/analysis/general/run_info.csv\"\n",
    "        temp = pd.read_csv(path, sep = \",\")\n",
    "        start= pd.Timestamp(temp.iloc[6,1])\n",
    "        end= pd.Timestamp(temp.iloc[7,1])\n",
    "        diff = end - start\n",
    "        temp2 = pd.DataFrame({'runtime': diff, 'sample_size': '5-pct', 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': \"rnd_\" + str(seed)  }, index = [rGs.index(seed)])\n",
    "        run_time_5pct_rGs = pd.concat([run_time_5pct_rGs, temp2], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time_5pct_sT = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\"+ str(elem) +\"-fCf_sCF_0.05_gS_4711_sT_600.0_3765/analysis/general/run_info.csv\"\n",
    "    temp = pd.read_csv(path, sep = \",\")\n",
    "    start= pd.Timestamp(temp.iloc[6,1])\n",
    "    end= pd.Timestamp(temp.iloc[7,1])\n",
    "    diff = end - start\n",
    "    run_time_5pct_sT.append(diff)\n",
    "run_time_5pct_sT = pd.DataFrame({'runtime': run_time_5pct_sT})\n",
    "run_time_5pct_sT.insert(1, 'sample_size', \"5-pct\")\n",
    "run_time_5pct_sT.insert(2, 'alpha', 1.0)\n",
    "run_time_5pct_sT.insert(3, 'stuck_time', 600.0)\n",
    "run_time_5pct_sT.insert(4,'global_seed', \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[196], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m11\u001b[39m,\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      3\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/lola/math_cluster/output/output-lausitz-5-pct-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(elem)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-fCf_0.05_sCF_0.10574_gS_4711_sT_600.0_3765/analysis/general/run_info.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     temp \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43msep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     start\u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimestamp(temp\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      6\u001b[0m     end\u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimestamp(temp\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "File \u001b[0;32m<frozen codecs>:312\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, errors)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_time_5pct_sT_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\"+str(elem)+\"-fCf_0.05_sCF_0.10574_gS_4711_sT_600.0_3765/analysis/general/run_info.csv\"\n",
    "    temp = pd.read_csv(path,sep = \",\")\n",
    "    start= pd.Timestamp(temp.iloc[6,1])\n",
    "    end= pd.Timestamp(temp.iloc[7,1])\n",
    "    diff = end - start\n",
    "    run_time_5pct_sT_sCf.append(diff)\n",
    "run_time_5pct_sT_sCf = pd.DataFrame({'runtime': run_time_5pct_sT_sCf})\n",
    "run_time_5pct_sT_sCf.insert(1, 'sample_size', \"5-pct\")\n",
    "run_time_5pct_sT_sCf.insert(2, 'alpha', 0.75)\n",
    "run_time_5pct_sT_sCf.insert(3, 'stuck_time', 600.0)\n",
    "run_time_5pct_sT_sCf.insert(4,'global_seed', \"default\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 pct #### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time_10pct = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" + str(elem) + \"-fCf_sCF_0.1_gS_4711_3765/analysis/general/run_info.csv\"\n",
    "    temp = pd.read_csv(path, sep=\",\")\n",
    "    start= pd.Timestamp(temp.iloc[6,1])\n",
    "    end= pd.Timestamp(temp.iloc[7,1])\n",
    "    diff = end - start\n",
    "    run_time_10pct.append(diff)\n",
    "run_time_10pct = pd.DataFrame({'runtime': run_time_10pct})\n",
    "run_time_10pct.insert(1, 'sample_size', \"10-pct\")\n",
    "run_time_10pct.insert(2, 'alpha', 1.0)\n",
    "run_time_10pct.insert(3, 'stuck_time', 30.0)\n",
    "run_time_10pct.insert(4,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time_10pct_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" + str(elem) + \"-fCf_0.1_sCF_0.17783_gS_4711_3765/analysis/general/run_info.csv\"\n",
    "    temp = pd.read_csv(path, sep=\",\")\n",
    "    start= pd.Timestamp(temp.iloc[6,1])\n",
    "    end= pd.Timestamp(temp.iloc[7,1])\n",
    "    diff = end - start\n",
    "    run_time_10pct_sCf.append(diff)\n",
    "run_time_10pct_sCf = pd.DataFrame({'runtime': run_time_10pct_sCf})\n",
    "run_time_10pct_sCf.insert(1, 'sample_size', \"10-pct\")\n",
    "run_time_10pct_sCf.insert(2, 'alpha', 0.75)\n",
    "run_time_10pct_sCf.insert(3, 'stuck_time', 30.0)\n",
    "run_time_10pct_sCf.insert(4,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_time_10pct_sT = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\"+ str(elem) + \"-fCf_sCF_0.1_gS_4711_sT_300.0_3765/analysis/general/run_info.csv\"\n",
    "    temp = pd.read_csv(path, sep=\",\")\n",
    "    start= pd.Timestamp(temp.iloc[6,1])\n",
    "    end= pd.Timestamp(temp.iloc[7,1])\n",
    "    diff = end - start\n",
    "    run_time_10pct_sT.append(diff)\n",
    "run_time_10pct_sT = pd.DataFrame({'runtime': run_time_10pct_sT})\n",
    "run_time_10pct_sT.insert(1, 'sample_size', \"10-pct\")\n",
    "run_time_10pct_sT.insert(2, 'alpha', 1.0)\n",
    "run_time_10pct_sT.insert(3, 'stuck_time', 300.0)\n",
    "run_time_10pct_sT.insert(4,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time_10pct_sT_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\" +str(elem) + \"-fCf_0.1_sCF_0.17783_gS_4711_sT_300.0_3765/analysis/general/run_info.csv\"\n",
    "    temp = pd.read_csv(path, sep=\",\")\n",
    "    start= pd.Timestamp(temp.iloc[6,1])\n",
    "    end= pd.Timestamp(temp.iloc[7,1])\n",
    "    diff = end - start\n",
    "    run_time_10pct_sT_sCf.append(diff)\n",
    "run_time_10pct_sT_sCf = pd.DataFrame({'runtime': run_time_10pct_sT_sCf})\n",
    "run_time_10pct_sT_sCf.insert(1, 'sample_size', \"10-pct\")\n",
    "run_time_10pct_sT_sCf.insert(2, 'alpha', 0.75)\n",
    "run_time_10pct_sT_sCf.insert(3, 'stuck_time', 300.0)\n",
    "run_time_10pct_sT_sCf.insert(4,'global_seed', \"default\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_sCF_0.25_gS_4711_3765/analysis/general/run_info.csv\"\n",
    "temp = pd.read_csv(path, sep = \",\")\n",
    "start= pd.Timestamp(temp.iloc[6,1])\n",
    "end= pd.Timestamp(temp.iloc[7,1])\n",
    "diff = end - start\n",
    "run_time_25pct = pd.DataFrame({'runtime': diff, 'sample_size': '25-pct', 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': \"default\"  }, index = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_0.25_sCF_0.35355_gS_4711_3765/analysis/general/run_info.csv\"\n",
    "temp = pd.read_csv(path, sep = \",\")\n",
    "start= pd.Timestamp(temp.iloc[6,1])\n",
    "end= pd.Timestamp(temp.iloc[7,1])\n",
    "diff = end - start\n",
    "run_time_25pct_sCf = pd.DataFrame({'runtime': diff, 'sample_size': '25-pct', 'alpha': 0.75, 'stuck_time': 30.0, 'global_seed': \"default\"  }, index = [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-25-pct-1-fCf_sCF_0.25_gS_4711_sT_120.0_3765/analysis/general/run_info.csv\"\n",
    "temp = pd.read_csv(path, sep = \",\")\n",
    "start= pd.Timestamp(temp.iloc[6,1])\n",
    "end= pd.Timestamp(temp.iloc[7,1])\n",
    "diff = end - start\n",
    "run_time_25pct_sT = pd.DataFrame({'runtime': diff, 'sample_size': '25-pct', 'alpha': 1.0, 'stuck_time': 120.0, 'global_seed': \"default\"  }, index = [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-25-pct-1-fCf_0.25_sCF_0.35355_gS_4711_sT_120.0_3765/analysis/general/run_info.csv\"\n",
    "temp = pd.read_csv(path, sep = \",\")\n",
    "start= pd.Timestamp(temp.iloc[6,1])\n",
    "end= pd.Timestamp(temp.iloc[7,1])\n",
    "diff = end - start\n",
    "run_time_25pct_sT_sCf = pd.DataFrame({'runtime': diff, 'sample_size': '25-pct', 'alpha': 0.75, 'stuck_time': 120.0, 'global_seed': \"default\"  }, index = [3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_sCF_0.5_gS_4711_3765/analysis/general/run_info.csv\"\n",
    "temp = pd.read_csv(path, sep = \",\")\n",
    "start= pd.Timestamp(temp.iloc[6,1])\n",
    "end= pd.Timestamp(temp.iloc[7,1])\n",
    "diff = end - start\n",
    "run_time_50pct = pd.DataFrame({'runtime': diff, 'sample_size': '50-pct', 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': \"default\"  }, index = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_0.5_sCF_0.5946_gS_4711_3765/analysis/general/run_info.csv\"\n",
    "temp = pd.read_csv(path, sep = \",\")\n",
    "start= pd.Timestamp(temp.iloc[6,1])\n",
    "end= pd.Timestamp(temp.iloc[7,1])\n",
    "diff = end - start\n",
    "run_time_50pct_sCf = pd.DataFrame({'runtime': diff, 'sample_size': '50-pct', 'alpha': 0.75, 'stuck_time': 30.0, 'global_seed': \"default\"  }, index = [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_sCF_0.5_gS_4711_sT_60.0_3765/analysis/general/run_info.csv\"\n",
    "temp = pd.read_csv(path, sep = \",\")\n",
    "start= pd.Timestamp(temp.iloc[6,1])\n",
    "end= pd.Timestamp(temp.iloc[7,1])\n",
    "diff = end - start\n",
    "run_time_50pct_sT = pd.DataFrame({'runtime': diff, 'sample_size': '50-pct', 'alpha': 1.0, 'stuck_time': 60.0, 'global_seed': \"default\"  }, index = [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_0.5_sCF_0.5946_gS_4711_sT_60.0_3765/analysis/general/run_info.csv\"\n",
    "temp = pd.read_csv(path, sep = \",\")\n",
    "start= pd.Timestamp(temp.iloc[6,1])\n",
    "end= pd.Timestamp(temp.iloc[7,1])\n",
    "diff = end - start\n",
    "run_time_50pct_sT_sCf = pd.DataFrame({'runtime': diff, 'sample_size': '50-pct', 'alpha': 0.75, 'stuck_time': 60.0, 'global_seed': \"default\"  }, index = [3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25-pct-doubled-fCf_0.5_sCF_0.5_gS_4711_3765/analysis/general/run_info.csv\"\n",
    "temp = pd.read_csv(path, sep = \",\")\n",
    "start= pd.Timestamp(temp.iloc[6,1])\n",
    "end= pd.Timestamp(temp.iloc[7,1])\n",
    "diff = end - start\n",
    "run_time_25_pct_doubled = pd.DataFrame({'runtime': diff, 'sample_size': '25-pct-doubled', 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': \"default\"  }, index = [4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  \"/home/lola/math_cluster/output/output-lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765/analysis/general/run_info.csv\"\n",
    "temp = pd.read_csv(path, sep = \",\")\n",
    "start= pd.Timestamp(temp.iloc[6,1])\n",
    "end= pd.Timestamp(temp.iloc[7,1])\n",
    "diff = end - start\n",
    "run_time_100pct = pd.DataFrame({'runtime': diff, 'sample_size': '100-pct', 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': \"default\"  }, index = [0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25.0-pct-quadrupled-fCf_1.0_sCF_1.0_gS_4711_3765/analysis/general/run_info.csv\"\n",
    "temp = pd.read_csv(path, sep = \",\")\n",
    "start= pd.Timestamp(temp.iloc[6,1])\n",
    "end= pd.Timestamp(temp.iloc[7,1])\n",
    "diff = end - start\n",
    "run_time_25_pct_quadrupled = pd.DataFrame({'runtime': diff, 'sample_size': '25-pct-quadrupled', 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': \"default\"  }, index = [4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pd.Concat,  convert minutes into decimal base?! and write csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat !\n",
    "runtimes_1_100 = pd.concat([run_time_1pct, run_time_1pct_sCf, run_time_1pct_sT, run_time_1pct_sT_sCf, run_time_1pct_rGs,\n",
    "                            run_time_5pct,run_time_5pct_sCf, run_time_5pct_sT, run_time_5pct_sT_sCf, run_time_5pct_rGs,\n",
    "                            run_time_10pct,run_time_10pct_sCf, run_time_10pct_sT, run_time_10pct_sT_sCf,                  \n",
    "                              run_time_25pct,run_time_25pct_sCf, run_time_25pct_sT, run_time_25pct_sT_sCf,\n",
    "                                run_time_50pct,run_time_50pct_sCf, run_time_50pct_sT, run_time_50pct_sT_sCf,run_time_25_pct_doubled,\n",
    "                                run_time_100pct, run_time_25_pct_quadrupled  ])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform minutes into decimal \n",
    "runtimes_1_100['runtime'] =  runtimes_1_100.runtime.astype('timedelta64[s]')/  pd.Timedelta(minutes=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimes_1_100 = runtimes_1_100.iloc[:, [1, 2, 3, 4,0]]\n",
    "runtimes_1_100.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/runtimes_1_to_100pct_correct.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Number of stuck-time violations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to filter the output events and count number of stuck-time violations per vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of stuck vehicles\n",
    "def countNumberOfStuckTimeViolations(pathToEvents, sampleSize, sampleNr, alpha, globalSeed, stuckTime):\n",
    "    events_file = pathToEvents\n",
    "   \n",
    "\n",
    "    # Read events - filter and return the listed event types only\n",
    "    events = matsim.event_reader(\n",
    "        events_file,\n",
    "        types=\"stuckAndContinue\",\n",
    "    )\n",
    "\n",
    "\n",
    "    stuck_time_violations = pd.DataFrame()\n",
    "\n",
    "    # Loop on all filtered events\n",
    "    for event in events:\n",
    "        if event[\"type\"] == \"stuckAndContinue\":\n",
    "            temp = pd.DataFrame(event, index = [0])\n",
    "            stuck_time_violations = pd.concat([stuck_time_violations, temp], ignore_index= True)\n",
    "    \n",
    "    #outputPath = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs/stuck_time_violations_detailed_per_run/stuck_time_violations_lausitz_\" + sampleSize +\"-\"+str(sampleNr) + \"-alpha-\"+ str(alpha) + \"-gS-\" + globalSeed + \"-sT-\" + str(stuckTime) + \".csv\"\n",
    "    #stuck_time_violations.to_csv(outputPath, index = False)\n",
    "    temp = pd.DataFrame({'n_stuck_time_violations': stuck_time_violations.shape[0], 'sample_size': sampleSize, 'sample_nr':sampleNr, 'alpha': alpha, 'global_seed': globalSeed, 'stuck_time': stuckTime }, index =[0])\n",
    "    return temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.01\"]\n",
    "storCapF =  [\"0.01\", \"0.03162\"]\n",
    "\n",
    "stuck_veh_1pct = pd.DataFrame()\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "        for sampleNr in range(1,11,1):\n",
    "            # calculate adjusted stuck time\n",
    "            default_stuck_time = 30.0\n",
    "            adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "            # declare sample size as str \"1-pct\"\n",
    "            sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "\n",
    "\n",
    "            if ((fCf == \"0.01\") & (sCf == '0.01')):\n",
    "                alpha = 1.0\n",
    "                # paths for case 1 and 3 \n",
    "                path_case1 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" + str(sampleNr) + \"-fCf_sCF_\"+ sCf + \"_gS_default_3765/lausitz-1pct-\"+str(sampleNr)+ \"-fCf_sCf_0.01_gS_default_3765.output_events.xml.gz\"\n",
    "                path_case3 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-1-pct-\" + str(sampleNr) + \"-fCf_sCF_0.01_gS_4711_sT_3000.0_3765.output_events.xml.gz\"\n",
    "                \n",
    "                # calculate number of stuck time violations and attach them to the data frame\n",
    "                temp_case1 = countNumberOfStuckTimeViolations(path_case1, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                temp_case3 = countNumberOfStuckTimeViolations(path_case3, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                stuck_veh_1pct = pd.concat([stuck_veh_1pct, temp_case1, temp_case3], ignore_index= True)\n",
    "\n",
    "            else:\n",
    "                alpha = 0.75\n",
    "                # paths for case 2 and 4 \n",
    "                path_case2 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" + str(sampleNr) + \"-fCf_0.01_sCF_\" + sCf + \"_gS_default_3765/lausitz-1pct-\" +str(sampleNr)+ \"-fCf_0.01_sCf_0.03162_gS_default_3765.output_events.xml.gz\"\n",
    "                path_case4 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" + str(sampleNr) + \"-fCf_0.01_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-1-pct-\" + str(sampleNr) +\"-fCf_0.01_sCF_0.03162_gS_4711_sT_3000.0_3765.output_events.xml.gz\"\n",
    "                \n",
    "                # calculate number of stuck time violations and attach them to the data frame\n",
    "                temp_case2 = countNumberOfStuckTimeViolations(path_case2, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                temp_case4 = countNumberOfStuckTimeViolations(path_case4, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                stuck_veh_1pct = pd.concat([stuck_veh_1pct, temp_case2, temp_case4], ignore_index= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1 pct random seed\n",
    "stuck_time_violations_1pct_rGs = pd.DataFrame()\n",
    "rGs = [4711,3254, 2306, 6384,4338, 6003, 5502, 9377, 5621, 9002 ]\n",
    "for seed in rGs:\n",
    "    if (seed == 4711):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        # insert number of stuck time violations from the first 1 pct sample\n",
    "        stuck_time_violations = stuck_veh_1pct[(stuck_veh_1pct['alpha'] == 1.0) & (stuck_veh_1pct['stuck_time'] == 30.0) & (stuck_veh_1pct[\"sample_nr\"] == 1)][\"n_stuck_time_violations\"]\n",
    "        temp = {'n_stuck_time_violations': stuck_time_violations, 'sample_size': '1-pct','sample_nr': 1, 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': global_seed  }\n",
    "        temp = pd.DataFrame(data=temp, index=[rGs.index(4711)])\n",
    "        stuck_time_violations_1pct_rGs = pd.concat([stuck_time_violations_1pct_rGs, temp])\n",
    "    elif (seed == 3254):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        path =\"/home/lola/math_cluster/output/output-lausitz-1pct-1-fCf_sCF_0.01_gS_3254_3765/lausitz-1pct-1-fCf_sCf_0.01_gS_3254_3765.output_events.xml.gz\"\n",
    "        stuck_time_violations = countNumberOfStuckTimeViolations(path, \"1-pct\", 1, 1.0 , global_seed, 30.0)\n",
    "        stuck_time_violations_1pct_rGs = pd.concat([stuck_time_violations_1pct_rGs, stuck_time_violations], axis = 0, ignore_index= True)\n",
    "    else:\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-1.0-pct-1-fCf_sCF_0.01_gS_\" +str(seed) + \"_3765/lausitz-1.0-pct-1-fCf_sCF_0.01_gS_\" + str(seed) + \"_3765.output_events.xml.gz\"\n",
    "        stuck_time_violations = countNumberOfStuckTimeViolations(path, \"1-pct\", 1, 1.0 , global_seed, 30.0)\n",
    "        stuck_time_violations_1pct_rGs = pd.concat([stuck_time_violations_1pct_rGs, stuck_time_violations], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenae and write to file\n",
    "stuck_time_violation_1pct = pd.concat([stuck_veh_1pct, stuck_time_violations_1pct_rGs])\n",
    "stuck_time_violation_1pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/stuck_time_violations_1_pct.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.05\"]\n",
    "storCapF =  [\"0.05\", \"0.10574\"]\n",
    "\n",
    "stuck_time_violations_5pct = pd.DataFrame()\n",
    "\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,11,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "\n",
    "                # declare paths \n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.05\") & (sCf == \"0.05\")):\n",
    "                    alpha = 1.0\n",
    "                    if (sampleNr == 6):\n",
    "                        path_case1  = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765/lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765-2.output_events.xml.gz\"\n",
    "                    \n",
    "                    else: \n",
    "                        path_case1 = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_3765/lausitz-5.0-pct-\"+str(sampleNr)+ \"-fCf_sCF_0.05_gS_4711_3765.output_events.xml.gz\"\n",
    "                    \n",
    "                    \n",
    "                    path_case3 =  \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-5-pct-\" + str(sampleNr) +\"-fCf_sCF_0.05_gS_4711_sT_600.0_3765.output_events.xml.gz\"\n",
    "\n",
    "                    temp_case1 = countNumberOfStuckTimeViolations(path_case1, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case3 = countNumberOfStuckTimeViolations(path_case3, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    stuck_time_violations_5pct = pd.concat([stuck_time_violations_5pct, temp_case1, temp_case3], ignore_index= True)\n",
    "                    \n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    if(sampleNr == 6):\n",
    "                        path_case2 = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_0.05_sCF_0.10574_gS_4711_3765/lausitz-5.0-pct-6-fCf_0.05_sCF_0.10574_gS_4711_3765-2.output_events.xml.gz\"\n",
    "                    else:\n",
    "                        path_case2 = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf  + \"_gS_4711_3765/lausitz-5.0-pct-\"+str(sampleNr)+ \"-fCf_0.05_sCF_0.10574_gS_4711_3765.output_events.xml.gz\"\n",
    "                    \n",
    "                    \n",
    "                    path_case4 = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str (sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-5-pct-\" + str(sampleNr) + \"-fCf_0.05_sCF_0.10574_gS_4711_sT_600.0_3765.output_events.xml.gz\"\n",
    "\n",
    "                    # calculate number of stuck time violations and attach them to the data frame\n",
    "                    temp_case2 = countNumberOfStuckTimeViolations(path_case2, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case4 = countNumberOfStuckTimeViolations(path_case4, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    stuck_time_violations_5pct = pd.concat([stuck_time_violations_5pct, temp_case2, temp_case4], ignore_index= True)\n",
    "                \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 pct random global seed, alpha = 1\n",
    "stuck_time_violations_5pct_rGs = pd.DataFrame()\n",
    "rGs = [4711, 3254, 2306, 6384,4338, 6003, 5502, 9377, 5621, 9002 ]\n",
    "\n",
    "for seed in rGs:\n",
    "    if (seed ==4711):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        stuck_time_violations = stuck_time_violations_5pct[(stuck_time_violations_5pct['alpha'] == 1.0) & (stuck_time_violations_5pct['stuck_time'] == 30.0) & (stuck_time_violations_5pct[\"sample_nr\"] == 1)][\"n_stuck_time_violations\"]\n",
    "        temp = {'n_stuck_time_violations': stuck_time_violations, 'sample_size': '5-pct','sample_nr': 1, 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': global_seed  }\n",
    "        temp = pd.DataFrame(data=temp, index=[rGs.index(4711)])\n",
    "        stuck_time_violations_5pct_rGs = pd.concat([stuck_time_violations_5pct_rGs, temp])\n",
    "    else:\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-1-fCf_sCF_0.05_gS_\" + str(seed) + \"_3765/lausitz-5.0-pct-1-fCf_sCF_0.05_gS_\"+ str(seed) +\"_3765.output_events.xml.gz\"\n",
    "        \n",
    "        stuck_time_violations = countNumberOfStuckTimeViolations(path, \"5-pct\", 1, 1.0 , global_seed, 30.0)\n",
    "        stuck_time_violations_5pct_rGs = pd.concat([stuck_time_violations_5pct_rGs, stuck_time_violations], axis = 0, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuck_time_violation_5pct = pd.concat([stuck_time_violations_5pct, stuck_time_violations_5pct_rGs])\n",
    "stuck_time_violation_5pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/stuck_time_violations_5_pct.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.1\"]\n",
    "storCapF =  [\"0.1\", \"0.17783\"]\n",
    "\n",
    "stuck_time_violations_10pct = pd.DataFrame()\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,11,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.1\") & (sCf == \"0.1\")):\n",
    "                    alpha = 1.0\n",
    "                    path_case1 = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_3765/lausitz-10.0-pct-\" + str(sampleNr) + \"-fCf_sCF_0.1_gS_4711_3765.output_events.xml.gz\"\n",
    "                    path_case3 = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-10-pct-\" + str(sampleNr) + \"-fCf_sCF_0.1_gS_4711_sT_300.0_3765.output_events.xml.gz\"\n",
    "                    \n",
    "                    temp_case1 = countNumberOfStuckTimeViolations(path_case1, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case3 = countNumberOfStuckTimeViolations(path_case3, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    stuck_time_violations_10pct = pd.concat([stuck_time_violations_10pct, temp_case1, temp_case3], ignore_index= True)     \n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    path_case2 = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\"+ str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf +\"_gS_4711_3765/lausitz-10.0-pct-\" + str(sampleNr) + \"-fCf_0.1_sCF_0.17783_gS_4711_3765.output_events.xml.gz\"\n",
    "                    path_case4 =\"/home/lola/math_cluster/output/output-lausitz-10-pct-\" + str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-10-pct-\" + str(sampleNr) + \"-fCf_0.1_sCF_0.17783_gS_4711_sT_300.0_3765.output_events.xml.gz\"\n",
    "\n",
    "                    # calculate number of stuck time violations and attach them to the data frame\n",
    "                    temp_case2 = countNumberOfStuckTimeViolations(path_case2, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case4 = countNumberOfStuckTimeViolations(path_case4, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    stuck_time_violations_10pct = pd.concat([stuck_time_violations_10pct, temp_case2, temp_case4], ignore_index= True)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "stuck_time_violations_10pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/stuck_time_violations_10_pct.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.25\"]\n",
    "storCapF =  [\"0.25\", \"0.35355\"]\n",
    "\n",
    "stuck_time_violations_25pct = pd.DataFrame()\n",
    "counter = 0\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,2,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "                \n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.25\") & (sCf == \"0.25\")):\n",
    "                    alpha = 1.0\n",
    "                    path_case1 = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_sCF_0.25_gS_4711_3765/lausitz-25.0-pct-fCf_sCF_0.25_gS_4711_3765.output_events.xml.gz\"\n",
    "                    path_case3 = \"/home/lola/math_cluster/output/output-lausitz-25-pct-1-fCf_sCF_0.25_gS_4711_sT_120.0_3765/lausitz-25-pct-1-fCf_sCF_0.25_gS_4711_sT_120.0_3765.output_events.xml.gz\"\n",
    "\n",
    "                    temp_case1 = countNumberOfStuckTimeViolations(path_case1, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case3 = countNumberOfStuckTimeViolations(path_case3, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    stuck_time_violations_25pct = pd.concat([stuck_time_violations_25pct, temp_case1, temp_case3], ignore_index= True)  \n",
    "\n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    path_case2 = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_0.25_sCF_0.35355_gS_4711_3765/lausitz-25.0-pct-fCf_0.25_sCF_0.35355_gS_4711_3765.output_events.xml.gz\"\n",
    "                    path_case4 = \"/home/lola/math_cluster/output/output-lausitz-25-pct-1-fCf_0.25_sCF_0.35355_gS_4711_sT_120.0_3765/lausitz-25-pct-1-fCf_0.25_sCF_0.35355_gS_4711_sT_120.0_3765.output_events.xml.gz\"\n",
    "\n",
    "                    # calculate number of stuck time violations and attach them to the data frame\n",
    "                    temp_case2 = countNumberOfStuckTimeViolations(path_case2, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case4 = countNumberOfStuckTimeViolations(path_case4, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    stuck_time_violations_25pct = pd.concat([stuck_time_violations_25pct, temp_case2, temp_case4], ignore_index= True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuck_time_violations_25pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/stuck_time_violations_25_pct.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.5\"]\n",
    "storCapF =  [\"0.5\", \"0.5946\"]\n",
    "\n",
    "stuck_time_violations_50pct = pd.DataFrame()\n",
    "counter = 0\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,2,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "\n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.5\") & (sCf == \"0.5\")):\n",
    "                    alpha = 1.0\n",
    "                    path_case1 = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_sCF_0.5_gS_4711_3765/lausitz-50.0-pct-fCf_sCF_0.5_gS_4711_3765.output_events.xml.gz\"\n",
    "                    path_case3 = \"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_sCF_0.5_gS_4711_sT_60.0_3765/lausitz-50-pct-1-fCf_sCF_0.5_gS_4711_sT_60.0_3765.output_events.xml.gz\"\n",
    "                    \n",
    "                    # calculate number of stuck time violations and attach them to the data frame\n",
    "                    temp_case1 = countNumberOfStuckTimeViolations(path_case1, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case3 = countNumberOfStuckTimeViolations(path_case3, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    stuck_time_violations_50pct = pd.concat([stuck_time_violations_50pct, temp_case1, temp_case3], ignore_index= True)     \n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    path_case2 = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_0.5_sCF_0.5946_gS_4711_3765/lausitz-50.0-pct-fCf_0.5_sCF_0.5946_gS_4711_3765.output_events.xml.gz\"\n",
    "                    path_case4 =\"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_0.5_sCF_0.5946_gS_4711_sT_60.0_3765/lausitz-50-pct-1-fCf_0.5_sCF_0.5946_gS_4711_sT_60.0_3765.output_events.xml.gz\"\n",
    "                    \n",
    "                    # calculate number of stuck time violations and attach them to the data frame\n",
    "                    temp_case2 = countNumberOfStuckTimeViolations(path_case2, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case4 = countNumberOfStuckTimeViolations(path_case4, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    stuck_time_violations_50pct = pd.concat([stuck_time_violations_50pct, temp_case2, temp_case4], ignore_index= True)\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25 pct doubled\n",
    "path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25-pct-doubled-fCf_0.5_sCF_0.5_gS_4711_3765/lausitz-25-pct-doubled-fCf_0.5_sCF_0.5_gS_4711__3765.output_events.xml.gz\"\n",
    "stuck_time_violations_25pct_doubled = countNumberOfStuckTimeViolations(path, \"25-pct-doubled\", 1, 1.0, \"default\", 30.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuck_time_violation_50pct = pd.concat([stuck_time_violations_50pct, stuck_time_violations_25pct_doubled])\n",
    "stuck_time_violation_50pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/stuck_time_violations_50_pct.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 pct\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765/lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765.output_events.xml.gz\"\n",
    "stuck_time_violations_100pct = countNumberOfStuckTimeViolations(path,  \"100-pct\", 1, 1.0, \"default\", 30.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25.0-pct-quadrupled-fCf_1.0_sCF_1.0_gS_4711_3765/lausitz-25-pct-quadrupled-fCf_1.0_sCF_1.0_gS_4711__3765.output_events.xml.gz\"\n",
    "stuck_time_violations_25pct_quadrupled = countNumberOfStuckTimeViolations(path,  \"25-pct-quadrupled\", 1, 1.0, \"default\", 30.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuck_time_violation_100pct = pd.concat([stuck_time_violations_100pct, stuck_time_violations_25pct_quadrupled])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pd.concat and write csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat\n",
    "stuck_time_violation_1_100 = pd.concat([stuck_time_violation_1pct, stuck_time_violation_5pct, stuck_time_violations_10pct,\n",
    "                                        stuck_time_violations_25pct, stuck_time_violations_50pct, stuck_time_violation_100pct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write csv\n",
    "stuck_time_violation_1_100.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/stuck_time_violations_1_to_100pct_July_28_2025.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Count number of link leave events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to filter output_events and count the number of link leave events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of link leave events\n",
    "def countLinkLeaveEvents(pathToEvents, sampleSize, sampleNr, alpha, globalSeed, stuckTime):\n",
    "    events_file = pathToEvents\n",
    "\n",
    "    # Read events - filter and return the listed event types only\n",
    "    events = matsim.event_reader(\n",
    "        events_file,\n",
    "        types=\"left link\",\n",
    "    )\n",
    "\n",
    "   \n",
    "    link_leave_event_counter = 0 \n",
    "\n",
    "    # Loop on all filtered events\n",
    "    for event in events:\n",
    "        if event[\"type\"] == \"left link\":\n",
    "            link_leave_event_counter += 1\n",
    "    temp = pd.DataFrame({'n_link_leave_events': link_leave_event_counter, 'sample_size': sampleSize, 'sample_nr':sampleNr, 'alpha': alpha, 'global_seed': globalSeed, 'stuck_time': stuckTime }, index =[0])\n",
    "    return temp\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.01\"]\n",
    "storCapF =  [\"0.01\", \"0.03162\"]\n",
    "\n",
    "link_leave_events_1pct = pd.DataFrame()\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "        for sampleNr in range(1,11,1):\n",
    "            # calculate adjusted stuck time\n",
    "            default_stuck_time = 30.0\n",
    "            adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "            # declare sample size as str \"1-pct\"\n",
    "            sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "\n",
    "            if ((fCf == \"0.01\") & (sCf == '0.01')):\n",
    "                alpha = 1.0\n",
    "                # paths for case 1 and 3 \n",
    "                path_case1 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" + str(sampleNr) + \"-fCf_sCF_\"+ sCf + \"_gS_default_3765/lausitz-1pct-\"+str(sampleNr)+ \"-fCf_sCf_0.01_gS_default_3765.output_events.xml.gz\"\n",
    "                path_case3 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-1-pct-\" + str(sampleNr) + \"-fCf_sCF_0.01_gS_4711_sT_3000.0_3765.output_events.xml.gz\"\n",
    "                \n",
    "                # calculate number of stuck time violations and attach them to the data frame\n",
    "                temp_case1 = countLinkLeaveEvents(path_case1, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                temp_case3 = countLinkLeaveEvents(path_case3, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                link_leave_events_1pct = pd.concat([link_leave_events_1pct, temp_case1, temp_case3], ignore_index= True)\n",
    "\n",
    "            else:\n",
    "                alpha = 0.75\n",
    "                # paths for case 2 and 4 \n",
    "                path_case2 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" + str(sampleNr) + \"-fCf_0.01_sCF_\" + sCf + \"_gS_default_3765/lausitz-1pct-\" +str(sampleNr)+ \"-fCf_0.01_sCf_0.03162_gS_default_3765.output_events.xml.gz\"\n",
    "                path_case4 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" + str(sampleNr) + \"-fCf_0.01_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-1-pct-\" + str(sampleNr) +\"-fCf_0.01_sCF_0.03162_gS_4711_sT_3000.0_3765.output_events.xml.gz\"\n",
    "                \n",
    "                # calculate number of stuck time violations and attach them to the data frame\n",
    "                temp_case2 = countLinkLeaveEvents(path_case2, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                temp_case4 = countLinkLeaveEvents(path_case4, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                link_leave_events_1pct = pd.concat([link_leave_events_1pct, temp_case2, temp_case4], ignore_index= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1 pct random seed\n",
    "link_leave_1pct_rGs = pd.DataFrame()\n",
    "rGs = [ 4711,3254, 2306, 6384,4338, 6003, 5502, 9377, 5621, 9002 ]\n",
    "for seed in rGs:\n",
    "    global_seed = \"rnd_\" + str(seed)\n",
    "    if (seed == 4711):  \n",
    "        \n",
    "        linke_leave_events = link_leave_events_1pct[(link_leave_events_1pct['alpha'] == 1.0) & (link_leave_events_1pct['stuck_time'] == 30.0) & (link_leave_events_1pct['sample_nr'] == 1)]['n_link_leave_events']\n",
    "        temp = {'n_link_leave_events': linke_leave_events , 'sample_size': '1-pct', 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': global_seed  }\n",
    "        temp = pd.DataFrame(data=temp, index=[rGs.index(4711)])\n",
    "        link_leave_1pct_rGs = pd.concat([link_leave_1pct_rGs, temp])\n",
    "    elif (seed == 3254):\n",
    "        path =\"/home/lola/math_cluster/output/output-lausitz-1pct-1-fCf_sCF_0.01_gS_3254_3765/lausitz-1pct-1-fCf_sCf_0.01_gS_3254_3765.output_events.xml.gz\"\n",
    "        temp = countLinkLeaveEvents(path, \"1-pct\", 1, 1.0, global_seed, 30.0)\n",
    "        link_leave_1pct_rGs = pd.concat([link_leave_1pct_rGs, temp], axis = 0)\n",
    "    else:\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-1.0-pct-1-fCf_sCF_0.01_gS_\" +str(seed) + \"_3765/lausitz-1.0-pct-1-fCf_sCF_0.01_gS_\" + str(seed) + \"_3765.output_events.xml.gz\"\n",
    "        temp = countLinkLeaveEvents(path, \"1-pct\", 1, alpha, global_seed, 30.0)\n",
    "        link_leave_1pct_rGs = pd.concat([link_leave_1pct_rGs, temp], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_leave_events_1pct = pd.concat([link_leave_events_1pct,link_leave_1pct_rGs], axis = 0, ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write 1 pct to csv\n",
    "link_leave_events_1pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/link_leave_1pct_all.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.05\"]\n",
    "storCapF =  [\"0.05\", \"0.10574\"]\n",
    "\n",
    "link_leave_events_5pct = pd.DataFrame()\n",
    "\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,11,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "\n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.05\") & (sCf == \"0.05\")):\n",
    "                    alpha = 1.0\n",
    "                    if (sampleNr == 6):\n",
    "                        path_case1  = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765/lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765-2.output_events.xml.gz\"\n",
    "                    \n",
    "                    else: \n",
    "                        path_case1 = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_3765/lausitz-5.0-pct-\"+str(sampleNr)+ \"-fCf_sCF_0.05_gS_4711_3765.output_events.xml.gz\"\n",
    "                    \n",
    "                    \n",
    "                    path_case3 =  \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-5-pct-\" + str(sampleNr) +\"-fCf_sCF_0.05_gS_4711_sT_600.0_3765.output_events.xml.gz\"\n",
    "\n",
    "                    temp_case1 = countLinkLeaveEvents(path_case1, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case3 = countLinkLeaveEvents(path_case3, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    link_leave_events_5pct = pd.concat([link_leave_events_5pct, temp_case1, temp_case3], ignore_index= True)\n",
    "                    \n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    if(sampleNr == 6):\n",
    "                        path_case2 = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_0.05_sCF_0.10574_gS_4711_3765/lausitz-5.0-pct-6-fCf_0.05_sCF_0.10574_gS_4711_3765-2.output_events.xml.gz\"\n",
    "                    else:\n",
    "                        path_case2 = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf  + \"_gS_4711_3765/lausitz-5.0-pct-\"+str(sampleNr)+ \"-fCf_0.05_sCF_0.10574_gS_4711_3765.output_events.xml.gz\"\n",
    "                    \n",
    "                    \n",
    "                    path_case4 = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str (sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-5-pct-\" + str(sampleNr) + \"-fCf_0.05_sCF_0.10574_gS_4711_sT_600.0_3765.output_events.xml.gz\"\n",
    "\n",
    "                    # calculate number of stuck time violations and attach them to the data frame\n",
    "                    temp_case2 = countLinkLeaveEvents(path_case2, sample_size_as_string, sampleNr, alpha, \"default\", default_stuck_time)\n",
    "                    temp_case4 = countLinkLeaveEvents(path_case4, sample_size_as_string, sampleNr, alpha, \"default\", adjusted_stuck_time)\n",
    "                    link_leave_events_5pct = pd.concat([link_leave_events_5pct, temp_case2, temp_case4], ignore_index= True)\n",
    "                \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_leave_5pct = []\n",
    "for elem in range(1,11,1):\n",
    "    if (elem==6):\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765/lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765-2.output_events.xml.gz\"\n",
    "        link_leave = countLinkLeaveEvents(path)\n",
    "        link_leave_5pct.append(link_leave)\n",
    "    else:\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(elem) + \"-fCf_sCF_0.05_gS_4711_3765/lausitz-5.0-pct-\"+str(elem)+ \"-fCf_sCF_0.05_gS_4711_3765.output_events.xml.gz\"\n",
    "        link_leave = countLinkLeaveEvents(path)\n",
    "        link_leave_5pct.append(link_leave)\n",
    "link_leave_5pct = pd.DataFrame({'n_link_leave': link_leave_5pct})\n",
    "link_leave_5pct.insert(1, 'sample_size', \"5-pct\")\n",
    "link_leave_5pct.insert(2, 'alpha', 1.0)\n",
    "link_leave_5pct.insert(3, 'global_seed', \"default\")\n",
    "link_leave_5pct.insert(4, 'stuck_time', 30.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_leave_5pct_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "    if (elem==6):\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_0.05_sCF_0.10574_gS_4711_3765/lausitz-5.0-pct-6-fCf_0.05_sCF_0.10574_gS_4711_3765-2.output_events.xml.gz\"\n",
    "        link_leave = countLinkLeaveEvents(path)\n",
    "        link_leave_5pct_sCf.append(link_leave)\n",
    "    else:\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(elem) + \"-fCf_0.05_sCF_0.10574_gS_4711_3765/lausitz-5.0-pct-\"+str(elem)+ \"-fCf_0.05_sCF_0.10574_gS_4711_3765.output_events.xml.gz\"\n",
    "        link_leave = countLinkLeaveEvents(path)\n",
    "        link_leave_5pct_sCf.append(link_leave)\n",
    "link_leave_5pct_sCf = pd.DataFrame({'n_link_leave': link_leave_5pct_sCf})\n",
    "link_leave_5pct_sCf.insert(1, 'sample_size', \"5-pct\")\n",
    "link_leave_5pct_sCf.insert(2, 'alpha', 0.75)\n",
    "link_leave_5pct_sCf.insert(3, 'global_seed', \"default\")\n",
    "link_leave_5pct_sCf.insert(4, 'stuck_time', 30.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 pct random global seed, alpha = 1\n",
    "link_leave_5pct_rGs = pd.DataFrame()\n",
    "rGs = [4711, 3254, 2306, 6384,4338, 6003, 5502, 9377, 5621, 9002 ]\n",
    "\n",
    "for seed in rGs:\n",
    "    if (seed ==4711):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        temp = {'n_link_leave': link_leave_5pct[\"n_link_leave\"].iloc[0], 'sample_size': '5-pct', 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': global_seed  }\n",
    "        temp = pd.DataFrame(data=temp, index=[rGs.index(4711)])\n",
    "        link_leave_5pct_rGs = pd.concat([link_leave_5pct_rGs, temp])\n",
    "    else:\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-1-fCf_sCF_0.05_gS_\" + str(seed) + \"_3765/lausitz-5.0-pct-1-fCf_sCF_0.05_gS_\"+ str(seed) +\"_3765.output_events.xml.gz\"\n",
    "        link_leave = countLinkLeaveEvents(path)\n",
    "        temp2 = pd.DataFrame({'n_link_leave': link_leave, 'sample_size': '5-pct', 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': \"rnd_\" + str(seed)  }, index = [rGs.index(seed)])\n",
    "        link_leave_5pct_rGs = pd.concat([link_leave_5pct_rGs, temp2], axis = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 pct, alpha = 1.0, sT scaled\n",
    "link_leave_5pct_sT = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str(elem) + \"-fCf_sCF_0.05_gS_4711_sT_600.0_3765/lausitz-5-pct-\" + str(elem) +\"-fCf_sCF_0.05_gS_4711_sT_600.0_3765.output_events.xml.gz\"\n",
    "    link_leave = countLinkLeaveEvents(path)\n",
    "    link_leave_5pct_sT.append(link_leave)\n",
    "link_leave_5pct_sT = pd.DataFrame({'n_link_leave': link_leave_5pct_sT})\n",
    "\n",
    "link_leave_5pct_sT.insert(1, 'sample_size', \"5-pct\")\n",
    "link_leave_5pct_sT.insert(2, 'alpha', 1.0)\n",
    "link_leave_5pct_sT.insert(3, 'global_seed', \"default\")\n",
    "link_leave_5pct_sT.insert(4, 'stuck_time', 600.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 pct, sT scaled, alpha = 0.75\n",
    "# 5 pct, alpha = 0.75, sT scaled\n",
    "link_leave_5pct_sT_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str (elem) + \"-fCf_0.05_sCF_0.10574_gS_4711_sT_600.0_3765/lausitz-5-pct-\" + str(elem) + \"-fCf_0.05_sCF_0.10574_gS_4711_sT_600.0_3765.output_events.xml.gz\"\n",
    "    link_leave = countLinkLeaveEvents(path)\n",
    "    link_leave_5pct_sT_sCf.append(link_leave)\n",
    "link_leave_5pct_sT_sCf = pd.DataFrame({'n_link_leave': link_leave_5pct_sT_sCf})\n",
    "\n",
    "link_leave_5pct_sT_sCf.insert(1, 'sample_size', \"5-pct\")\n",
    "link_leave_5pct_sT_sCf.insert(2, 'alpha', 0.75)\n",
    "link_leave_5pct_sT_sCf.insert(3, 'global_seed', \"default\")\n",
    "link_leave_5pct_sT_sCf.insert(4, 'stuck_time', 600.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_leave_5pct_all = pd.concat([link_leave_5pct, link_leave_5pct_sCf, link_leave_5pct_rGs, link_leave_5pct_sT, link_leave_5pct_sT_sCf], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_leave_5pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/link_leave_5pct_all.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_leave_10pct = []\n",
    "for elem in range(1,11,1):\n",
    "    #       /home/lola/math_cluster/output/output-lausitz-10.0-pct-1-fCf_sCF_0.1_gS_4711_3765/lausitz-10.0-pct-1-fCf_sCF_0.1_gS_4711_3765.output_events.xml.gz\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" + str(elem) + \"-fCf_sCF_0.1_gS_4711_3765/lausitz-10.0-pct-\"+str(elem)+ \"-fCf_sCF_0.1_gS_4711_3765.output_events.xml.gz\"\n",
    "    link_leave = countLinkLeaveEvents(path)\n",
    "    link_leave_10pct.append(link_leave)\n",
    "link_leave_10pct = pd.DataFrame({'n_link_leave': link_leave_10pct})\n",
    "link_leave_10pct.insert(1, 'sample_size', \"10-pct\")\n",
    "link_leave_10pct.insert(2, 'alpha', 1.0)\n",
    "link_leave_10pct.insert(3, 'global_seed', \"default\")\n",
    "link_leave_10pct.insert(4, 'stuck_time', 30.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_leave_10pct_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "    #       /home/lola/math_cluster/output/output-lausitz-10.0-pct-1-fCf_0.1_sCF_0.17783_gS_4711_3765/lausitz-10.0-pct-1-fCf_0.1_sCF_0.17783_gS_4711_3765.output_events.xml.gz     \n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" + str(elem) + \"-fCf_0.1_sCF_0.17783_gS_4711_3765/lausitz-10.0-pct-\" +str(elem)+ \"-fCf_0.1_sCF_0.17783_gS_4711_3765.output_events.xml.gz\"\n",
    "    link_leave = countLinkLeaveEvents(path)\n",
    "    link_leave_10pct_sCf.append(link_leave)\n",
    "link_leave_10pct_sCf = pd.DataFrame({'n_link_leave': link_leave_10pct_sCf})\n",
    "link_leave_10pct_sCf.insert(1, 'sample_size', \"10-pct\")\n",
    "link_leave_10pct_sCf.insert(2, 'alpha', 0.75)\n",
    "link_leave_10pct_sCf.insert(3, 'global_seed', \"default\")\n",
    "link_leave_10pct_sCf.insert(4, 'stuck_time', 30.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 pct, alpha = 1, sT scaled\n",
    "link_leave_10pct_sT = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\" + str(elem) + \"-fCf_sCF_0.1_gS_4711_sT_300.0_3765/lausitz-10-pct-\" + str(elem) + \"-fCf_sCF_0.1_gS_4711_sT_300.0_3765.output_events.xml.gz\"\n",
    "    link_leave = countLinkLeaveEvents(path)\n",
    "    link_leave_10pct_sT.append(link_leave)\n",
    "link_leave_10pct_sT = pd.DataFrame({'n_link_leave': link_leave_10pct_sT})\n",
    "link_leave_10pct_sT.insert(1, 'sample_size', \"10-pct\")\n",
    "link_leave_10pct_sT.insert(2, 'alpha', 1.0)\n",
    "link_leave_10pct_sT.insert(3, 'global_seed', \"default\")\n",
    "link_leave_10pct_sT.insert(4, 'stuck_time', 300.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 pct, alpha = 0.75, sT scaled\n",
    "link_leave_10pct_sT_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\" + str(elem) + \"-fCf_0.1_sCF_0.17783_gS_4711_sT_300.0_3765/lausitz-10-pct-\" + str(elem) + \"-fCf_0.1_sCF_0.17783_gS_4711_sT_300.0_3765.output_events.xml.gz\"\n",
    "    link_leave = countLinkLeaveEvents(path)\n",
    "    link_leave_10pct_sT_sCf.append(link_leave)\n",
    "link_leave_10pct_sT_sCf = pd.DataFrame({'n_link_leave': link_leave_10pct_sT_sCf})\n",
    "link_leave_10pct_sT_sCf.insert(1, 'sample_size', \"10-pct\")\n",
    "link_leave_10pct_sT_sCf.insert(2, 'alpha', 0.75)\n",
    "link_leave_10pct_sT_sCf.insert(3, 'global_seed', \"default\")\n",
    "link_leave_10pct_sT_sCf.insert(4, 'stuck_time', 300.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_leave_10_pct_all = pd.concat([link_leave_10pct, link_leave_10pct_sCf, link_leave_10pct_sT, link_leave_10pct_sT_sCf], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write csv\n",
    "link_leave_10_pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/link_leave_10_pct_all.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25 pct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25 pct, alpha = 1\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_sCF_0.25_gS_4711_3765/lausitz-25.0-pct-fCf_sCF_0.25_gS_4711_3765.output_events.xml.gz\"\n",
    "link_leave = countLinkLeaveEvents(path)\n",
    "link_leave_25pct = pd.DataFrame({'n_link_leave': link_leave,'sample_size': \"25-pct\", 'alpha': 1.0,\n",
    "                                'global_seed': \"default\", 'stuck_time': 30.0 }, index= [0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25 pct, alpha = 0.75\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_0.25_sCF_0.35355_gS_4711_3765/lausitz-25.0-pct-fCf_0.25_sCF_0.35355_gS_4711_3765.output_events.xml.gz\"\n",
    "link_leave = countLinkLeaveEvents(path)\n",
    "link_leave_25pct_sCf = pd.DataFrame({'n_link_leave': link_leave,'sample_size': \"25-pct\", 'alpha': 0.75,\n",
    "                                'global_seed': \"default\", 'stuck_time': 30.0 }, index = [1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25 pct, alpha = 1, sT scaled\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-25-pct-1-fCf_sCF_0.25_gS_4711_sT_120.0_3765/lausitz-25-pct-1-fCf_sCF_0.25_gS_4711_sT_120.0_3765.output_events.xml.gz\"\n",
    "link_leave = countLinkLeaveEvents(path)\n",
    "link_leave_25pct_sT = pd.DataFrame({'n_link_leave': link_leave,'sample_size': \"25-pct\", 'alpha': 1.0,\n",
    "                                'global_seed': \"default\", 'stuck_time': 120.0 }, index = [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25 pct, alpha = 0.75, sT scaled\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-25-pct-1-fCf_0.25_sCF_0.35355_gS_4711_sT_120.0_3765/lausitz-25-pct-1-fCf_0.25_sCF_0.35355_gS_4711_sT_120.0_3765.output_events.xml.gz\"\n",
    "link_leave = countLinkLeaveEvents(path)\n",
    "link_leave_25pct_sT_sCf = pd.DataFrame({'n_link_leave': link_leave,'sample_size': \"25-pct\", 'alpha': 0.75,\n",
    "                                'global_seed': \"default\", 'stuck_time': 120.0 }, index = [3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_leave_25_pct_all = pd.concat([link_leave_25pct, link_leave_25pct_sCf, link_leave_25pct_sT, link_leave_25pct_sT_sCf], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write csv\n",
    "link_leave_25_pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/link_leave_25_pct_all.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 pct, alpha = 1\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_sCF_0.5_gS_4711_3765/lausitz-50.0-pct-fCf_sCF_0.5_gS_4711_3765.output_events.xml.gz\"\n",
    "link_leave = countLinkLeaveEvents(path)\n",
    "link_leave_50pct = pd.DataFrame({'n_link_leave': link_leave,'sample_size': \"50-pct\", 'alpha': 1.0,\n",
    "                                'global_seed': \"default\", 'stuck_time': 30.0 }, index = [0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 pct, alpha = 0.75\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_0.5_sCF_0.5946_gS_4711_3765/lausitz-50.0-pct-fCf_0.5_sCF_0.5946_gS_4711_3765.output_events.xml.gz\"\n",
    "link_leave = countLinkLeaveEvents(path)\n",
    "link_leave_50pct_sCf = pd.DataFrame({'n_link_leave': link_leave,'sample_size': \"50-pct\", 'alpha': 0.75,\n",
    "                                'global_seed': \"default\", 'stuck_time': 30.0 }, index = [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 pct, alpha = 1, sT scaled\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_sCF_0.5_gS_4711_sT_60.0_3765/lausitz-50-pct-1-fCf_sCF_0.5_gS_4711_sT_60.0_3765.output_events.xml.gz\"\n",
    "link_leave = countLinkLeaveEvents(path)\n",
    "link_leave_50pct_sT = pd.DataFrame({'n_link_leave': link_leave,'sample_size': \"50-pct\", 'alpha': 1.0,\n",
    "                                'global_seed': \"default\", 'stuck_time': 60.0 }, index = [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 pct, alpha = 0.75, sT scaled\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_0.5_sCF_0.5946_gS_4711_sT_60.0_3765/lausitz-50-pct-1-fCf_0.5_sCF_0.5946_gS_4711_sT_60.0_3765.output_events.xml.gz\"\n",
    "link_leave = countLinkLeaveEvents(path)\n",
    "link_leave_50pct_sT_sCf = pd.DataFrame({'n_link_leave': link_leave,'sample_size': \"50-pct\", 'alpha': 0.75,\n",
    "                                'global_seed': \"default\", 'stuck_time': 60.0 }, index = [3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_leave_50_pct_all = pd.concat([link_leave_50pct, link_leave_50pct_sCf, link_leave_50pct_sT, link_leave_50pct_sT_sCf], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write csv\n",
    "link_leave_50_pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/link_leave_50_pct_all.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 pct\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765/lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765.output_events.xml.gz\"\n",
    "link_leave = countLinkLeaveEvents(path)\n",
    "link_leave_100pct = pd.DataFrame({'n_link_leave': link_leave,'sample_size': \"100-pct\", 'alpha': 1.0,\n",
    "                                'global_seed': \"default\", 'stuck_time': 30.0 }, index = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write csv\n",
    "link_leave_100pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/link_leave_only_100pct.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_leave_1_100 = pd.concat([link_leave_1pct_all, link_leave_5pct_all, link_leave_10_pct_all, link_leave_25_pct_all, link_leave_50_pct_all, link_leave_100pct ], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write csv\n",
    "link_leave_1_100.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/link_leave_1_100pct.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Determine average speed per road type and hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def networkInfoToDf(pathToNetwork):\n",
    "    # pathToNetwork\n",
    "    # '/home/lola/Downloads/lausitz-v2024.2-network-with-pt.xml.gz'\n",
    "    input = gzip.open(pathToNetwork, 'r')\n",
    "    tree = ET.parse(input)\n",
    "    root = tree.getroot()\n",
    "    # convert network to data frame \n",
    "    ids = []\n",
    "    length = []\n",
    "    freespeed = []\n",
    "    capacity = []\n",
    "    type_of_link = []\n",
    "\n",
    "    for links in root.findall('links'):\n",
    "        for link in links:\n",
    "            #print(link.tag, link.attrib)\n",
    "            ids.append(link.attrib['id'])\n",
    "            length.append(float(link.attrib['length']))\n",
    "            freespeed.append(float(link.attrib['freespeed'])*3.6)\n",
    "            capacity.append(float(link.attrib['capacity']))\n",
    "            type_counter = 0\n",
    "            for child in link:\n",
    "                for attr in child:\n",
    "                    if (attr.attrib['name'] == \"type\"):\n",
    "                        try:\n",
    "                            type_of_link.append(attr.text)\n",
    "                            type_counter = 1\n",
    "                        except:\n",
    "                            type_of_link.append('NA')\n",
    "            if(type_counter == 0):\n",
    "                type_of_link.append('NA')\n",
    "    network_df = pd.DataFrame({'ID': ids, 'length': length, 'freespeed': freespeed, 'capacity': capacity, 'type': type_of_link })\n",
    "    return network_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToLausitzNetwork = '/home/lola/Downloads/lausitz-v2024.2-network-with-pt.xml.gz'\n",
    "df_net = networkInfoToDf(pathToLausitzNetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToEventsActend= \"/home/lola/math_cluster/output/output-lausitz-1pct-1-fCf_sCF_0.01_gS_default_3765/lausitz-1pct-1-fCf_sCf_0.01_gS_default_3765.output_events.xml.gz\"\n",
    "\n",
    "enter_events = matsim.event_reader(pathToEventsActend, types= \"departure,entered link,left link,arrival\")\n",
    "\n",
    "# departure\n",
    "time_dep = []\n",
    "id_dep= []\n",
    "link_dep = []\n",
    "legMode_dep = []\n",
    "\n",
    "# entered\n",
    "time_entered = []\n",
    "vehicle_entered = []\n",
    "link_entered = []\n",
    "type_entered = []\n",
    "\n",
    "# left\n",
    "time_left = []\n",
    "vehicle_left = []\n",
    "link_left = []\n",
    "type_left = []\n",
    "\n",
    "# arrival\n",
    "time_arr = []\n",
    "id_arr = []\n",
    "link_arr = []\n",
    "type_arr = []\n",
    "legMode_arr = []\n",
    "\n",
    "for event in enter_events:\n",
    "    if event['type'] == \"departure\":\n",
    "        time_dep.append(event['time'])\n",
    "        id_dep.append(event['person'])\n",
    "        link_dep.append(event['link'])\n",
    "        legMode_dep.append(event['legMode'])\n",
    "    elif  event['type'] == \"entered link\":\n",
    "        time_entered.append(event['time'])\n",
    "        vehicle_entered.append(event['vehicle'])\n",
    "        link_entered.append(event['link'])\n",
    "        type_entered.append(event['type'])    \n",
    "    elif  event['type'] == \"left link\":\n",
    "        time_left.append(event['time'])\n",
    "        vehicle_left.append(event['vehicle'])\n",
    "        link_left.append(event['link'])\n",
    "        type_left.append(event['type'])\n",
    "    elif event['type'] == \"arrival\":\n",
    "        time_arr.append(event['time'])\n",
    "        link_arr.append(event['link'])\n",
    "        type_arr.append(event['type'])\n",
    "        id_arr.append(event['person'])\n",
    "        legMode_arr.append(event['legMode'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "departures = pd.DataFrame({'ID': id_dep, 'time_dep': time_dep, 'link_dep': link_dep, 'legMode': legMode_dep})\n",
    "departures = departures[(departures['ID'].str.contains('pt_') == False)]\n",
    "departures = departures[(departures['legMode'].str.contains('car') == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['328016866', '-31963626'], dtype=object)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(departures[departures['ID'] == \"258\"]['link_dep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrivals = pd.DataFrame({'ID': id_arr, 'time_arr': time_arr, 'link_arr': link_arr, 'legMode': legMode_arr})\n",
    "arrivals = arrivals[(arrivals['ID'].str.contains('pt_') == False)]\n",
    "arrivals = arrivals[(arrivals['legMode'].str.contains('car') == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>time_arr</th>\n",
       "      <th>link_arr</th>\n",
       "      <th>legMode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>380570</td>\n",
       "      <td>12162.0</td>\n",
       "      <td>150117434</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>302983</td>\n",
       "      <td>12605.0</td>\n",
       "      <td>-154388434#6</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>385943</td>\n",
       "      <td>13457.0</td>\n",
       "      <td>-59428113#1</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>289176</td>\n",
       "      <td>13876.0</td>\n",
       "      <td>-16854749</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>533673</td>\n",
       "      <td>14210.0</td>\n",
       "      <td>-23678816#1</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60662</th>\n",
       "      <td>1028527</td>\n",
       "      <td>104686.0</td>\n",
       "      <td>98941287#0</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60671</th>\n",
       "      <td>307825</td>\n",
       "      <td>110306.0</td>\n",
       "      <td>232865155</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60677</th>\n",
       "      <td>494385</td>\n",
       "      <td>112573.0</td>\n",
       "      <td>27077724</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60680</th>\n",
       "      <td>924963</td>\n",
       "      <td>112954.0</td>\n",
       "      <td>4446448</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60686</th>\n",
       "      <td>922</td>\n",
       "      <td>114717.0</td>\n",
       "      <td>-1108379497</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15647 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  time_arr      link_arr legMode\n",
       "13      380570   12162.0     150117434     car\n",
       "16      302983   12605.0  -154388434#6     car\n",
       "22      385943   13457.0   -59428113#1     car\n",
       "28      289176   13876.0     -16854749     car\n",
       "31      533673   14210.0   -23678816#1     car\n",
       "...        ...       ...           ...     ...\n",
       "60662  1028527  104686.0    98941287#0     car\n",
       "60671   307825  110306.0     232865155     car\n",
       "60677   494385  112573.0      27077724     car\n",
       "60680   924963  112954.0       4446448     car\n",
       "60686      922  114717.0   -1108379497     car\n",
       "\n",
       "[15647 rows x 4 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrivals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_storage = []\n",
    "arr_storage = []\n",
    "person_storage = []\n",
    "trip_number = []\n",
    "depLink_storage = []\n",
    "arrLink_storage = []\n",
    "trip_id_storage = []\n",
    "\n",
    "unique_person = departures['ID'].unique()\n",
    "for person in unique_person:\n",
    "    temp_depLink = np.array(departures[departures['ID'] == person]['link_dep'])\n",
    "    temp_arrLink = np.array(arrivals[arrivals['ID'] == person]['link_arr'])\n",
    "    temp_dep = np.array(departures[departures['ID'] == person]['time_dep'])\n",
    "    temp_arr = np.array(arrivals[arrivals['ID'] == person]['time_arr'])\n",
    "    for element in range(0, len(temp_dep),1):\n",
    "        trip_id_storage.append(str(person) +\"_\" +str(element +1))\n",
    "        person_storage.append(person)\n",
    "        trip_number.append(element +1)\n",
    "        dep_storage.append(temp_dep[element])\n",
    "        arr_storage.append(temp_arr[element])\n",
    "        depLink_storage.append(temp_depLink[element])\n",
    "        arrLink_storage.append(temp_arrLink[element])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dep_arr = pd.DataFrame({'person_id': person_storage,'trip_number': trip_number, 'trip_id': trip_id_storage,'dep_time': dep_storage, 'dep_link': depLink_storage, 'arr_time': arr_storage, 'arr_link': arrLink_storage})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToEventsEnter = \"/home/lola/math_cluster/output/output-lausitz-1pct-1-fCf_sCF_0.01_gS_default_3765/lausitz-1pct-1-fCf_sCf_0.01_gS_default_3765.output_events.xml.gz\"\n",
    "\n",
    "enter_events = matsim.event_reader(pathToEventsEnter, types= \"entered link\")\n",
    "\n",
    "time_entered = []\n",
    "vehicle_entered = []\n",
    "link_entered = []\n",
    "type_entered = []\n",
    "\n",
    "for event in enter_events:\n",
    "    if event[\"type\"] == \"entered link\":\n",
    "        time_entered.append(event['time'])\n",
    "        vehicle_entered.append(event['vehicle'])\n",
    "        link_entered.append(event['link'])\n",
    "        type_entered.append(event['type'])\n",
    "        \n",
    "df_linkEnter = pd.DataFrame({'enter_time' : time_entered, 'link_id': link_entered, 'vehicle_id': vehicle_entered, 'type_of_event': type_entered})\n",
    "df_enter_no_pt = df_linkEnter[(df_linkEnter['link_id'].str.contains('pt_') == False)].copy()\n",
    "df_enter_no_pt = pd.merge(df_enter_no_pt, df_net[['ID', 'type']], how='left', left_on='link_id', right_on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToEventsStuck= \"/home/lola/math_cluster/output/output-lausitz-1pct-1-fCf_sCF_0.01_gS_default_3765/lausitz-1pct-1-fCf_sCf_0.01_gS_default_3765.output_events.xml.gz\"\n",
    "\n",
    "enter_events = matsim.event_reader(pathToEventsEnter, types= \"stuckAndContinue\")\n",
    "\n",
    "time_entered = []\n",
    "vehicle_entered = []\n",
    "link_entered = []\n",
    "type_entered = []\n",
    "\n",
    "for event in enter_events:\n",
    "    if event[\"type\"] == \"stuckAndContinue\":\n",
    "        time_entered.append(event['time'])\n",
    "        vehicle_entered.append(event['person'] + \"_car\")\n",
    "        link_entered.append(event['link'])\n",
    "        type_entered.append(event['type'])\n",
    "        \n",
    "df_linkStuck = pd.DataFrame({'time_leave' : time_entered, 'link_id': link_entered, 'vehicle_id': vehicle_entered, 'type_of_event': type_entered})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToEventsLeave = \"/home/lola/math_cluster/output/output-lausitz-1pct-1-fCf_sCF_0.01_gS_default_3765/lausitz-1pct-1-fCf_sCf_0.01_gS_default_3765.output_events.xml.gz\"\n",
    "leave_events = matsim.event_reader(pathToEventsLeave, types= \"left link\")\n",
    "\n",
    "time_left = []\n",
    "vehicle_left = []\n",
    "link_left = []\n",
    "type_left = []\n",
    "\n",
    "for event in leave_events: \n",
    "    if event[\"type\"] == \"left link\":\n",
    "        time_left.append(event['time'])\n",
    "        vehicle_left.append(event['vehicle'])\n",
    "        link_left.append(event['link'])\n",
    "        type_left.append(event['type'])\n",
    "\n",
    "\n",
    "df_linkLeft = pd.DataFrame({'leave_time' : time_left, 'link_id': link_left, 'vehicle_id': vehicle_left, 'type_of_event': type_left})\n",
    "df_leave_no_pt = df_linkLeft[(df_linkLeft['link_id'].str.contains('pt_') == False )]\n",
    "df_leave_no_pt = pd.merge(df_leave_no_pt, df_net[['ID', 'type']], how='left', left_on='link_id', right_on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to plans\n",
    "pathToPlans = \"/home/lola/math_cluster/output/output-lausitz-1pct-1-fCf_sCF_0.01_gS_default_3765/lausitz-1pct-1-fCf_sCf_0.01_gS_default_3765.output_experienced_plans.xml.gz\"\n",
    "\n",
    "input = gzip.open(pathToPlans, 'r')\n",
    "tree = ET.parse(input)\n",
    "root = tree.getroot()\n",
    "# convert network to data frame \n",
    "\n",
    "\n",
    "person_id2 = []\n",
    "vehicle_id = []\n",
    "route_storage2 =[]\n",
    "travel_time2 = []\n",
    "trip_id_storage2 = []\n",
    "start_link = []\n",
    "end_link = []\n",
    "first_route = []\n",
    "last_route = []\n",
    "\n",
    "#iterate over a persons in the experienced plans file \n",
    "for person in root.findall('person'):\n",
    "    trip_counter = 0\n",
    "    # there is only the selected plan in the experienced plans file, so it is enough to only find all plans which are children of a person\n",
    "    for plan in person.findall('plan'):\n",
    "          if(plan.attrib['selected'] != 'yes'):\n",
    "              continue\n",
    "          else:     \n",
    "            # find all legs\n",
    "            for leg in plan.findall('leg'):\n",
    "                #print(leg.attrib['mode'])\n",
    "                # only for car legs\n",
    "                if (leg.attrib['mode'] != \"car\"):\n",
    "                    continue\n",
    "                elif(leg.attrib['mode'] == \"car\"):\n",
    "                    trip_counter += 1\n",
    "                    # find the route\n",
    "                    for route in leg.findall('route'):              \n",
    "                        # get all links of the route:\n",
    "                        temp_route = route.text.split()\n",
    "                        if (len(temp_route) == 0):\n",
    "                            print(\"route of length 0 \")\n",
    "                            continue\n",
    "                        else: \n",
    "                            if((route.attrib['start_link'] == temp_route[0]) & ((route.attrib['end_link'] == temp_route[len(temp_route)-1])) ):\n",
    "                                for element in temp_route[1:len(temp_route)-2]:\n",
    "                                   person_id2.append(person.attrib['id'])\n",
    "                                   route_storage2.append(element)\n",
    "                                   #travel_time2.append(route.attrib['trav_time'])\n",
    "                                   trip_id_storage2.append(person.attrib['id'] + \"_\" + str(trip_counter))\n",
    "                                   vehicle_id.append(person.attrib['id'] + \"_car\")\n",
    "                                   start_link.append(temp_route[0])\n",
    "                                   end_link.append(temp_route[len(temp_route)-1])\n",
    "                                   first_route.append(temp_route[1])\n",
    "                                   last_route.append(temp_route[len(temp_route)-2])\n",
    "\n",
    "                            elif((route.attrib['start_link'] == temp_route[0]) & ((route.attrib['end_link'] != temp_route[len(temp_route)-1])) ):\n",
    "                                for element in temp_route[1:len(temp_route)-1]:\n",
    "                                   person_id2.append(person.attrib['id'])\n",
    "                                   route_storage2.append(element)\n",
    "                                   #travel_time2.append(route.attrib['trav_time'])\n",
    "                                   trip_id_storage2.append(person.attrib['id'] + \"_\" + str(trip_counter))\n",
    "                                   vehicle_id.append(person.attrib['id'] + \"_car\")\n",
    "                                   start_link.append(temp_route[0])\n",
    "                                   end_link.append(route.attrib['end_link'])\n",
    "                                   first_route.append(temp_route[1])\n",
    "                                   last_route.append(temp_route[len(temp_route)-1])\n",
    "\n",
    "                            elif((route.attrib['start_link'] != temp_route[0]) & ((route.attrib['end_link'] == temp_route[len(temp_route)-1])) ):\n",
    "                                for element in temp_route[0:len(temp_route)-2]:\n",
    "                                   person_id2.append(person.attrib['id'])\n",
    "                                   route_storage2.append(element)\n",
    "                                   #travel_time2.append(route.attrib['trav_time'])\n",
    "                                   trip_id_storage2.append(person.attrib['id'] + \"_\" + str(trip_counter))\n",
    "                                   vehicle_id.append(person.attrib['id'] + \"_car\")\n",
    "                                   start_link.append(route.attrib['start_link'])\n",
    "                                   end_link.append(temp_route[len(temp_route)-1])\n",
    "                                   first_route.append(temp_route[0])\n",
    "                                   last_route.append(temp_route[len(temp_route)-2])\n",
    "                            \n",
    "\n",
    "\n",
    "                            else: \n",
    "                                for element in temp_route:\n",
    "                                   person_id2.append(person.attrib['id'])\n",
    "                                   route_storage2.append(element)\n",
    "                                   #travel_time2.append(route.attrib['trav_time'])\n",
    "                                   trip_id_storage2.append(person.attrib['id'] + \"_\" + str(trip_counter))\n",
    "                                   vehicle_id.append(person.attrib['id'] + \"_car\")\n",
    "                                   start_link.append(route.attrib['start_link'])\n",
    "                                   end_link.append(route.attrib['end_link'])\n",
    "                                   first_route.append(temp_route[0])\n",
    "                                   last_route.append(temp_route[len(temp_route)-1])\n",
    "                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Person_Link = pd.DataFrame({'person': person_id2, 'trip_id':trip_id_storage2, 'vehicle_id': vehicle_id, 'link_id':route_storage2, 'start_link': start_link, 'end_link': end_link,})\n",
    "#df_Person_Link['trav_time_seconds'] = pd.to_timedelta(df_Person_Link['trav_time']) / np.timedelta64(1, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Person_Link = pd.merge(df_Person_Link, df_dep_arr[['trip_id', 'dep_time', 'dep_link', 'arr_time', 'arr_link']], on = \"trip_id\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Person_Link = pd.merge(df_Person_Link, df_net[['ID', 'type', 'length']], how='left', left_on='link_id', right_on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Person_Link = df_Person_Link[(df_Person_Link['type'] == 'highway.secondary') | (df_Person_Link['type'] == 'highway.residential') | (df_Person_Link['type'] == 'highway.tertiary') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Person_Link = df_Person_Link.drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"CREATE TABLE PersonLink AS SELECT * FROM df_Person_Link\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"CREATE TABLE LinkEnter AS SELECT * FROM df_enter_no_pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"CREATE TABLE LinkLeave AS SELECT * FROM df_leave_no_pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person</th>\n",
       "      <th>trip_id</th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>start_link</th>\n",
       "      <th>end_link</th>\n",
       "      <th>dep_time</th>\n",
       "      <th>dep_link</th>\n",
       "      <th>arr_time</th>\n",
       "      <th>arr_link</th>\n",
       "      <th>type</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100020</td>\n",
       "      <td>100020_1</td>\n",
       "      <td>100020_car</td>\n",
       "      <td>390466917#1</td>\n",
       "      <td>-129258289</td>\n",
       "      <td>481651812#0</td>\n",
       "      <td>46485.0</td>\n",
       "      <td>-129258289</td>\n",
       "      <td>53450.0</td>\n",
       "      <td>481651812#0</td>\n",
       "      <td>highway.residential</td>\n",
       "      <td>113.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100020</td>\n",
       "      <td>100020_1</td>\n",
       "      <td>100020_car</td>\n",
       "      <td>-389439108#0</td>\n",
       "      <td>-129258289</td>\n",
       "      <td>481651812#0</td>\n",
       "      <td>46485.0</td>\n",
       "      <td>-129258289</td>\n",
       "      <td>53450.0</td>\n",
       "      <td>481651812#0</td>\n",
       "      <td>highway.residential</td>\n",
       "      <td>248.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100020</td>\n",
       "      <td>100020_1</td>\n",
       "      <td>100020_car</td>\n",
       "      <td>-225101773#1</td>\n",
       "      <td>-129258289</td>\n",
       "      <td>481651812#0</td>\n",
       "      <td>46485.0</td>\n",
       "      <td>-129258289</td>\n",
       "      <td>53450.0</td>\n",
       "      <td>481651812#0</td>\n",
       "      <td>highway.residential</td>\n",
       "      <td>48.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100020</td>\n",
       "      <td>100020_1</td>\n",
       "      <td>100020_car</td>\n",
       "      <td>762963414</td>\n",
       "      <td>-129258289</td>\n",
       "      <td>481651812#0</td>\n",
       "      <td>46485.0</td>\n",
       "      <td>-129258289</td>\n",
       "      <td>53450.0</td>\n",
       "      <td>481651812#0</td>\n",
       "      <td>highway.residential</td>\n",
       "      <td>128.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100020</td>\n",
       "      <td>100020_1</td>\n",
       "      <td>100020_car</td>\n",
       "      <td>225101767#0</td>\n",
       "      <td>-129258289</td>\n",
       "      <td>481651812#0</td>\n",
       "      <td>46485.0</td>\n",
       "      <td>-129258289</td>\n",
       "      <td>53450.0</td>\n",
       "      <td>481651812#0</td>\n",
       "      <td>highway.residential</td>\n",
       "      <td>91.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821787</th>\n",
       "      <td>999971</td>\n",
       "      <td>999971_3</td>\n",
       "      <td>999971_car</td>\n",
       "      <td>15488937#3</td>\n",
       "      <td>-24397343#0</td>\n",
       "      <td>-303025950</td>\n",
       "      <td>72011.0</td>\n",
       "      <td>-24397343#0</td>\n",
       "      <td>72672.0</td>\n",
       "      <td>-303025950</td>\n",
       "      <td>highway.secondary</td>\n",
       "      <td>18.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821788</th>\n",
       "      <td>999971</td>\n",
       "      <td>999971_3</td>\n",
       "      <td>999971_car</td>\n",
       "      <td>12830362</td>\n",
       "      <td>-24397343#0</td>\n",
       "      <td>-303025950</td>\n",
       "      <td>72011.0</td>\n",
       "      <td>-24397343#0</td>\n",
       "      <td>72672.0</td>\n",
       "      <td>-303025950</td>\n",
       "      <td>highway.secondary</td>\n",
       "      <td>41.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821789</th>\n",
       "      <td>999971</td>\n",
       "      <td>999971_3</td>\n",
       "      <td>999971_car</td>\n",
       "      <td>509752406</td>\n",
       "      <td>-24397343#0</td>\n",
       "      <td>-303025950</td>\n",
       "      <td>72011.0</td>\n",
       "      <td>-24397343#0</td>\n",
       "      <td>72672.0</td>\n",
       "      <td>-303025950</td>\n",
       "      <td>highway.secondary</td>\n",
       "      <td>6.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821790</th>\n",
       "      <td>999971</td>\n",
       "      <td>999971_3</td>\n",
       "      <td>999971_car</td>\n",
       "      <td>571374633#0</td>\n",
       "      <td>-24397343#0</td>\n",
       "      <td>-303025950</td>\n",
       "      <td>72011.0</td>\n",
       "      <td>-24397343#0</td>\n",
       "      <td>72672.0</td>\n",
       "      <td>-303025950</td>\n",
       "      <td>highway.secondary</td>\n",
       "      <td>87.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821791</th>\n",
       "      <td>999971</td>\n",
       "      <td>999971_3</td>\n",
       "      <td>999971_car</td>\n",
       "      <td>571374633#1</td>\n",
       "      <td>-24397343#0</td>\n",
       "      <td>-303025950</td>\n",
       "      <td>72011.0</td>\n",
       "      <td>-24397343#0</td>\n",
       "      <td>72672.0</td>\n",
       "      <td>-303025950</td>\n",
       "      <td>highway.secondary</td>\n",
       "      <td>133.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>423173 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        person   trip_id  vehicle_id       link_id   start_link     end_link  \\\n",
       "0       100020  100020_1  100020_car   390466917#1   -129258289  481651812#0   \n",
       "1       100020  100020_1  100020_car  -389439108#0   -129258289  481651812#0   \n",
       "2       100020  100020_1  100020_car  -225101773#1   -129258289  481651812#0   \n",
       "3       100020  100020_1  100020_car     762963414   -129258289  481651812#0   \n",
       "4       100020  100020_1  100020_car   225101767#0   -129258289  481651812#0   \n",
       "...        ...       ...         ...           ...          ...          ...   \n",
       "821787  999971  999971_3  999971_car    15488937#3  -24397343#0   -303025950   \n",
       "821788  999971  999971_3  999971_car      12830362  -24397343#0   -303025950   \n",
       "821789  999971  999971_3  999971_car     509752406  -24397343#0   -303025950   \n",
       "821790  999971  999971_3  999971_car   571374633#0  -24397343#0   -303025950   \n",
       "821791  999971  999971_3  999971_car   571374633#1  -24397343#0   -303025950   \n",
       "\n",
       "        dep_time     dep_link  arr_time     arr_link                 type  \\\n",
       "0        46485.0   -129258289   53450.0  481651812#0  highway.residential   \n",
       "1        46485.0   -129258289   53450.0  481651812#0  highway.residential   \n",
       "2        46485.0   -129258289   53450.0  481651812#0  highway.residential   \n",
       "3        46485.0   -129258289   53450.0  481651812#0  highway.residential   \n",
       "4        46485.0   -129258289   53450.0  481651812#0  highway.residential   \n",
       "...          ...          ...       ...          ...                  ...   \n",
       "821787   72011.0  -24397343#0   72672.0   -303025950    highway.secondary   \n",
       "821788   72011.0  -24397343#0   72672.0   -303025950    highway.secondary   \n",
       "821789   72011.0  -24397343#0   72672.0   -303025950    highway.secondary   \n",
       "821790   72011.0  -24397343#0   72672.0   -303025950    highway.secondary   \n",
       "821791   72011.0  -24397343#0   72672.0   -303025950    highway.secondary   \n",
       "\n",
       "        length  \n",
       "0       113.91  \n",
       "1       248.63  \n",
       "2        48.94  \n",
       "3       128.95  \n",
       "4        91.53  \n",
       "...        ...  \n",
       "821787   18.15  \n",
       "821788   41.00  \n",
       "821789    6.21  \n",
       "821790   87.32  \n",
       "821791  133.74  \n",
       "\n",
       "[423173 rows x 12 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Person_Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enter_time</th>\n",
       "      <th>link_id</th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>type_of_event</th>\n",
       "      <th>ID</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1007.0</td>\n",
       "      <td>328016863</td>\n",
       "      <td>258_car</td>\n",
       "      <td>entered link</td>\n",
       "      <td>328016863</td>\n",
       "      <td>highway.motorway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1011.0</td>\n",
       "      <td>328016865</td>\n",
       "      <td>258_car</td>\n",
       "      <td>entered link</td>\n",
       "      <td>328016865</td>\n",
       "      <td>highway.motorway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1022.0</td>\n",
       "      <td>990069024</td>\n",
       "      <td>258_car</td>\n",
       "      <td>entered link</td>\n",
       "      <td>990069024</td>\n",
       "      <td>highway.motorway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1041.0</td>\n",
       "      <td>27506355</td>\n",
       "      <td>258_car</td>\n",
       "      <td>entered link</td>\n",
       "      <td>27506355</td>\n",
       "      <td>highway.motorway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1130.0</td>\n",
       "      <td>243078522</td>\n",
       "      <td>258_car</td>\n",
       "      <td>entered link</td>\n",
       "      <td>243078522</td>\n",
       "      <td>highway.motorway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854658</th>\n",
       "      <td>114676.0</td>\n",
       "      <td>-256625055</td>\n",
       "      <td>922_car</td>\n",
       "      <td>entered link</td>\n",
       "      <td>-256625055</td>\n",
       "      <td>highway.secondary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854659</th>\n",
       "      <td>114680.0</td>\n",
       "      <td>-687216323</td>\n",
       "      <td>922_car</td>\n",
       "      <td>entered link</td>\n",
       "      <td>-687216323</td>\n",
       "      <td>highway.secondary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854660</th>\n",
       "      <td>114683.0</td>\n",
       "      <td>-256625050#1</td>\n",
       "      <td>922_car</td>\n",
       "      <td>entered link</td>\n",
       "      <td>-256625050#1</td>\n",
       "      <td>highway.secondary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854661</th>\n",
       "      <td>114692.0</td>\n",
       "      <td>-256625050#0</td>\n",
       "      <td>922_car</td>\n",
       "      <td>entered link</td>\n",
       "      <td>-256625050#0</td>\n",
       "      <td>highway.secondary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854662</th>\n",
       "      <td>114715.0</td>\n",
       "      <td>-1108379497</td>\n",
       "      <td>922_car</td>\n",
       "      <td>entered link</td>\n",
       "      <td>-1108379497</td>\n",
       "      <td>highway.secondary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>854663 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        enter_time       link_id vehicle_id type_of_event            ID  \\\n",
       "0           1007.0     328016863    258_car  entered link     328016863   \n",
       "1           1011.0     328016865    258_car  entered link     328016865   \n",
       "2           1022.0     990069024    258_car  entered link     990069024   \n",
       "3           1041.0      27506355    258_car  entered link      27506355   \n",
       "4           1130.0     243078522    258_car  entered link     243078522   \n",
       "...            ...           ...        ...           ...           ...   \n",
       "854658    114676.0    -256625055    922_car  entered link    -256625055   \n",
       "854659    114680.0    -687216323    922_car  entered link    -687216323   \n",
       "854660    114683.0  -256625050#1    922_car  entered link  -256625050#1   \n",
       "854661    114692.0  -256625050#0    922_car  entered link  -256625050#0   \n",
       "854662    114715.0   -1108379497    922_car  entered link   -1108379497   \n",
       "\n",
       "                     type  \n",
       "0        highway.motorway  \n",
       "1        highway.motorway  \n",
       "2        highway.motorway  \n",
       "3        highway.motorway  \n",
       "4        highway.motorway  \n",
       "...                   ...  \n",
       "854658  highway.secondary  \n",
       "854659  highway.secondary  \n",
       "854660  highway.secondary  \n",
       "854661  highway.secondary  \n",
       "854662  highway.secondary  \n",
       "\n",
       "[854663 rows x 6 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_enter_no_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "result= duckdb.query(\"\"\"SELECT \n",
    "    m.person,\n",
    "    m.vehicle_id,\n",
    "    m.trip_id,\n",
    "    m.link_id,\n",
    "    m.dep_link,\n",
    "    m.dep_time,\n",
    "    m.arr_link,\n",
    "    m.arr_time,\n",
    "    m.type,\n",
    "    m.length,\n",
    "    s1.enter_time AS time_link_entered,\n",
    "    t2.leave_time AS time_link_left,\n",
    "FROM PersonLink m\n",
    "LEFT JOIN LinkEnter s1 \n",
    "    ON m.link_id = s1.link_id\n",
    "    AND m.vehicle_id = s1.vehicle_id\n",
    "    AND s1.enter_time BETWEEN m.dep_time AND m.arr_time                            \n",
    "LEFT JOIN LinkLeave t2 \n",
    "    ON m.link_id = t2.link_id\n",
    "    AND m.vehicle_id = t2.vehicle_id\n",
    "    AND t2.leave_time BETWEEN m.dep_time AND m.arr_time;\n",
    "                      \"\"\").to_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['time_on_link'] = result['time_link_left'] - result['time_link_entered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['m_per_s'] = result['length'] / result['time_on_link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['hour_link_entered'] = np.floor(result['time_link_entered'] / 3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.sort_values(by=['type', 'hour_link_entered'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_storage = []\n",
    "type_storage = []\n",
    "speed_storage = []\n",
    "\n",
    "for roadType in result['type'].unique():\n",
    "    for hour in result[(result['type']== roadType)]['hour_link_entered'].unique():\n",
    "        hour_storage.append(hour)\n",
    "        type_storage.append(roadType)\n",
    "        speed_storage.append(np.mean(result[(result['type']== roadType) & (result['hour_link_entered']== hour)]['m_per_s'])*3.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_aggr = pd.DataFrame({'type': type_storage, 'hour': hour_storage, 'speed': speed_storage})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>hour</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>3.0</td>\n",
       "      <td>39.795303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.993264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>5.0</td>\n",
       "      <td>37.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>6.0</td>\n",
       "      <td>37.843843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>7.0</td>\n",
       "      <td>37.403912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>8.0</td>\n",
       "      <td>37.903739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>9.0</td>\n",
       "      <td>37.042573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>10.0</td>\n",
       "      <td>37.140061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>11.0</td>\n",
       "      <td>37.174748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>12.0</td>\n",
       "      <td>37.198799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>13.0</td>\n",
       "      <td>37.038509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>14.0</td>\n",
       "      <td>36.569085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>15.0</td>\n",
       "      <td>37.298173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>16.0</td>\n",
       "      <td>36.939597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>17.0</td>\n",
       "      <td>37.356371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.785301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>19.0</td>\n",
       "      <td>37.196257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>20.0</td>\n",
       "      <td>37.772558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>21.0</td>\n",
       "      <td>37.379420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>22.0</td>\n",
       "      <td>36.423795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>23.0</td>\n",
       "      <td>37.159726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>24.0</td>\n",
       "      <td>36.628999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>25.0</td>\n",
       "      <td>38.716368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>26.0</td>\n",
       "      <td>34.177927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>27.0</td>\n",
       "      <td>36.877576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>highway.tertiary</td>\n",
       "      <td>28.0</td>\n",
       "      <td>39.100443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                type  hour      speed\n",
       "55  highway.tertiary   3.0  39.795303\n",
       "56  highway.tertiary   4.0  38.993264\n",
       "57  highway.tertiary   5.0  37.545455\n",
       "58  highway.tertiary   6.0  37.843843\n",
       "59  highway.tertiary   7.0  37.403912\n",
       "60  highway.tertiary   8.0  37.903739\n",
       "61  highway.tertiary   9.0  37.042573\n",
       "62  highway.tertiary  10.0  37.140061\n",
       "63  highway.tertiary  11.0  37.174748\n",
       "64  highway.tertiary  12.0  37.198799\n",
       "65  highway.tertiary  13.0  37.038509\n",
       "66  highway.tertiary  14.0  36.569085\n",
       "67  highway.tertiary  15.0  37.298173\n",
       "68  highway.tertiary  16.0  36.939597\n",
       "69  highway.tertiary  17.0  37.356371\n",
       "70  highway.tertiary  18.0  36.785301\n",
       "71  highway.tertiary  19.0  37.196257\n",
       "72  highway.tertiary  20.0  37.772558\n",
       "73  highway.tertiary  21.0  37.379420\n",
       "74  highway.tertiary  22.0  36.423795\n",
       "75  highway.tertiary  23.0  37.159726\n",
       "76  highway.tertiary  24.0  36.628999\n",
       "77  highway.tertiary  25.0  38.716368\n",
       "78  highway.tertiary  26.0  34.177927\n",
       "79  highway.tertiary  27.0  36.877576\n",
       "80  highway.tertiary  28.0  39.100443"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_aggr[res_aggr['type'] == \"highway.tertiary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"DROP TABLE Plans_reduced;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"DROP TABLE LinkLeaveEvents_reduced;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"DROP TABLE LinkEnterEvents_reduced;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "423198"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person</th>\n",
       "      <th>trip_number</th>\n",
       "      <th>trip_id</th>\n",
       "      <th>dep_time</th>\n",
       "      <th>trav_time</th>\n",
       "      <th>wait_time</th>\n",
       "      <th>traveled_distance</th>\n",
       "      <th>euclidean_distance</th>\n",
       "      <th>main_mode</th>\n",
       "      <th>longest_distance_mode</th>\n",
       "      <th>...</th>\n",
       "      <th>start_x</th>\n",
       "      <th>start_y</th>\n",
       "      <th>end_facility_id</th>\n",
       "      <th>end_link</th>\n",
       "      <th>end_x</th>\n",
       "      <th>end_y</th>\n",
       "      <th>first_pt_boarding_stop</th>\n",
       "      <th>last_pt_egress_stop</th>\n",
       "      <th>dep_time_seconds</th>\n",
       "      <th>trav_time_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>876705</td>\n",
       "      <td>1</td>\n",
       "      <td>876705_1</td>\n",
       "      <td>05:36:40</td>\n",
       "      <td>00:20:35</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>23695</td>\n",
       "      <td>16035</td>\n",
       "      <td>car</td>\n",
       "      <td>car</td>\n",
       "      <td>...</td>\n",
       "      <td>911727.55</td>\n",
       "      <td>5691523.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-63744442#0</td>\n",
       "      <td>895787.53</td>\n",
       "      <td>5689779.38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20200.0</td>\n",
       "      <td>1235.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       person  trip_number   trip_id  dep_time trav_time wait_time  \\\n",
       "14637  876705            1  876705_1  05:36:40  00:20:35  00:00:00   \n",
       "\n",
       "       traveled_distance  euclidean_distance main_mode longest_distance_mode  \\\n",
       "14637              23695               16035       car                   car   \n",
       "\n",
       "       ...    start_x     start_y end_facility_id     end_link      end_x  \\\n",
       "14637  ...  911727.55  5691523.21             NaN  -63744442#0  895787.53   \n",
       "\n",
       "            end_y  first_pt_boarding_stop  last_pt_egress_stop  \\\n",
       "14637  5689779.38                     NaN                  NaN   \n",
       "\n",
       "      dep_time_seconds  trav_time_seconds  \n",
       "14637          20200.0             1235.0  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outTrips[(outTrips['person'] == \"876705\") & (outTrips['end_link']==\"-63744442#0\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>leave_time</th>\n",
       "      <th>link_id</th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>type_of_event</th>\n",
       "      <th>ID</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156344</th>\n",
       "      <td>32823.0</td>\n",
       "      <td>-63744442#0</td>\n",
       "      <td>876705_car</td>\n",
       "      <td>left link</td>\n",
       "      <td>-63744442#0</td>\n",
       "      <td>highway.residential</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        leave_time      link_id  vehicle_id type_of_event           ID  \\\n",
       "156344     32823.0  -63744442#0  876705_car     left link  -63744442#0   \n",
       "\n",
       "                       type  \n",
       "156344  highway.residential  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_leave_no_pt[(df_leave_no_pt['vehicle_id'] == \"876705_car\") & (df_leave_no_pt['link_id']== \"-63744442#0\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enter_time</th>\n",
       "      <th>link_id</th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>type_of_event</th>\n",
       "      <th>ID</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18578</th>\n",
       "      <td>21192.0</td>\n",
       "      <td>121970294#0</td>\n",
       "      <td>876705_car</td>\n",
       "      <td>entered link</td>\n",
       "      <td>121970294#0</td>\n",
       "      <td>highway.residential</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       enter_time      link_id  vehicle_id type_of_event           ID  \\\n",
       "18578     21192.0  121970294#0  876705_car  entered link  121970294#0   \n",
       "\n",
       "                      type  \n",
       "18578  highway.residential  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_enter_no_pt[(df_enter_no_pt['vehicle_id'] == \"876705_car\") & (df_enter_no_pt['link_id']== \"121970294#0\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_id</th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>dep_time_seconds</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>type</th>\n",
       "      <th>length</th>\n",
       "      <th>enter_time</th>\n",
       "      <th>leave_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>298624</th>\n",
       "      <td>121970294#0</td>\n",
       "      <td>876705_car</td>\n",
       "      <td>20200.0</td>\n",
       "      <td>21164.0</td>\n",
       "      <td>highway.residential</td>\n",
       "      <td>430.41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            link_id  vehicle_id  dep_time_seconds  arrival_time  \\\n",
       "298624  121970294#0  876705_car           20200.0       21164.0   \n",
       "\n",
       "                       type  length  enter_time  leave_time  \n",
       "298624  highway.residential  430.41         NaN         NaN  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[(result['vehicle_id']== \"876705_car\")& (result['link_id']== \"121970294#0\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Executed Scores at iteration 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### scores ####\n",
    "# 1pct, alpha = 1\n",
    "scores_1pct =pd.DataFrame()\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" + str(elem) + \"-fCf_sCF_0.01_gS_default_3765/lausitz-1pct-\" + str(elem) + \"-fCf_sCf_0.01_gS_default_3765.scorestats.csv\"\n",
    "    temp = pd.read_csv(path, sep=\";\")\n",
    "    df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"1-pct\", \"sample_nr\": elem, \"alpha\": 1.0, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "    scores_1pct = pd.concat([scores_1pct, df], axis = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 pct, alpha = 0.75\n",
    "scores_1pct_sCf = pd.DataFrame()\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" + str(elem) + \"-fCf_0.01_sCF_0.03162_gS_default_3765/lausitz-1pct-\" + str(elem) + \"-fCf_0.01_sCf_0.03162_gS_default_3765.scorestats.csv\"\n",
    "    temp = pd.read_csv(path, sep=\";\")\n",
    "    df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"1-pct\", \"sample_nr\": elem, \"alpha\": 0.75, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "    scores_1pct_sCf = pd.concat([scores_1pct_sCf, df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 pct, alpha 1, random global seed\n",
    "#  1 pct random seed\n",
    "scores_1pct_rGs = pd.DataFrame()\n",
    "rGs = [ 4711,3254, 2306, 6384,4338, 6003, 5502, 9377, 5621, 9002 ]\n",
    "for seed in rGs:\n",
    "    if (seed == 4711):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        # insert number of stuck time violations from the first 1 pct sample\n",
    "        df = pd.DataFrame({\"avg_executed_it_500\":scores_1pct[\"avg_executed_it_500\"].iloc[0], \"sample_size\": \"1-pct\", \"sample_nr\": 1, \"alpha\": 1.0, \"stuck_time\": 30.0, \"global_seed\": global_seed }, index = [rGs.index(seed)])\n",
    "        scores_1pct_rGs = pd.concat([scores_1pct_rGs, df])\n",
    "    elif (seed == 3254):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        path =\"/home/lola/math_cluster/output/output-lausitz-1pct-1-fCf_sCF_0.01_gS_3254_3765/lausitz-1pct-1-fCf_sCf_0.01_gS_3254_3765.scorestats.csv\"\n",
    "        temp = pd.read_csv(path, sep = \";\")\n",
    "        df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"1-pct\", \"sample_nr\": 1, \"alpha\":1.0, \"stuck_time\": 30.0, \"global_seed\": global_seed }, index = [rGs.index(seed)])\n",
    "        scores_1pct_rGs = pd.concat([scores_1pct_rGs, df], axis = 0)\n",
    "    else:\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-1.0-pct-1-fCf_sCF_0.01_gS_\" + str(seed) + \"_3765/lausitz-1.0-pct-1-fCf_sCF_0.01_gS_\" + str(seed) + \"_3765.scorestats.csv\"\n",
    "        temp = pd.read_csv(path, sep = \";\")\n",
    "        df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"1-pct\", \"sample_nr\": 1, \"alpha\":1.0, \"stuck_time\": 30.0, \"global_seed\": global_seed }, index = [rGs.index(seed)])\n",
    "        scores_1pct_rGs = pd.concat([scores_1pct_rGs, df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 pct, alpha = 1, sT scaled\n",
    "scores_1pct_sT =pd.DataFrame()\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" + str(elem) + \"-fCf_sCF_0.01_gS_4711_sT_3000.0_3765/lausitz-1-pct-\" + str(elem) + \"-fCf_sCF_0.01_gS_4711_sT_3000.0_3765.scorestats.csv\"\n",
    "    temp = pd.read_csv(path, sep=\";\")\n",
    "    df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"1-pct\", \"sample_nr\": elem, \"alpha\": 1.0, \"stuck_time\": 3000.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "    scores_1pct_sT = pd.concat([scores_1pct_sT, df], axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 pct, alpha = 0.75, sT scaled\n",
    "scores_1pct_sT_sCf =pd.DataFrame()\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\"+ str(elem) + \"-fCf_0.01_sCF_0.03162_gS_4711_sT_3000.0_3765/lausitz-1-pct-\" + str(elem) + \"-fCf_0.01_sCF_0.03162_gS_4711_sT_3000.0_3765.scorestats.csv\"\n",
    "    temp = pd.read_csv(path, sep=\";\")\n",
    "    df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"1-pct\", \"sample_nr\": elem, \"alpha\": 0.75, \"stuck_time\": 3000.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "    scores_1pct_sT_sCf = pd.concat([scores_1pct_sT_sCf, df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_1pct_all = pd.concat([scores_1pct, scores_1pct_sCf, scores_1pct_rGs, scores_1pct_sT, scores_1pct_sT_sCf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_1pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/scores_all_1pct_samples.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 pct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### scores, 5 pct ####\n",
    "# 5pct, alpha = 1\n",
    "scores_5pct = pd.DataFrame()\n",
    "\n",
    "for elem in range(1,11,1):\n",
    "    if (elem==6):\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765/lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765-2.scorestats.csv\"\n",
    "        temp = pd.read_csv(path, sep=\";\")\n",
    "        df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"5-pct\", \"sample_nr\": elem, \"alpha\": 1.0, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "        scores_5pct = pd.concat([scores_5pct, df], axis = 0)\n",
    "    else:\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(elem) + \"-fCf_sCF_0.05_gS_4711_3765/lausitz-5.0-pct-\" + str(elem) + \"-fCf_sCF_0.05_gS_4711_3765.scorestats.csv\"\n",
    "        temp = pd.read_csv(path, sep=\";\")\n",
    "        df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"5-pct\", \"sample_nr\": elem, \"alpha\": 1.0, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "        scores_5pct = pd.concat([scores_5pct, df], axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 pct, alpha 0.75 \n",
    "scores_5pct_sCf = pd.DataFrame()\n",
    "\n",
    "for elem in range(1,11,1):\n",
    "    if (elem==6):\n",
    "        path =\"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_0.05_sCF_0.10574_gS_4711_3765/lausitz-5.0-pct-6-fCf_0.05_sCF_0.10574_gS_4711_3765-2.scorestats.csv\"\n",
    "        temp = pd.read_csv(path, sep=\";\")\n",
    "        df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"5-pct\", \"sample_nr\": elem, \"alpha\": 0.75, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "        scores_5pct_sCf = pd.concat([scores_5pct_sCf, df], axis = 0)\n",
    "\n",
    "    else:\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(elem) + \"-fCf_0.05_sCF_0.10574_gS_4711_3765/lausitz-5.0-pct-\" + str(elem) + \"-fCf_0.05_sCF_0.10574_gS_4711_3765.scorestats.csv\"\n",
    "        temp = pd.read_csv(path, sep=\";\")\n",
    "        df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"5-pct\", \"sample_nr\": elem, \"alpha\":0.75, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "        scores_5pct_sCf = pd.concat([scores_5pct_sCf, df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_5pct_rGs = pd.DataFrame()\n",
    "rGs = [4711, 3254, 2306, 6384,4338, 6003, 5502, 9377, 5621, 9002 ]\n",
    "\n",
    "for seed in rGs:\n",
    "    if (seed ==4711):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        temp = {'avg_executed_it_500': scores_5pct[\"avg_executed_it_500\"].iloc[0], 'sample_size': '5-pct', \"sample_nr\": 1, 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': global_seed  }\n",
    "        temp = pd.DataFrame(data=temp, index=[rGs.index(4711)])\n",
    "        scores_5pct_rGs = pd.concat([scores_5pct_rGs, temp])\n",
    "    else:\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-1-fCf_sCF_0.05_gS_\" + str(seed) + \"_3765/lausitz-5.0-pct-1-fCf_sCF_0.05_gS_\" + str(seed) + \"_3765.scorestats.csv\"\n",
    "        temp = pd.read_csv(path, sep = \";\")\n",
    "        temp = {'avg_executed_it_500': temp[\"avg_executed\"].iloc[500], 'sample_size': '5-pct', \"sample_nr\": 1, 'alpha': 1.0, 'stuck_time': 30.0, 'global_seed': global_seed  }\n",
    "        df = pd.DataFrame(data=temp, index=[rGs.index(seed)])\n",
    "        scores_5pct_rGs = pd.concat([scores_5pct_rGs, df], axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 pct, alpha 1.0, sT scaled\n",
    "scores_5pct_sT =pd.DataFrame()\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str(elem) + \"-fCf_sCF_0.05_gS_4711_sT_600.0_3765/lausitz-5-pct-\" + str(elem) + \"-fCf_sCF_0.05_gS_4711_sT_600.0_3765.scorestats.csv\"\n",
    "    temp = pd.read_csv(path, sep=\";\")\n",
    "    df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"5-pct\", \"sample_nr\": elem, \"alpha\": 1.0, \"stuck_time\": 600.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "    scores_5pct_sT = pd.concat([scores_5pct_sT, df], axis = 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 pct, alpha 0.75, sT scaled\n",
    "scores_5pct_sT_sCf =pd.DataFrame()\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str(elem) + \"-fCf_0.05_sCF_0.10574_gS_4711_sT_600.0_3765/lausitz-5-pct-\" + str(elem) + \"-fCf_0.05_sCF_0.10574_gS_4711_sT_600.0_3765.scorestats.csv\"\n",
    "    temp = pd.read_csv(path, sep=\";\")\n",
    "    df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"5-pct\", \"sample_nr\": elem, \"alpha\": 0.75, \"stuck_time\": 600.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "    scores_5pct_sT_sCf = pd.concat([scores_5pct_sT_sCf, df], axis = 0)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_5pct_all = pd.concat([scores_5pct, scores_5pct_sCf, scores_5pct_rGs, scores_5pct_sT, scores_5pct_sT_sCf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_5pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/scores_all_5pct_samples.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### scores, 10 pct ####\n",
    "# 10 pct, alpha = 1.0\n",
    "scores_10pct = pd.DataFrame()\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" + str(elem) + \"-fCf_sCF_0.1_gS_4711_3765/lausitz-10.0-pct-\" + str(elem) + \"-fCf_sCF_0.1_gS_4711_3765.scorestats.csv\"\n",
    "    temp = pd.read_csv(path, sep=\";\")\n",
    "    df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"10-pct\", \"sample_nr\": elem, \"alpha\": 1.0, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "    scores_10pct = pd.concat([scores_10pct, df], axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 pct, alpha = 0.75\n",
    "scores_10pct_sCf = pd.DataFrame()\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" + str(elem) + \"-fCf_0.1_sCF_0.17783_gS_4711_3765/lausitz-10.0-pct-\" + str(elem) + \"-fCf_0.1_sCF_0.17783_gS_4711_3765.scorestats.csv\"\n",
    "    temp = pd.read_csv(path, sep=\";\")\n",
    "    df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"10-pct\", \"sample_nr\": elem, \"alpha\": 0.75, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "    scores_10pct = pd.concat([scores_10pct, df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 pct, alpha = 1, sT scaled\n",
    "scores_10pct_sT = pd.DataFrame()\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\" + str(elem) + \"-fCf_sCF_0.1_gS_4711_sT_300.0_3765/lausitz-10-pct-\" + str(elem) + \"-fCf_sCF_0.1_gS_4711_sT_300.0_3765.scorestats.csv\"\n",
    "    temp = pd.read_csv(path, sep=\";\")\n",
    "    df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"10-pct\", \"sample_nr\": elem, \"alpha\": 1.0, \"stuck_time\": 300.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "    scores_10pct_sT = pd.concat([scores_10pct_sT, df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 pct, alpha = 0.75, sT scaled\n",
    "# 10 pct, alpha = 1, sT scaled\n",
    "scores_10pct_sT_sCf = pd.DataFrame()\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\" + str(elem) + \"-fCf_0.1_sCF_0.17783_gS_4711_sT_300.0_3765/lausitz-10-pct-\" + str(elem) + \"-fCf_0.1_sCF_0.17783_gS_4711_sT_300.0_3765.scorestats.csv\"\n",
    "    temp = pd.read_csv(path, sep=\";\")\n",
    "    df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"10-pct\", \"sample_nr\": elem, \"alpha\": 0.75, \"stuck_time\": 300.0, \"global_seed\": \"default\" }, index = [elem])\n",
    "    scores_10pct_sT_sCf = pd.concat([scores_10pct_sT_sCf, df], axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_10pct_all = pd.concat([scores_10pct, scores_10pct_sCf, scores_10pct_sT, scores_10pct_sT_sCf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_10pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/scores_all_10pct_samples.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### scores, 25 pct ####\n",
    "scores_25pct = pd.DataFrame()\n",
    " # 25 pct, alpha = 1\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_sCF_0.25_gS_4711_3765/lausitz-25.0-pct-fCf_sCF_0.25_gS_4711_3765.scorestats.csv\"\n",
    "temp = pd.read_csv(path, sep = \";\")\n",
    "df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"25-pct\", \"sample_nr\": 1, \"alpha\": 1.0, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [0])\n",
    "scores_25pct = pd.concat([scores_25pct, df], axis = 0)\n",
    "\n",
    "# 25 pct, alpha = 0.75\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_0.25_sCF_0.35355_gS_4711_3765/lausitz-25.0-pct-fCf_0.25_sCF_0.35355_gS_4711_3765.scorestats.csv\"\n",
    "temp = pd.read_csv(path, sep = \";\")\n",
    "df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"25-pct\", \"sample_nr\": 1, \"alpha\": 0.75, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [1])\n",
    "scores_25pct = pd.concat([scores_25pct, df], axis = 0)\n",
    "\n",
    " # 25 pct, alpha = 1, sT scaled\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-25-pct-1-fCf_sCF_0.25_gS_4711_sT_120.0_3765/lausitz-25-pct-1-fCf_sCF_0.25_gS_4711_sT_120.0_3765.scorestats.csv\"\n",
    "temp = pd.read_csv(path, sep = \";\")\n",
    "df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"25-pct\", \"sample_nr\": 1, \"alpha\": 1.0, \"stuck_time\": 120.0, \"global_seed\": \"default\" }, index = [2])\n",
    "scores_25pct = pd.concat([scores_25pct, df], axis = 0)\n",
    "\n",
    " # 25 pct, alpha = 0.75, sT scaled\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-25-pct-1-fCf_0.25_sCF_0.35355_gS_4711_sT_120.0_3765/lausitz-25-pct-1-fCf_0.25_sCF_0.35355_gS_4711_sT_120.0_3765.scorestats.csv\"\n",
    "temp = pd.read_csv(path, sep = \";\")\n",
    "df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"25-pct\", \"sample_nr\": 1, \"alpha\": 0.75, \"stuck_time\": 120.0, \"global_seed\": \"default\" }, index = [0])\n",
    "scores_25pct = pd.concat([scores_25pct, df], axis = 0)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_25pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/scores_all_25pct_samples.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### scores, 50 pct #### \n",
    "scores_50pct = pd.DataFrame()\n",
    "# 50 pct, alpha = 1\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_sCF_0.5_gS_4711_3765/lausitz-50.0-pct-fCf_sCF_0.5_gS_4711_3765.scorestats.csv\"\n",
    "temp = pd.read_csv(path, sep = \";\")\n",
    "df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"50-pct\", \"sample_nr\": 1, \"alpha\": 1.0, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [0])\n",
    "scores_50pct = pd.concat([scores_50pct, df], axis = 0)\n",
    "\n",
    "# 50 pct, alpha = 0.75\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_0.5_sCF_0.5946_gS_4711_3765/lausitz-50.0-pct-fCf_0.5_sCF_0.5946_gS_4711_3765.scorestats.csv\"\n",
    "temp = pd.read_csv(path, sep = \";\")\n",
    "df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"50-pct\", \"sample_nr\": 1, \"alpha\": 0.75, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [1])\n",
    "scores_50pct = pd.concat([scores_50pct, df], axis = 0)\n",
    "\n",
    "# 50 pct/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_sCF_0.5_gS_4711_sT_60.0_3765/lausitz-50-pct-1-fCf_sCF_0.5_gS_4711_sT_60.0_3765.scorestats.csv\"\n",
    "temp = pd.read_csv(path, sep = \";\")\n",
    "df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"50-pct\", \"sample_nr\": 1, \"alpha\": 1.0, \"stuck_time\": 60.0, \"global_seed\": \"default\" }, index = [2])\n",
    "scores_50pct = pd.concat([scores_50pct, df], axis = 0)\n",
    "\n",
    "# 50 pct, alpha = 0.75, sT scaled\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_0.5_sCF_0.5946_gS_4711_sT_60.0_3765/lausitz-50-pct-1-fCf_0.5_sCF_0.5946_gS_4711_sT_60.0_3765.scorestats.csv\"\n",
    "temp = pd.read_csv(path, sep = \";\")\n",
    "df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"50-pct\", \"sample_nr\": 1, \"alpha\": 0.75, \"stuck_time\": 60.0, \"global_seed\": \"default\" }, index = [0])\n",
    "scores_50pct = pd.concat([scores_50pct, df], axis = 0)\n",
    "\n",
    "\n",
    "# 25 pct doubled, alpha = 1.0, sT 30.0\n",
    "path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25-pct-doubled-fCf_0.5_sCF_0.5_gS_4711_3765/lausitz-25-pct-doubled-fCf_0.5_sCF_0.5_gS_4711__3765.scorestats.csv\"\n",
    "temp = pd.read_csv(path, sep = \";\")\n",
    "df = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"25-pct-doubled\", \"sample_nr\": 1, \"alpha\": 1.0 , \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [0])\n",
    "scores_50pct = pd.concat([scores_50pct, df], axis = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_50pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/scores_all_50pct_samples.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### scores , 100 pct ####\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765/lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765.scorestats.csv\"\n",
    "temp = pd.read_csv(path, sep = \";\")\n",
    "scores_100pct = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"100-pct\", \"sample_nr\": 1, \"alpha\": 1.0, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [0])\n",
    "\n",
    "path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25.0-pct-quadrupled-fCf_1.0_sCF_1.0_gS_4711_3765/lausitz-25-pct-quadrupled-fCf_1.0_sCF_1.0_gS_4711__3765.scorestats.csv\"\n",
    "temp = pd.read_csv(path, sep = \";\")\n",
    "scores_25_pct_quadrupled = pd.DataFrame({\"avg_executed_it_500\":temp[\"avg_executed\"].iloc[500], \"sample_size\": \"25-pct-quadrupled\", \"sample_nr\": 1, \"alpha\": 1.0, \"stuck_time\": 30.0, \"global_seed\": \"default\" }, index = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_1_100 = pd.concat([scores_1pct_all, scores_5pct_all, scores_10pct_all, scores_25pct, scores_50pct, scores_100pct, scores_25_pct_quadrupled], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_1_100.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/scores_all_1_100pct_samples.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5./6. Average travel time and traveled distance & N departures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAvgTravelTime(pathToFile, sampleSize, sampleNr, alpha, stuckTime, globalSeed):\n",
    "    temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
    "    temp[\"trav_time\"] = pd.to_timedelta(temp.trav_time)\n",
    "    avg_trav_time = temp[\"trav_time\"].sum() / temp[\"trav_time\"].shape[0]\n",
    "    df_avg_trav_time = pd.DataFrame({\"avg_trav_time\": avg_trav_time, \"sample_size\": sampleSize, \"sample_nr\": sampleNr, \"alpha\" : alpha, \"stuck_time\": stuckTime, \"global_seed\": globalSeed}, index=[sampleNr])\n",
    "    return df_avg_trav_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAvgTravelDistance(pathToFile, sampleSize, sampleNr, alpha, stuckTime, globalSeed):\n",
    "    temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
    "    avg_trav_dist = np.mean(temp[\"traveled_distance\"])\n",
    "    df_avg_trav_dist = pd.DataFrame({\"avg_trav_dist\": avg_trav_dist, \"sample_size\": sampleSize, \"sample_nr\": sampleNr, \"alpha\" : alpha, \"stuck_time\": stuckTime, \"global_seed\": globalSeed}, index=[sampleNr])\n",
    "    return df_avg_trav_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcDepPerHour(pathToFile, sampleSize, sampleNr, alpha, stuckTime, globalSeed):\n",
    "    temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
    "    departure_hours = []\n",
    "    for element in temp['dep_time']:\n",
    "        departure_hours.append(element[:2])\n",
    "        \n",
    "    df_all_dep = pd.DataFrame({'dep_hour': departure_hours})\n",
    "\n",
    "    df_n_of_dep_per_hour = pd.DataFrame()\n",
    "    for element in df_all_dep['dep_hour'].unique():\n",
    "        temp_dep_hour = pd.DataFrame({'hour': element, 'n_departues': df_all_dep[(df_all_dep['dep_hour']== element)].shape[0] }, index = [0])\n",
    "        df_n_of_dep_per_hour = pd.concat([df_n_of_dep_per_hour, temp_dep_hour], axis = 0, ignore_index= True)\n",
    "\n",
    "    df_n_of_dep_per_hour = df_n_of_dep_per_hour.sort_values(by=['hour'])\n",
    "    df_n_of_dep_per_hour.insert(2,\"sample_size\", sampleSize)\n",
    "    df_n_of_dep_per_hour.insert(3,\"sample_nr\", sampleNr)\n",
    "    df_n_of_dep_per_hour.insert(4,'alpha', alpha)\n",
    "    df_n_of_dep_per_hour.insert(5,'stuck_time', stuckTime)\n",
    "    df_n_of_dep_per_hour.insert(6,'global_seed', globalSeed)\n",
    "    return df_n_of_dep_per_hour\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.01\"]\n",
    "storCapF =  [\"0.01\", \"0.03162\"]\n",
    "\n",
    "# initialize empty data frames\n",
    "avg_travel_times_1pct = pd.DataFrame()\n",
    "avg_travel_distances_1pct = pd.DataFrame()\n",
    "dep_per_hour_1pct = pd.DataFrame()\n",
    "\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "        for sampleNr in range(1,11,1):\n",
    "            # calculate adjusted stuck time\n",
    "            default_stuck_time = 30.0\n",
    "            adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "            # declare sample size as str \"1-pct\"\n",
    "            sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "\n",
    "            if ((fCf == \"0.01\") & (sCf == '0.01')):\n",
    "                # declare alpha\n",
    "                alpha = 1.0\n",
    "                # paths for case 1 and 3 \n",
    "                path_case1 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\"+ str(sampleNr) +\"-fCf_sCF_\" + sCf + \"_gS_default_3765/lausitz-1pct-\"+ str(sampleNr) +\"-fCf_sCf_0.01_gS_default_3765.output_trips.csv.gz\"\n",
    "                path_case3 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" +str(sampleNr) +\"-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-1-pct-\" +str(sampleNr) + \"-fCf_sCF_0.01_gS_4711_sT_3000.0_3765.output_trips.csv.gz\"\n",
    "                \n",
    "                # average travel time\n",
    "                aTt_case1= calcAvgTravelTime(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                aTt_case3= calcAvgTravelTime(path_case3, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                # concatenate with existing values\n",
    "                avg_travel_times_1pct = pd.concat([avg_travel_times_1pct, aTt_case1, aTt_case3], ignore_index= True)\n",
    "\n",
    "                # average traveled distances\n",
    "                aTd_case1 = calcAvgTravelDistance(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                aTd_case3 = calcAvgTravelDistance(path_case3, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                # concatenate with existing values\n",
    "                avg_travel_distances_1pct = pd.concat([avg_travel_distances_1pct, aTd_case1, aTd_case3], ignore_index= True)\n",
    "\n",
    "                # number of departures per hour \n",
    "                dPh_case1 = calcDepPerHour(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                dPh_case3 = calcDepPerHour(path_case3, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                # concatenate results\n",
    "                dep_per_hour_1pct = pd.concat([dep_per_hour_1pct, dPh_case1, dPh_case3], ignore_index= True)\n",
    "\n",
    "\n",
    "            else:\n",
    "                alpha = 0.75\n",
    "                path_case2 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" +str(sampleNr) + \"-fCf_0.01_sCF_\" + sCf + \"_gS_default_3765/lausitz-1pct-\" + str(sampleNr) + \"-fCf_0.01_sCf_0.03162_gS_default_3765.output_trips.csv.gz\"\n",
    "                path_case4 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" + str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-1-pct-\" + str(sampleNr) + \"-fCf_0.01_sCF_0.03162_gS_4711_sT_3000.0_3765.output_trips.csv.gz\"\n",
    "                \n",
    "                # average travel time\n",
    "                aTt_case2= calcAvgTravelTime(path_case2, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                aTt_case4= calcAvgTravelTime(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                # concatenate with existing values\n",
    "                avg_travel_times_1pct = pd.concat([avg_travel_times_1pct, aTt_case2, aTt_case4], ignore_index= True)\n",
    "\n",
    "                # average traveled distances\n",
    "                aTd_case2 = calcAvgTravelDistance(path_case2, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                aTd_case4 = calcAvgTravelDistance(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                # concatenate with existing values\n",
    "                avg_travel_distances_1pct = pd.concat([avg_travel_distances_1pct, aTd_case2, aTd_case4], ignore_index= True)\n",
    "\n",
    "\n",
    "                # number of departures per hour \n",
    "                dPh_case2 = calcDepPerHour(path_case2, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                dPh_case4 = calcDepPerHour(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                # concatenate results\n",
    "                dep_per_hour_1pct = pd.concat([dep_per_hour_1pct, dPh_case2, dPh_case4], ignore_index= True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 pct rGs\n",
    "avg_trav_time_1pct_rGs = pd.DataFrame()\n",
    "avg_trav_dist_1pct_rGs = pd.DataFrame()\n",
    "rGs = [ 4711,3254, 2306, 6384,4338, 6003, 5502, 9377, 5621, 9002 ]\n",
    "for seed in rGs:\n",
    "    if (seed == 4711):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        # Create Data frame and insert first value from avg_trav_time_1pct\n",
    "        trav_time_4711 = avg_travel_times_1pct[(avg_travel_times_1pct['alpha']== 1.0) & (avg_travel_times_1pct['stuck_time']== 30.0)][\"avg_trav_time\"].iloc[0] \n",
    "        df1 = pd.DataFrame({\"avg_trav_time\":trav_time_4711, \"sample_size\": \"1-pct\", \"sample_nr\": 1, \"alpha\": 1.0, \"stuck_time\": 30.0, \"global_seed\": global_seed }, index = [rGs.index(seed)])\n",
    "        avg_trav_time_1pct_rGs = pd.concat([avg_trav_time_1pct_rGs, df1])\n",
    "        \n",
    "        # Create Data frame and insert first value from avg_trav_dist_1pct\n",
    "        trav_dist_4711 = avg_travel_distances_1pct[(avg_travel_distances_1pct['alpha']== 1.0) & (avg_travel_distances_1pct['stuck_time']== 30.0)][\"avg_trav_dist\"].iloc[0] \n",
    "        df1 = pd.DataFrame({\"avg_trav_dist\":trav_dist_4711, \"sample_size\": \"1-pct\", \"sample_nr\": 1, \"alpha\": 1.0, \"stuck_time\": 30.0, \"global_seed\": global_seed }, index = [rGs.index(seed)])\n",
    "        avg_trav_dist_1pct_rGs = pd.concat([avg_trav_dist_1pct_rGs, df1])\n",
    "\n",
    "    elif (seed == 3254):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        path =\"/home/lola/math_cluster/output/output-lausitz-1pct-1-fCf_sCF_0.01_gS_3254_3765/lausitz-1pct-1-fCf_sCf_0.01_gS_3254_3765.output_trips.csv.gz\"\n",
    "\n",
    "        # average travel time\n",
    "        aTt_case5= calcAvgTravelTime(path, sample_size_as_string, 1,  alpha, default_stuck_time, global_seed)\n",
    "        avg_trav_time_1pct_rGs = pd.concat([avg_trav_time_1pct_rGs, aTt_case5], axis = 0, ignore_index = True)\n",
    "\n",
    "        # average traveled distance\n",
    "        aTd_case5= calcAvgTravelDistance(path, sample_size_as_string, 1,  alpha, default_stuck_time, global_seed)\n",
    "        avg_trav_dist_1pct_rGs = pd.concat([avg_trav_dist_1pct_rGs, aTd_case5], axis = 0, ignore_index= True)\n",
    "\n",
    "    else:\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        path =\"/home/lola/math_cluster/output/output-lausitz-1.0-pct-1-fCf_sCF_0.01_gS_\"+ str(seed) + \"_3765/lausitz-1.0-pct-1-fCf_sCF_0.01_gS_\" +str(seed) +\"_3765.output_trips.csv.gz\"\n",
    "        # average travel time\n",
    "        aTt_case5= calcAvgTravelTime(path, sample_size_as_string, 1,  alpha, default_stuck_time, global_seed)\n",
    "        avg_trav_time_1pct_rGs = pd.concat([avg_trav_time_1pct_rGs, aTt_case5], axis = 0, ignore_index = True)\n",
    "\n",
    "        # average traveled distance\n",
    "        aTd_case5= calcAvgTravelDistance(path, sample_size_as_string, 1,  alpha, default_stuck_time, global_seed)\n",
    "        avg_trav_dist_1pct_rGs = pd.concat([avg_trav_dist_1pct_rGs, aTd_case5], axis = 0, ignore_index= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_trav_time_1pct_all = pd.concat([avg_travel_times_1pct, avg_trav_time_1pct_rGs], axis = 0, ignore_index= True)\n",
    "avg_trav_dist_1pct_all = pd.concat([avg_travel_distances_1pct, avg_trav_dist_1pct_rGs ], axis = 0 , ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_trav_time_1pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/avg_trav_time_all_1pct_samples.csv', index = False) \n",
    "avg_trav_dist_1pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/avg_trav_dist_all_1pct_samples.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.05\"]\n",
    "storCapF =  [\"0.05\", \"0.10574\"]\n",
    "\n",
    "# initialize empty data frames\n",
    "avg_travel_times_5pct = pd.DataFrame()\n",
    "avg_travel_distances_5pct = pd.DataFrame()\n",
    "dep_per_hour_5pct = pd.DataFrame()\n",
    "\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,11,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "\n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.05\") & (sCf == \"0.05\")):\n",
    "                    alpha = 1.0\n",
    "                    if (sampleNr == 6):\n",
    "                        path_case1  = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_sCF_\" + sCf + \"_gS_4711_3765/lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765-2.output_trips.csv.gz\"\n",
    "                    \n",
    "                    else: \n",
    "                        path_case1 = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(sampleNr) + \"-fCf_sCF_0.05_gS_4711_3765/lausitz-5.0-pct-\" + str(sampleNr) + \"-fCf_sCF_0.05_gS_4711_3765.output_trips.csv.gz\"\n",
    "                    \n",
    "                    \n",
    "                    path_case3 =  \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" +str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_sT_600.0_3765/lausitz-5-pct-\" + str(sampleNr) + \"-fCf_sCF_0.05_gS_4711_sT_600.0_3765.output_trips.csv.gz\"\n",
    "\n",
    "                    # average travel time\n",
    "                    aTt_case1= calcAvgTravelTime(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    aTt_case3= calcAvgTravelTime(path_case3, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    avg_travel_times_5pct = pd.concat([avg_travel_times_5pct, aTt_case1, aTt_case3], ignore_index= True)\n",
    "\n",
    "                    # average traveled distances\n",
    "                    aTd_case1 = calcAvgTravelDistance(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    aTd_case3 = calcAvgTravelDistance(path_case3, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    avg_travel_distances_5pct = pd.concat([avg_travel_distances_5pct, aTd_case1, aTd_case3], ignore_index= True)\n",
    "\n",
    "                    # number of departures per hour \n",
    "                    dPh_case1 = calcDepPerHour(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    dPh_case3 = calcDepPerHour(path_case3, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate results\n",
    "                    dep_per_hour_5pct = pd.concat([dep_per_hour_5pct, dPh_case1, dPh_case3], ignore_index= True)\n",
    "                        \n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    if(sampleNr == 6):\n",
    "                        path_case2 =  \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_0.05_sCF_0.10574_gS_4711_3765/lausitz-5.0-pct-6-fCf_0.05_sCF_0.10574_gS_4711_3765-2.output_trips.csv.gz\"\n",
    "                    else:\n",
    "                        path_case2 = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" +str(sampleNr) + \"-fCf_0.05_sCF_0.10574_gS_4711_3765/lausitz-5.0-pct-\" + str(sampleNr) + \"-fCf_0.05_sCF_0.10574_gS_4711_3765.output_trips.csv.gz\"\n",
    "                    \n",
    "                    \n",
    "                    path_case4 = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str(sampleNr) + \"-fCf_0.05_sCF_0.10574_gS_4711_sT_600.0_3765/lausitz-5-pct-\" + str(sampleNr) + \"-fCf_0.05_sCF_0.10574_gS_4711_sT_600.0_3765.output_trips.csv.gz\"\n",
    "\n",
    "                    # average travel time\n",
    "                    aTt_case2= calcAvgTravelTime(path_case2, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    aTt_case4= calcAvgTravelTime(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    avg_travel_times_5pct = pd.concat([avg_travel_times_5pct, aTt_case2, aTt_case4], ignore_index= True)\n",
    "\n",
    "                    # average traveled distances\n",
    "                    aTd_case2 = calcAvgTravelDistance(path_case2, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    aTd_case4 = calcAvgTravelDistance(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    avg_travel_distances_5pct = pd.concat([avg_travel_distances_5pct, aTd_case2, aTd_case4], ignore_index= True)\n",
    "\n",
    "\n",
    "                    # number of departures per hour \n",
    "                    dPh_case2 = calcDepPerHour(path_case2, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    dPh_case4 = calcDepPerHour(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate results\n",
    "                    dep_per_hour_5pct = pd.concat([dep_per_hour_5pct, dPh_case2, dPh_case4], ignore_index= True)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 pct random global seed\n",
    "avg_trav_time_5pct_rGs = pd.DataFrame()\n",
    "avg_trav_dist_5pct_rGs = pd.DataFrame()\n",
    "rGs = [ 4711,3254, 2306, 6384,4338, 6003, 5502, 9377, 5621, 9002 ]\n",
    "for seed in rGs:\n",
    "    if (seed == 4711):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        # Create Data frame and insert first value from avg_trav_time_5pct\n",
    "        avg_tT = avg_travel_times_5pct[(avg_travel_times_5pct['alpha'] == 1.0) & (avg_travel_times_5pct['stuck_time'] == 30.0) & (avg_travel_times_5pct['sample_nr'] ==1)][\"avg_trav_time\"]\n",
    "        df1 = pd.DataFrame({\"avg_trav_time\":avg_tT, \"sample_size\": \"5-pct\", \"sample_nr\": 1, \"alpha\": 1.0, \"stuck_time\": 30.0, \"global_seed\": global_seed }, index = [rGs.index(seed)])\n",
    "        avg_trav_time_5pct_rGs = pd.concat([avg_trav_time_5pct_rGs, df1])\n",
    "        \n",
    "        # Create Data frame and insert first value from avg_trav_dist_5pct\n",
    "        avg_tD = avg_travel_distances_5pct[(avg_travel_distances_5pct['alpha'] == 1.0) & (avg_travel_distances_5pct['stuck_time'] == 30.0) & (avg_travel_distances_5pct['sample_nr'] ==1)][\"avg_trav_dist\"]\n",
    "        df1 = pd.DataFrame({\"avg_trav_dist\":avg_tD, \"sample_size\": \"5-pct\", \"sample_nr\": 1, \"alpha\": 1.0, \"stuck_time\": 30.0, \"global_seed\": global_seed }, index = [rGs.index(seed)])\n",
    "        #print(df1[\"avg_trav_dist\"])\n",
    "        avg_trav_dist_5pct_rGs = pd.concat([avg_trav_dist_5pct_rGs, df1])\n",
    "\n",
    "    elif (seed == 3254):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        path =\"/home/lola/math_cluster/output/output-lausitz-5.0-pct-1-fCf_sCF_0.05_gS_3254_3765/lausitz-5.0-pct-1-fCf_sCF_0.05_gS_3254_3765.output_trips.csv.gz\"\n",
    "        temp = pd.read_csv(path, compression= \"gzip\", sep=\";\")\n",
    "        temp[\"trav_time\"] = pd.to_timedelta(temp.trav_time)\n",
    "        avg_trav_time =  temp[\"trav_time\"].sum() / temp[\"trav_time\"].shape[0]\n",
    "        # average travel time\n",
    "        df1 = pd.DataFrame({\"avg_trav_time\":avg_trav_time, \"sample_size\": \"5-pct\", \"sample_nr\": 1, \"alpha\":1.0, \"stuck_time\": 30.0, \"global_seed\": global_seed }, index = [rGs.index(seed)])\n",
    "        avg_trav_time_5pct_rGs = pd.concat([avg_trav_time_5pct_rGs, df1], axis = 0)\n",
    "        # average traveled distance\n",
    "        avg_trav_dist = np.mean(temp[\"traveled_distance\"])\n",
    "        df_avg_trav_dist = pd.DataFrame({\"avg_trav_dist\": avg_trav_dist, \"sample_size\": \"5-pct\", \"sample_nr\": 1, \"alpha\" : 1.0, \"stuck_time\": 30.0, \"global_seed\": global_seed}, index=[rGs.index(seed)])\n",
    "        #print(df_avg_trav_dist[\"avg_trav_dist\"])\n",
    "        avg_trav_dist_5pct_rGs = pd.concat([avg_trav_dist_5pct_rGs, df_avg_trav_dist], axis = 0)\n",
    "\n",
    "    else:\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        path =\"/home/lola/math_cluster/output/output-lausitz-5.0-pct-1-fCf_sCF_0.05_gS_\" + str(seed) + \"_3765/lausitz-5.0-pct-1-fCf_sCF_0.05_gS_\" + str(seed) + \"_3765.output_trips.csv.gz\"\n",
    "        temp = pd.read_csv(path, compression= \"gzip\", sep=\";\")\n",
    "        temp[\"trav_time\"] = pd.to_timedelta(temp.trav_time)\n",
    "        avg_trav_time =  temp[\"trav_time\"].sum() / temp[\"trav_time\"].shape[0]\n",
    "        # average travel time\n",
    "        df1 = pd.DataFrame({\"avg_trav_time\":avg_trav_time, \"sample_size\": \"5-pct\", \"sample_nr\": 1, \"alpha\":1.0, \"stuck_time\": 30.0, \"global_seed\": global_seed }, index = [rGs.index(seed)])\n",
    "        avg_trav_time_5pct_rGs = pd.concat([avg_trav_time_5pct_rGs, df1], axis = 0)\n",
    "        # average traveled distance\n",
    "        avg_trav_dist = np.mean(temp[\"traveled_distance\"])\n",
    "        df_avg_trav_dist = pd.DataFrame({\"avg_trav_dist\": avg_trav_dist, \"sample_size\": \"5-pct\", \"sample_nr\": 1, \"alpha\" : 1.0, \"stuck_time\": 30.0, \"global_seed\": global_seed}, index=[rGs.index(seed)])\n",
    "        #print(df_avg_trav_dist[\"avg_trav_dist\"])\n",
    "        avg_trav_dist_5pct_rGs = pd.concat([avg_trav_dist_5pct_rGs, df_avg_trav_dist], axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_trav_time_5pct_all = pd.concat([avg_travel_times_5pct, avg_trav_time_5pct_rGs], axis = 0)\n",
    "\n",
    "avg_trav_dist_5pct_all = pd.concat([avg_travel_distances_5pct, avg_trav_dist_5pct_rGs], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_trav_time_5pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/avg_trav_time_all_5pct_samples.csv', index = False) \n",
    "avg_trav_dist_5pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/avg_trav_dist_all_5pct_samples.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11553/1471602282.py:2: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
      "/tmp/ipykernel_11553/1471602282.py:2: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
      "/tmp/ipykernel_11553/1066597957.py:2: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
      "/tmp/ipykernel_11553/1066597957.py:2: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
      "/tmp/ipykernel_11553/951122156.py:2: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
      "/tmp/ipykernel_11553/951122156.py:2: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
      "/tmp/ipykernel_11553/1471602282.py:2: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
      "/tmp/ipykernel_11553/1471602282.py:2: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
      "/tmp/ipykernel_11553/1066597957.py:2: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
      "/tmp/ipykernel_11553/1066597957.py:2: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
      "/tmp/ipykernel_11553/951122156.py:2: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
      "/tmp/ipykernel_11553/951122156.py:2: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n"
     ]
    }
   ],
   "source": [
    "flowCapF = [\"0.1\"]\n",
    "storCapF =  [\"0.1\", \"0.17783\"]\n",
    "\n",
    "# initialize empty data frames\n",
    "avg_travel_times_10pct = pd.DataFrame()\n",
    "avg_travel_distances_10pct = pd.DataFrame()\n",
    "dep_per_hour_10pct = pd.DataFrame()\n",
    "\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,11,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.1\") & (sCf == \"0.1\")):\n",
    "                    alpha = 1.0\n",
    "                    path_case1 = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" +str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_3765/lausitz-10.0-pct-\" +str(sampleNr) + \"-fCf_sCF_0.1_gS_4711_3765.output_trips.csv.gz\"\n",
    "                    path_case3 = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\" +str(sampleNr) + \"-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-10-pct-\" +str(sampleNr) + \"-fCf_sCF_0.1_gS_4711_sT_300.0_3765.output_trips.csv.gz\"\n",
    "                    \n",
    "                    # average travel time\n",
    "                    aTt_case1= calcAvgTravelTime(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    aTt_case3= calcAvgTravelTime(path_case3, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    avg_travel_times_10pct = pd.concat([avg_travel_times_10pct, aTt_case1, aTt_case3], ignore_index= True)\n",
    "\n",
    "                    # average traveled distances\n",
    "                    aTd_case1 = calcAvgTravelDistance(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    aTd_case3 = calcAvgTravelDistance(path_case3, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    avg_travel_distances_10pct = pd.concat([avg_travel_distances_10pct, aTd_case1, aTd_case3], ignore_index= True)\n",
    "\n",
    "                    # number of departures per hour \n",
    "                    dPh_case1 = calcDepPerHour(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    dPh_case3 = calcDepPerHour(path_case3, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate results\n",
    "                    dep_per_hour_10pct = pd.concat([dep_per_hour_10pct, dPh_case1, dPh_case3], ignore_index= True)\n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    path_case2 = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" + str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf +  \"_gS_4711_3765/lausitz-10.0-pct-\" + str(sampleNr) +\"-fCf_0.1_sCF_0.17783_gS_4711_3765.output_trips.csv.gz\"\n",
    "                    path_case4 = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\" +str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\"+ str(adjusted_stuck_time) + \"_3765/lausitz-10-pct-\" + str(sampleNr) + \"-fCf_0.1_sCF_0.17783_gS_4711_sT_300.0_3765.output_trips.csv.gz\"\n",
    "\n",
    "                    # average travel time\n",
    "                    aTt_case2= calcAvgTravelTime(path_case2, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    aTt_case4= calcAvgTravelTime(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    avg_travel_times_10pct = pd.concat([avg_travel_times_10pct, aTt_case2, aTt_case4], ignore_index= True)\n",
    "\n",
    "                    # average traveled distances\n",
    "                    aTd_case2 = calcAvgTravelDistance(path_case2, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    aTd_case4 = calcAvgTravelDistance(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    avg_travel_distances_10pct = pd.concat([avg_travel_distances_10pct, aTd_case2, aTd_case4], ignore_index= True)\n",
    "\n",
    "\n",
    "                    # number of departures per hour \n",
    "                    dPh_case2 = calcDepPerHour(path_case2, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    dPh_case4 = calcDepPerHour(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate results\n",
    "                    dep_per_hour_10pct = pd.concat([dep_per_hour_10pct, dPh_case2, dPh_case4], ignore_index= True)\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_travel_times_10pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/avg_trav_time_all_10pct_samples.csv', index = False) \n",
    "avg_travel_distances_10pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/avg_trav_dist_all_10pct_samples.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.25\"]\n",
    "storCapF =  [\"0.25\", \"0.35355\"]\n",
    "\n",
    "# initialize empty data frames\n",
    "avg_travel_times_25pct = pd.DataFrame()\n",
    "avg_travel_distances_25pct = pd.DataFrame()\n",
    "dep_per_hour_25pct = pd.DataFrame()\n",
    "\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,2,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "                \n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.25\") & (sCf == \"0.25\")):\n",
    "                    alpha = 1.0\n",
    "                    path_case1 = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_sCF_\" + sCf + \"_gS_4711_3765/lausitz-25.0-pct-fCf_sCF_0.25_gS_4711_3765.output_trips.csv.gz\"\n",
    "                    path_case3 = \"/home/lola/math_cluster/output/output-lausitz-25-pct-1-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-25-pct-1-fCf_sCF_0.25_gS_4711_sT_120.0_3765.output_trips.csv.gz\"\n",
    "\n",
    "                    # average travel time\n",
    "                    aTt_case1= calcAvgTravelTime(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    aTt_case3= calcAvgTravelTime(path_case3, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    avg_travel_times_25pct = pd.concat([avg_travel_times_25pct, aTt_case1, aTt_case3], ignore_index= True)\n",
    "\n",
    "                    # average traveled distances\n",
    "                    aTd_case1 = calcAvgTravelDistance(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    aTd_case3 = calcAvgTravelDistance(path_case3, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    avg_travel_distances_25pct = pd.concat([avg_travel_distances_25pct, aTd_case1, aTd_case3], ignore_index= True)\n",
    "\n",
    "                    # number of departures per hour \n",
    "                    dPh_case1 = calcDepPerHour(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    dPh_case3 = calcDepPerHour(path_case3, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate results\n",
    "                    dep_per_hour_25pct = pd.concat([dep_per_hour_25pct, dPh_case1, dPh_case3], ignore_index= True)\n",
    "\n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    path_case2 = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_3765/lausitz-25.0-pct-fCf_0.25_sCF_0.35355_gS_4711_3765.output_trips.csv.gz\"\n",
    "                    path_case4 = \"/home/lola/math_cluster/output/output-lausitz-25-pct-1-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-25-pct-1-fCf_0.25_sCF_0.35355_gS_4711_sT_120.0_3765.output_trips.csv.gz\"\n",
    "                    \n",
    "                    # average travel time\n",
    "                    aTt_case2= calcAvgTravelTime(path_case2, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    aTt_case4= calcAvgTravelTime(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    avg_travel_times_25pct = pd.concat([avg_travel_times_25pct, aTt_case2, aTt_case4], ignore_index= True)\n",
    "\n",
    "                    # average traveled distances\n",
    "                    aTd_case2 = calcAvgTravelDistance(path_case2, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    aTd_case4 = calcAvgTravelDistance(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    avg_travel_distances_25pct = pd.concat([avg_travel_distances_25pct, aTd_case2, aTd_case4], ignore_index= True)\n",
    "\n",
    "\n",
    "                    # number of departures per hour \n",
    "                    dPh_case2 = calcDepPerHour(path_case2, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    dPh_case4 = calcDepPerHour(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate results\n",
    "                    dep_per_hour_25pct = pd.concat([dep_per_hour_25pct, dPh_case2, dPh_case4], ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_travel_times_25pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/avg_trav_time_all_25pct_samples.csv', index = False) \n",
    "avg_travel_distances_25pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/avg_trav_dist_all_25pct_samples.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.5\"]\n",
    "storCapF =  [\"0.5\", \"0.5946\"]\n",
    "\n",
    "# initialize empty data frames\n",
    "avg_travel_times_50pct = pd.DataFrame()\n",
    "avg_travel_distances_50pct = pd.DataFrame()\n",
    "dep_per_hour_50pct = pd.DataFrame()\n",
    "\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,2,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "\n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.5\") & (sCf == \"0.5\")):\n",
    "                    alpha = 1.0\n",
    "                    path_case1 = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_sCF_\" + sCf + \"_gS_4711_3765/lausitz-50.0-pct-fCf_sCF_0.5_gS_4711_3765.output_trips.csv.gz\"\n",
    "                    path_case3 = \"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-50-pct-1-fCf_sCF_0.5_gS_4711_sT_60.0_3765.output_trips.csv.gz\"\n",
    "                    \n",
    "                    # average travel time\n",
    "                    aTt_case1= calcAvgTravelTime(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    aTt_case3= calcAvgTravelTime(path_case3, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    avg_travel_times_50pct = pd.concat([avg_travel_times_50pct, aTt_case1, aTt_case3], ignore_index= True)\n",
    "\n",
    "                    # average traveled distances\n",
    "                    aTd_case1 = calcAvgTravelDistance(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    aTd_case3 = calcAvgTravelDistance(path_case3, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    avg_travel_distances_50pct = pd.concat([avg_travel_distances_50pct, aTd_case1, aTd_case3], ignore_index= True)\n",
    "\n",
    "                    # number of departures per hour \n",
    "                    dPh_case1 = calcDepPerHour(path_case1, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    dPh_case3 = calcDepPerHour(path_case3, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate results\n",
    "                    dep_per_hour_50pct = pd.concat([dep_per_hour_50pct, dPh_case1, dPh_case3], ignore_index= True)\n",
    "\n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    path_case2 = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_3765/lausitz-50.0-pct-fCf_0.5_sCF_0.5946_gS_4711_3765.output_trips.csv.gz\"\n",
    "                    path_case4 = \"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-50-pct-1-fCf_0.5_sCF_0.5946_gS_4711_sT_60.0_3765.output_trips.csv.gz\"\n",
    "                    \n",
    "                    # average travel time\n",
    "                    aTt_case2= calcAvgTravelTime(path_case2, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    aTt_case4= calcAvgTravelTime(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    avg_travel_times_50pct = pd.concat([avg_travel_times_50pct, aTt_case2, aTt_case4], ignore_index= True)\n",
    "\n",
    "                    # average traveled distances\n",
    "                    aTd_case2 = calcAvgTravelDistance(path_case2, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    aTd_case4 = calcAvgTravelDistance(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    avg_travel_distances_50pct = pd.concat([avg_travel_distances_50pct, aTd_case2, aTd_case4], ignore_index= True)\n",
    "\n",
    "\n",
    "                    # number of departures per hour \n",
    "                    dPh_case2 = calcDepPerHour(path_case2, sample_size_as_string, sampleNr,  alpha, default_stuck_time, 'default')\n",
    "                    dPh_case4 = calcDepPerHour(path_case4, sample_size_as_string, sampleNr,  alpha, adjusted_stuck_time, 'default')\n",
    "\n",
    "                    # concatenate results\n",
    "                    dep_per_hour_50pct = pd.concat([dep_per_hour_50pct, dPh_case2, dPh_case4], ignore_index= True)\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25 pct doubled\n",
    "\n",
    "path_case1 = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25-pct-doubled-fCf_0.5_sCF_0.5_gS_4711_3765/lausitz-25-pct-doubled-fCf_0.5_sCF_0.5_gS_4711__3765.output_trips.csv.gz\"\n",
    "# average travel time\n",
    "aTt_case1= calcAvgTravelTime(path_case1, \"25-pct-doubled\", 1,  1.0, 30.0, 'default')\n",
    "\n",
    "# concatenate with existing values\n",
    "avg_travel_times_50pct = pd.concat([avg_travel_times_50pct, aTt_case1], ignore_index= True)\n",
    "\n",
    "# average traveled distances\n",
    "aTd_case1 = calcAvgTravelDistance(path_case1, \"25-pct-doubled\", 1,  1.0, 30.0, 'default')\n",
    "\n",
    "# concatenate with existing values\n",
    "avg_travel_distances_50pct = pd.concat([avg_travel_distances_50pct, aTd_case1], ignore_index= True)\n",
    "\n",
    "# number of departures per hour \n",
    "dPh_case1 = calcDepPerHour(path_case1, \"25-pct-doubled\", 1,  1.0, 30.0, 'default')\n",
    "\n",
    "# concatenate results\n",
    "dep_per_hour_50pct = pd.concat([dep_per_hour_50pct, dPh_case1], ignore_index= True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_travel_times_50pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/avg_trav_time_all_50pct_samples.csv', index = False) \n",
    "avg_travel_distances_50pct.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/avg_trav_dist_all_50pct_samples.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 pct\n",
    "dep_per_hour_100pct = pd.DataFrame()\n",
    "avg_travel_times_100pct = pd.DataFrame()\n",
    "avg_travel_distances_100pct = pd.DataFrame()\n",
    "path_case1 = \"/home/lola/math_cluster/output/output-lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765/lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765.output_trips.csv.gz\"\n",
    "aTt_case1= calcAvgTravelTime(path_case1, \"100-pct\", 1,  1.0, 30.0, 'default')\n",
    "\n",
    "# concatenate with existing values\n",
    "avg_travel_times_100pct = pd.concat([avg_travel_times_100pct, aTt_case1], ignore_index= True)\n",
    "\n",
    "# average traveled distances\n",
    "aTd_case1 = calcAvgTravelDistance(path_case1, \"100-pct\", 1,  1.0, 30.0, 'default')\n",
    "\n",
    "# concatenate with existing values\n",
    "avg_travel_distances_100pct = pd.concat([avg_travel_distances_100pct, aTd_case1], ignore_index= True)\n",
    "\n",
    "# number of departures per hour \n",
    "dPh_case1 = calcDepPerHour(path_case1, \"100-pct\", 1,  1.0, 30.0, 'default')\n",
    "\n",
    "# concatenate results\n",
    "dep_per_hour_100pct = pd.concat([dep_per_hour_100pct, dPh_case1], ignore_index= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11553/1471602282.py:2: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
      "/tmp/ipykernel_11553/1066597957.py:2: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
      "/tmp/ipykernel_11553/951122156.py:2: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n"
     ]
    }
   ],
   "source": [
    "# 25 pct quadrupled\n",
    "path_case1 = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25.0-pct-quadrupled-fCf_1.0_sCF_1.0_gS_4711_3765/lausitz-25-pct-quadrupled-fCf_1.0_sCF_1.0_gS_4711__3765.output_trips.csv.gz\"\n",
    "# average travel time\n",
    "aTt_case1= calcAvgTravelTime(path_case1, \"25-pct-quadrupled\", 1,  1.0, 30.0, 'default')\n",
    "\n",
    "# concatenate with existing values\n",
    "avg_travel_times_100pct = pd.concat([avg_travel_times_100pct, aTt_case1], ignore_index= True)\n",
    "\n",
    "# average traveled distances\n",
    "aTd_case1 = calcAvgTravelDistance(path_case1, \"25-pct-quadrupled\", 1,  1.0, 30.0, 'default')\n",
    "\n",
    "# concatenate with existing values\n",
    "avg_travel_distances_100pct = pd.concat([avg_travel_distances_100pct, aTd_case1], ignore_index= True)\n",
    "\n",
    "# number of departures per hour \n",
    "dPh_case1 = calcDepPerHour(path_case1, \"25-pct-quadrupled\", 1,  1.0, 30.0, 'default')\n",
    "\n",
    "# concatenate results\n",
    "dep_per_hour_100pct = pd.concat([dep_per_hour_100pct, dPh_case1], ignore_index= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat \n",
    "avg_trav_time_all = pd.concat([avg_trav_time_1pct_all, avg_trav_time_5pct_all, avg_travel_times_10pct, avg_travel_times_25pct, avg_travel_times_50pct, avg_travel_times_100pct], axis=0)\n",
    "avg_trav_dist_all = pd.concat([avg_trav_dist_1pct_all, avg_trav_dist_5pct_all, avg_travel_distances_10pct, avg_travel_distances_25pct, avg_travel_distances_50pct, avg_travel_distances_100pct], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_per_hour_all = pd.concat([dep_per_hour_1pct, dep_per_hour_5pct, dep_per_hour_10pct, dep_per_hour_25pct, dep_per_hour_50pct, dep_per_hour_100pct], axis = 0, ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_departures = pd.DataFrame()\n",
    "list_adj_Factors = []\n",
    "\n",
    "for sampleSize in dep_per_hour_all['sample_size'].unique():\n",
    "    for alpha in dep_per_hour_all[(dep_per_hour_all['sample_size'] == sampleSize)]['alpha'].unique():\n",
    "        for stuckTime in dep_per_hour_all[(dep_per_hour_all['sample_size'] == sampleSize) & (dep_per_hour_all['alpha'] == alpha)]['stuck_time'].unique():\n",
    "            for hour in dep_per_hour_all[(dep_per_hour_all['sample_size'] == sampleSize) & (dep_per_hour_all['alpha'] == alpha) & (dep_per_hour_all['stuck_time'] == stuckTime)]['hour'].unique():\n",
    "                # calculate adjustment factor\n",
    "                if (sampleSize.find('doubled') > -1):\n",
    "                    adj_Factor = 2.0\n",
    "                elif(sampleSize.find('quadrupled') > -1):\n",
    "                    adj_Factor = 1.0\n",
    "                else:\n",
    "                    sZ = sampleSize.replace(\"-pct\", \"\")\n",
    "                    adj_Factor = 100.0 / float(sZ)\n",
    "                list_adj_Factors.append(adj_Factor)\n",
    "\n",
    "                avg_departures_scaled = np.mean(dep_per_hour_all[(dep_per_hour_all['sample_size'] == sampleSize) & (dep_per_hour_all['alpha'] == alpha) & (dep_per_hour_all['stuck_time'] == stuckTime) \n",
    "                                                          & (dep_per_hour_all['hour'] == hour)]['n_departues'])*adj_Factor\n",
    "                temp = pd.DataFrame({'sample_size': sampleSize, 'alpha': alpha, 'stuck_time': stuckTime, 'hour': hour, 'avg_dep_scaled': avg_departures_scaled}, index = [0])\n",
    "                aggregated_departures = pd.concat([aggregated_departures, temp], axis = 0, ignore_index= True)\n",
    "                \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_departures_unscaled = pd.DataFrame()\n",
    "list_adj_Factors = []\n",
    "\n",
    "for sampleSize in dep_per_hour_all['sample_size'].unique():\n",
    "    for alpha in dep_per_hour_all[(dep_per_hour_all['sample_size'] == sampleSize)]['alpha'].unique():\n",
    "        for stuckTime in dep_per_hour_all[(dep_per_hour_all['sample_size'] == sampleSize) & (dep_per_hour_all['alpha'] == alpha)]['stuck_time'].unique():\n",
    "            for hour in dep_per_hour_all[(dep_per_hour_all['sample_size'] == sampleSize) & (dep_per_hour_all['alpha'] == alpha) & (dep_per_hour_all['stuck_time'] == stuckTime)]['hour'].unique():\n",
    "                # calculate adjustment factor\n",
    "                if (sampleSize.find('doubled') > -1):\n",
    "                    adj_Factor = 2.0\n",
    "                elif(sampleSize.find('quadrupled') > -1):\n",
    "                    adj_Factor = 1.0\n",
    "                else:\n",
    "                    sZ = sampleSize.replace(\"-pct\", \"\")\n",
    "                    adj_Factor = 100.0 / float(sZ)\n",
    "                list_adj_Factors.append(adj_Factor)\n",
    "\n",
    "                avg_departures_scaled = np.mean(dep_per_hour_all[(dep_per_hour_all['sample_size'] == sampleSize) & (dep_per_hour_all['alpha'] == alpha) & (dep_per_hour_all['stuck_time'] == stuckTime) \n",
    "                                                          & (dep_per_hour_all['hour'] == hour)]['n_departues'])\n",
    "                temp = pd.DataFrame({'sample_size': sampleSize, 'alpha': alpha, 'stuck_time': stuckTime, 'hour': hour, 'avg_dep_scaled': avg_departures_scaled}, index = [0])\n",
    "                aggregated_departures_unscaled = pd.concat([aggregated_departures_unscaled, temp], axis = 0, ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_trav_time_all[\"avg_trav_time\"] = (avg_trav_time_all.avg_trav_time.astype('timedelta64[s]')/  pd.Timedelta(minutes=60))*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_trav_time_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/avg_trav_time_1_to_100_pct_samples.csv', index = False)\n",
    "avg_trav_dist_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/avg_trav_dist_1_to_100_pct_sample.csv', index = False)\n",
    "aggregated_departures.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/aggregated_departures_1_to_100_pct_samples_already_scaled.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_departures_unscaled.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/aggregated_departures_1_to_100_pct_samples_unscaled.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Travel Time distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Travel time categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sixMinuteCategoriesUpToTwoHours(df):\n",
    "    categories = []\n",
    "    for t in df[\"trav_time2\"]:\n",
    "\n",
    "\n",
    "        if (0.0 <= t and t < 0.10):\n",
    "            categories.append(\"0:00_0:06\")\n",
    "        elif (0.10 <= t and t < 0.20):\n",
    "            categories.append(\"0:06_0:12\") \n",
    "        elif (0.20 <= t and t < 0.30):\n",
    "            categories.append(\"0:12_0:18\")\n",
    "        elif (0.30 <= t and t < 0.40):\n",
    "            categories.append(\"0:18_0:24\")\n",
    "        elif (0.40 <= t and t < 0.50):\n",
    "            categories.append(\"0:24_0:30\")\n",
    "        elif (0.50 <= t and t < 0.60):\n",
    "            categories.append(\"0:30_0:36\")  \n",
    "        elif (0.60 <= t and t < 0.70):\n",
    "            categories.append(\"0:36_0:42\")\n",
    "        elif (0.70 <= t and t < 0.80):\n",
    "            categories.append(\"0:42_0:48\")\n",
    "        elif (0.80 <= t and t < 0.90):\n",
    "            categories.append(\"0:48_0:54\")\n",
    "        elif (0.90 <= t and t < 1.00):\n",
    "            categories.append(\"0:54_1:00\")\n",
    "\n",
    "\n",
    "        elif (1.0 <= t and t < 1.10):\n",
    "            categories.append(\"1:00_1:06\")\n",
    "        elif (1.10 <= t and t < 1.20):\n",
    "            categories.append(\"1:06_1:12\") \n",
    "        elif (1.20 <= t and t < 1.30):\n",
    "            categories.append(\"1:12_1:18\")\n",
    "        elif (1.30 <= t and t < 1.40):\n",
    "            categories.append(\"1:18_1:24\")\n",
    "        elif (1.40 <= t and t < 1.50):\n",
    "            categories.append(\"1:24_1:30\")\n",
    "        elif (1.50 <= t and t < 1.60):\n",
    "            categories.append(\"1:30_1:36\")  \n",
    "        elif (1.60 <= t and t < 1.70):\n",
    "            categories.append(\"1:36_1:42\")\n",
    "        elif (1.70 <= t and t < 1.80):\n",
    "            categories.append(\"1:42_1:48\")\n",
    "        elif (1.80 <= t and t < 1.90):\n",
    "            categories.append(\"1:48_1:54\")\n",
    "        elif (1.90 <= t and t < 2.00):\n",
    "            categories.append(\"1:54_2:00\")\n",
    "\n",
    "        else: categories.append(\">2h\")\n",
    "\n",
    "    temp = pd.DataFrame({ \"categories\": categories})\n",
    "    temp = temp.sort_values(by=['categories'])\n",
    "\n",
    "    freq_6_min_cat = []\n",
    "    for cat in temp.categories.unique():\n",
    "        f = temp[(temp[\"categories\"]==cat)].shape[0]\n",
    "        freq_6_min_cat.append(f)\n",
    "    \n",
    "    trav_times_6min_cat = pd.DataFrame({\"category\":temp.categories.unique(), \"freq\": freq_6_min_cat })\n",
    "    return trav_times_6min_cat \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateSixMinCat(pathToFile, SampleNr):\n",
    "    temp = pd.read_csv(pathToFile, compression= \"gzip\", sep=\";\")\n",
    "    temp[\"trav_time\"] = pd.to_timedelta(temp.trav_time)\n",
    "    temp[\"trav_time2\"] = temp.trav_time.astype('timedelta64[s]')/  pd.Timedelta(minutes=60)\n",
    "    trav_time_cat = sixMinuteCategoriesUpToTwoHours(temp)\n",
    "    sampleNrAsString = str(SampleNr)\n",
    "    trav_time_cat.rename(columns={\"freq\": \"freq_\" + sampleNrAsString}, inplace = True )\n",
    "    return trav_time_cat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.01\"]\n",
    "storCapF =  [\"0.01\", \"0.03162\"]\n",
    "\n",
    "# initialize empty data frames\n",
    "avg_travel_time_dist_1pct_case1 = []\n",
    "avg_travel_time_dist_1pct_case2 = []\n",
    "avg_travel_time_dist_1pct_case3 = []\n",
    "avg_travel_time_dist_1pct_case4 = []\n",
    "\n",
    "\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "        for sampleNr in range(1,11,1):\n",
    "            # calculate adjusted stuck time\n",
    "            default_stuck_time = 30.0\n",
    "            adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "            # declare sample size as str \"1-pct\"\n",
    "            sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "\n",
    "            if ((fCf == \"0.01\") & (sCf == '0.01')):\n",
    "                # paths for case 1 and 3 \n",
    "                path_case1 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\"+ str(sampleNr) +\"-fCf_sCF_\" + sCf + \"_gS_default_3765/lausitz-1pct-\"+ str(sampleNr) +\"-fCf_sCf_0.01_gS_default_3765.output_trips.csv.gz\"\n",
    "                path_case3 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" +str(sampleNr) +\"-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-1-pct-\" +str(sampleNr) + \"-fCf_sCF_0.01_gS_4711_sT_3000.0_3765.output_trips.csv.gz\"\n",
    "                \n",
    "                # calculate the six minute categories and append them\n",
    "                avg_travel_time_dist_1pct_case1.append(calculateSixMinCat(path_case1, sampleNr))\n",
    "                avg_travel_time_dist_1pct_case3.append(calculateSixMinCat(path_case3, sampleNr))\n",
    "\n",
    "            else:\n",
    "                \n",
    "                # calculate the six minute categories and append them\n",
    "                path_case2 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" +str(sampleNr) + \"-fCf_0.01_sCF_\" + sCf + \"_gS_default_3765/lausitz-1pct-\" + str(sampleNr) + \"-fCf_0.01_sCf_0.03162_gS_default_3765.output_trips.csv.gz\"\n",
    "                path_case4 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" + str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/lausitz-1-pct-\" + str(sampleNr) + \"-fCf_0.01_sCF_0.03162_gS_4711_sT_3000.0_3765.output_trips.csv.gz\"\n",
    "                \n",
    "                avg_travel_time_dist_1pct_case2.append(calculateSixMinCat(path_case2, sampleNr))\n",
    "                avg_travel_time_dist_1pct_case4.append(calculateSixMinCat(path_case4, sampleNr))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_travel_time_dist_1pct_case1 = pd.DataFrame({'sixMinCategories': avg_travel_time_dist_1pct_case1[0]['category'], 'freq_1': avg_travel_time_dist_1pct_case1[0]['freq_1'] })\n",
    "df_avg_travel_time_dist_1pct_case2 = pd.DataFrame({'sixMinCategories': avg_travel_time_dist_1pct_case2[0]['category'], 'freq_1': avg_travel_time_dist_1pct_case2[0]['freq_1'] })\n",
    "df_avg_travel_time_dist_1pct_case3 = pd.DataFrame({'sixMinCategories': avg_travel_time_dist_1pct_case3[0]['category'], 'freq_1': avg_travel_time_dist_1pct_case3[0]['freq_1'] })\n",
    "df_avg_travel_time_dist_1pct_case4 = pd.DataFrame({'sixMinCategories': avg_travel_time_dist_1pct_case4[0]['category'], 'freq_1': avg_travel_time_dist_1pct_case4[0]['freq_1'] })\n",
    "\n",
    "for sampleNr in range(1,10,1):\n",
    "    colname = \"freq_\" + str(sampleNr + 1)\n",
    "    df_avg_travel_time_dist_1pct_case1.insert(sampleNr + 1, colname,avg_travel_time_dist_1pct_case1[sampleNr][colname] )\n",
    "    df_avg_travel_time_dist_1pct_case2.insert(sampleNr + 1, colname,avg_travel_time_dist_1pct_case2[sampleNr][colname] )\n",
    "    df_avg_travel_time_dist_1pct_case3.insert(sampleNr + 1, colname,avg_travel_time_dist_1pct_case3[sampleNr][colname] )\n",
    "    df_avg_travel_time_dist_1pct_case4.insert(sampleNr + 1, colname,avg_travel_time_dist_1pct_case4[sampleNr][colname] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_case1 = []\n",
    "mean_case2 = []\n",
    "mean_case3 = []\n",
    "mean_case4 = []\n",
    "for category in range(0,21,1):\n",
    "    mean_case1.append(np.mean(df_avg_travel_time_dist_1pct_case1.iloc[category,1:11]))\n",
    "    mean_case2.append(np.mean(df_avg_travel_time_dist_1pct_case2.iloc[category,1:11]))\n",
    "    mean_case3.append(np.mean(df_avg_travel_time_dist_1pct_case3.iloc[category,1:11]))\n",
    "    mean_case4.append(np.mean(df_avg_travel_time_dist_1pct_case4.iloc[category,1:11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_travel_time_dist_1pct_case1.insert(11, 'mean', mean_case1)\n",
    "df_avg_travel_time_dist_1pct_case2.insert(11, 'mean', mean_case2)\n",
    "df_avg_travel_time_dist_1pct_case3.insert(11, 'mean', mean_case3)\n",
    "df_avg_travel_time_dist_1pct_case4.insert(11, 'mean', mean_case4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_travel_time_dist_1pct_case1.insert(12, 'sample_size', \"1-pct\")\n",
    "df_avg_travel_time_dist_1pct_case1.insert(13, 'alpha', 1.0)\n",
    "df_avg_travel_time_dist_1pct_case1.insert(14, 'stuck_time', 30.0)\n",
    "df_avg_travel_time_dist_1pct_case1.insert(15,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_travel_time_dist_1pct_case2.insert(12, 'sample_size', \"1-pct\")\n",
    "df_avg_travel_time_dist_1pct_case2.insert(13, 'alpha', 0.75)\n",
    "df_avg_travel_time_dist_1pct_case2.insert(14, 'stuck_time', 30.0)\n",
    "df_avg_travel_time_dist_1pct_case2.insert(15,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_travel_time_dist_1pct_case3.insert(12, 'sample_size', \"1-pct\")\n",
    "df_avg_travel_time_dist_1pct_case3.insert(13, 'alpha', 1.0)\n",
    "df_avg_travel_time_dist_1pct_case3.insert(14, 'stuck_time', 3000.0)\n",
    "df_avg_travel_time_dist_1pct_case3.insert(15,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_travel_time_dist_1pct_case4.insert(12, 'sample_size', \"1-pct\")\n",
    "df_avg_travel_time_dist_1pct_case4.insert(13, 'alpha', 0.75)\n",
    "df_avg_travel_time_dist_1pct_case4.insert(14, 'stuck_time', 3000.0)\n",
    "df_avg_travel_time_dist_1pct_case4.insert(15,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_travel_time_dist_1pct_all = pd.concat([df_avg_travel_time_dist_1pct_case1, df_avg_travel_time_dist_1pct_case2, df_avg_travel_time_dist_1pct_case3, df_avg_travel_time_dist_1pct_case4], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_travel_time_dist_1pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/trav_time_categories_1pct_cases_1_4.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trav_time_5pct_categories = []\n",
    "for elem in range(1,11,1):\n",
    "    if (elem == 6):\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765/lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765-2.output_trips.csv.gz\"\n",
    "        temp = pd.read_csv(path, compression = \"gzip\", sep = \";\")\n",
    "        temp[\"trav_time\"] = pd.to_timedelta(temp.trav_time)\n",
    "        temp[\"trav_time2\"] = temp.trav_time.astype('timedelta64[s]')/  pd.Timedelta(minutes=60)\n",
    "        trav_time_cat = sixMinuteCategoriesUpToTwoHours(temp)\n",
    "        trav_time_cat.rename(columns={\"freq\": \"freq_\" + str(elem)}, inplace = True )\n",
    "        trav_time_5pct_categories.append(trav_time_cat)\n",
    "    else: \n",
    "        # ERROR WRONG PATH, Now corrected\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(elem) + \"-fCf_sCF_0.05_gS_4711_3765/lausitz-5.0-pct-\" + str(elem) + \"-fCf_sCF_0.05_gS_4711_3765.output_trips.csv.gz\"\n",
    "        temp = pd.read_csv(path, compression= \"gzip\", sep=\";\")\n",
    "        temp[\"trav_time\"] = pd.to_timedelta(temp.trav_time)\n",
    "        temp[\"trav_time2\"] = temp.trav_time.astype('timedelta64[s]')/  pd.Timedelta(minutes=60)\n",
    "        trav_time_cat = sixMinuteCategoriesUpToTwoHours(temp)\n",
    "        trav_time_cat.rename(columns={\"freq\": \"freq_\" + str(elem)}, inplace = True )\n",
    "        trav_time_5pct_categories.append(trav_time_cat)\n",
    "# left join and calculate mean\n",
    "trav_time_5pct_cat_all = pd.merge(trav_time_5pct_categories[0], trav_time_5pct_categories[1], on = ['category'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    trav_time_5pct_cat_all = pd.merge(trav_time_5pct_cat_all, trav_time_5pct_categories[elem], on = ['category'], how='left')\n",
    "\n",
    "trav_time_5pct_cat_all['mean'] = trav_time_5pct_cat_all.iloc[:,1:11].mean(axis = 1)\n",
    "trav_time_5pct_cat_all.insert(12, 'sample_size', \"5-pct\")\n",
    "trav_time_5pct_cat_all.insert(13, 'alpha', 1.0)\n",
    "trav_time_5pct_cat_all.insert(14, 'stuck_time', 30.0)\n",
    "trav_time_5pct_cat_all.insert(15,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8501/1743431341.py:4: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(path, compression= \"gzip\", sep=\";\")\n"
     ]
    }
   ],
   "source": [
    "trav_time_10pct_categories = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" +str(elem) + \"-fCf_sCF_0.1_gS_4711_3765/lausitz-10.0-pct-\" +str(elem) + \"-fCf_sCF_0.1_gS_4711_3765.output_trips.csv.gz\"\n",
    "    temp = pd.read_csv(path, compression= \"gzip\", sep=\";\")\n",
    "    temp[\"trav_time\"] = pd.to_timedelta(temp.trav_time)\n",
    "    temp[\"trav_time2\"] = temp.trav_time.astype('timedelta64[s]')/  pd.Timedelta(minutes=60)\n",
    "    trav_time_cat = sixMinuteCategoriesUpToTwoHours(temp)\n",
    "    trav_time_cat.rename(columns={\"freq\": \"freq_\" + str(elem)}, inplace = True )\n",
    "    trav_time_10pct_categories.append(trav_time_cat)\n",
    "# left join and calculate mean\n",
    "trav_time_10pct_cat_all = pd.merge(trav_time_10pct_categories[0], trav_time_10pct_categories[1], on = ['category'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    trav_time_10pct_cat_all = pd.merge(trav_time_10pct_cat_all, trav_time_10pct_categories[elem], on = ['category'], how='left')\n",
    "\n",
    "trav_time_10pct_cat_all['mean'] = trav_time_10pct_cat_all.iloc[:,1:11].mean(axis = 1)\n",
    "trav_time_10pct_cat_all.insert(12, 'sample_size', \"10-pct\")\n",
    "trav_time_10pct_cat_all.insert(13, 'alpha', 1.0)\n",
    "trav_time_10pct_cat_all.insert(14, 'stuck_time', 30.0)\n",
    "trav_time_10pct_cat_all.insert(15,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_sCF_0.25_gS_4711_3765/lausitz-25.0-pct-fCf_sCF_0.25_gS_4711_3765.output_trips.csv.gz\"\n",
    "t_25pct = pd.read_csv(path, compression= \"gzip\", sep=\";\")\n",
    "t_25pct[\"trav_time\"] = pd.to_timedelta(t_25pct.trav_time)\n",
    "t_25pct[\"trav_time2\"] = t_25pct.trav_time.astype('timedelta64[s]')/  pd.Timedelta(minutes=60)\n",
    "trav_time_25pct_categories = sixMinuteCategoriesUpToTwoHours(t_25pct)\n",
    "trav_time_25pct_categories.rename(columns={\"freq\": \"mean\"}, inplace = True )\n",
    "trav_time_25pct_categories.insert(2, 'sample_size', \"25-pct\")\n",
    "trav_time_25pct_categories.insert(3, 'alpha', 1.0)\n",
    "trav_time_25pct_categories.insert(4, 'stuck_time', 30.0)\n",
    "trav_time_25pct_categories.insert(5,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_sCF_0.5_gS_4711_3765/lausitz-50.0-pct-fCf_sCF_0.5_gS_4711_3765.output_trips.csv.gz\"\n",
    "t_50pct = pd.read_csv(path, compression= \"gzip\", sep=\";\")\n",
    "t_50pct[\"trav_time\"] = pd.to_timedelta(t_50pct.trav_time)\n",
    "t_50pct[\"trav_time2\"] = t_50pct.trav_time.astype('timedelta64[s]')/  pd.Timedelta(minutes=60)\n",
    "trav_time_50pct_categories = sixMinuteCategoriesUpToTwoHours(t_50pct)\n",
    "trav_time_50pct_categories.rename(columns={\"freq\": \"mean\"}, inplace = True )\n",
    "trav_time_50pct_categories.insert(2, 'sample_size', \"50-pct\")\n",
    "trav_time_50pct_categories.insert(3, 'alpha', 1.0)\n",
    "trav_time_50pct_categories.insert(4, 'stuck_time', 30.0)\n",
    "trav_time_50pct_categories.insert(5,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25 pct doubled\n",
    "path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25-pct-doubled-fCf_0.5_sCF_0.5_gS_4711_3765/lausitz-25-pct-doubled-fCf_0.5_sCF_0.5_gS_4711__3765.output_trips.csv.gz\"\n",
    "t_25pct_doubled = pd.read_csv(path, compression= \"gzip\", sep=\";\")\n",
    "t_25pct_doubled[\"trav_time\"] = pd.to_timedelta(t_25pct_doubled.trav_time)\n",
    "t_25pct_doubled[\"trav_time2\"] = t_25pct_doubled.trav_time.astype('timedelta64[s]')/  pd.Timedelta(minutes=60)\n",
    "trav_time_25pct_doubled_categories = sixMinuteCategoriesUpToTwoHours(t_25pct_doubled)\n",
    "trav_time_25pct_doubled_categories.rename(columns={\"freq\": \"mean\"}, inplace = True )\n",
    "trav_time_25pct_doubled_categories.insert(2, 'sample_size', \"25-pct-doubled\")\n",
    "trav_time_25pct_doubled_categories.insert(3, 'alpha', 1.0)\n",
    "trav_time_25pct_doubled_categories.insert(4, 'stuck_time', 30.0)\n",
    "trav_time_25pct_doubled_categories.insert(5,'global_seed', \"default\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765/lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765.output_trips.csv.gz\"\n",
    "t_100pct = pd.read_csv(path, compression= \"gzip\", sep=\";\")\n",
    "t_100pct[\"trav_time\"] = pd.to_timedelta(t_100pct.trav_time)\n",
    "t_100pct[\"trav_time2\"] = t_100pct.trav_time.astype('timedelta64[s]')/  pd.Timedelta(minutes=60)\n",
    "trav_time_100pct_categories = sixMinuteCategoriesUpToTwoHours(t_100pct)\n",
    "trav_time_100pct_categories.rename(columns={\"freq\": \"mean\"}, inplace = True )\n",
    "trav_time_100pct_categories.insert(2, 'sample_size', \"100-pct\")\n",
    "trav_time_100pct_categories.insert(3, 'alpha', 1.0)\n",
    "trav_time_100pct_categories.insert(4, 'stuck_time', 30.0)\n",
    "trav_time_100pct_categories.insert(5,'global_seed', \"default\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8501/3909260947.py:3: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  t_25pct_quadrupled = pd.read_csv(path, compression= \"gzip\", sep=\";\")\n"
     ]
    }
   ],
   "source": [
    "# 25 pct quadrupled\n",
    "path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25.0-pct-quadrupled-fCf_1.0_sCF_1.0_gS_4711_3765/lausitz-25-pct-quadrupled-fCf_1.0_sCF_1.0_gS_4711__3765.output_trips.csv.gz\"\n",
    "t_25pct_quadrupled = pd.read_csv(path, compression= \"gzip\", sep=\";\")\n",
    "t_25pct_quadrupled[\"trav_time\"] = pd.to_timedelta(t_25pct_quadrupled.trav_time)\n",
    "t_25pct_quadrupled[\"trav_time2\"] = t_25pct_quadrupled.trav_time.astype('timedelta64[s]')/  pd.Timedelta(minutes=60)\n",
    "trav_time_25pct_quadrupled_categories = sixMinuteCategoriesUpToTwoHours(t_25pct_quadrupled)\n",
    "trav_time_25pct_quadrupled_categories.rename(columns={\"freq\": \"mean\"}, inplace = True )\n",
    "trav_time_25pct_quadrupled_categories.insert(2, 'sample_size', \"25-pct-quadrupled\")\n",
    "trav_time_25pct_quadrupled_categories.insert(3, 'alpha', 1.0)\n",
    "trav_time_25pct_quadrupled_categories.insert(4, 'stuck_time', 30.0)\n",
    "trav_time_25pct_quadrupled_categories.insert(5,'global_seed', \"default\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concat and write csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trav_time_categories_all = pd.concat([df_avg_travel_time_dist_1pct_all, trav_time_5pct_cat_all, trav_time_10pct_cat_all,  trav_time_25pct_categories, trav_time_50pct_categories,trav_time_25pct_doubled_categories, trav_time_100pct_categories, trav_time_25pct_quadrupled_categories], axis = 0)\n",
    "trav_time_categories_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/trav_time_categories_1_100.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Network Congestion Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nci_1pct = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-1pct-\"+ str(elem) + \"-fCf_sCF_0.01_gS_default_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\" \n",
    "    temp = pd.read_csv(path, sep=\",\")\n",
    "    temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "    nci_1pct.append(temp)\n",
    "\n",
    "nci_1pct_df = pd.merge(nci_1pct[0], nci_1pct[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    nci_1pct_df = pd.merge(nci_1pct_df, nci_1pct[elem], on = ['road_type', 'hour'], how='left')\n",
    "\n",
    "nci_1pct_df['congestion_index_mean'] = nci_1pct_df.iloc[: ,2:11].mean(axis=1)\n",
    "nci_1pct_df.insert(13, 'sample_size', \"1-pct\")\n",
    "nci_1pct_df.insert(14, 'alpha', 1.0)\n",
    "nci_1pct_df.insert(15, 'stuck_time', 30.0)\n",
    "nci_1pct_df.insert(16,'global_seed', \"default\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nci_1pct_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "    #       /home/lola/math_cluster/output/output-lausitz-1pct-1-fCf_0.01_sCF_0.03162_gS_default_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-1pct-\"+ str(elem) + \"-fCf_0.01_sCF_0.03162_gS_default_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\" \n",
    "    temp = pd.read_csv(path, sep=\",\")\n",
    "    temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "    nci_1pct_sCf.append(temp)\n",
    "\n",
    "nci_1pct_sCf_df = pd.merge(nci_1pct_sCf[0], nci_1pct_sCf[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    nci_1pct_sCf_df = pd.merge(nci_1pct_sCf_df, nci_1pct_sCf[elem], on = ['road_type', 'hour'], how='left')\n",
    "\n",
    "nci_1pct_sCf_df['congestion_index_mean'] = nci_1pct_sCf_df.iloc[: ,2:11].mean(axis=1)\n",
    "nci_1pct_sCf_df.insert(13, 'sample_size', \"1-pct\")\n",
    "nci_1pct_sCf_df.insert(14, 'alpha', 0.75)\n",
    "nci_1pct_sCf_df.insert(15, 'stuck_time', 30.0)\n",
    "nci_1pct_sCf_df.insert(16,'global_seed', \"default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random global seed\n",
    "nci_1pct_rGs = []\n",
    "rGs = [ 4711,3254, 2306, 6384,4338, 6003, 5502, 9377, 5621, 9002 ]\n",
    "for seed in rGs:\n",
    "    if (seed == 4711):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        temp = nci_1pct_df[[\"road_type\", \"hour\", \"congestion_index_1\"]].copy()\n",
    "        nci_1pct_rGs.append(temp)\n",
    "    elif (seed == 3254):\n",
    "        global_seed  = \"rnd_\" + str(seed)\n",
    "        path =\"/home/lola/math_cluster/output/output-lausitz-1pct-1-fCf_sCF_0.01_gS_3254_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "        temp = pd.read_csv(path, sep = \",\")\n",
    "        temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(rGs.index(seed) + 1)}, inplace = True)\n",
    "        nci_1pct_rGs.append(temp)\n",
    "    else:\n",
    "        global_seed  = \"rnd_\" + str(seed)\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-1.0-pct-1-fCf_sCF_0.01_gS_\"+str(seed) + \"_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "        temp = pd.read_csv(path, sep = \",\")\n",
    "        temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(rGs.index(seed) + 1 )}, inplace = True)\n",
    "        nci_1pct_rGs.append(temp)\n",
    "\n",
    "nci_1pct_rGs_df = pd.merge(nci_1pct_rGs[0], nci_1pct_rGs[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    nci_1pct_rGs_df = pd.merge(nci_1pct_rGs_df, nci_1pct_rGs[elem], on = ['road_type', 'hour'], how='left')\n",
    "\n",
    "nci_1pct_rGs_df['congestion_index_mean'] = nci_1pct_rGs_df.iloc[: ,2:11].mean(axis=1)\n",
    "nci_1pct_rGs_df.insert(13, 'sample_size', \"1-pct\")\n",
    "nci_1pct_rGs_df.insert(14, 'alpha', 1.0)\n",
    "nci_1pct_rGs_df.insert(15, 'stuck_time', 30.0)\n",
    "nci_1pct_rGs_df.insert(16,'global_seed', \"rnd\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nci_1pct_sT = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" + str(elem) + \"-fCf_sCF_0.01_gS_4711_sT_3000.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\" \n",
    "    temp = pd.read_csv(path, sep=\",\")\n",
    "    temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "    nci_1pct_sT.append(temp)\n",
    "    \n",
    "nci_1pct_sT_df = pd.merge(nci_1pct_sT[0], nci_1pct_sT[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    nci_1pct_sT_df = pd.merge(nci_1pct_sT_df, nci_1pct_sT[elem], on = ['road_type', 'hour'], how='left')    \n",
    "    \n",
    "nci_1pct_sT_df['congestion_index_mean'] = nci_1pct_sT_df.iloc[: ,2:11].mean(axis=1)\n",
    "nci_1pct_sT_df.insert(13, 'sample_size', \"1-pct\")\n",
    "nci_1pct_sT_df.insert(14, 'alpha', 1.0)\n",
    "nci_1pct_sT_df.insert(15, 'stuck_time', 3000.0)\n",
    "nci_1pct_sT_df.insert(16,'global_seed', \"default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nci_1pct_sT_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" + str(elem) + \"-fCf_0.01_sCF_0.03162_gS_4711_sT_3000.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\" \n",
    "    temp = pd.read_csv(path, sep=\",\")\n",
    "    temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "    nci_1pct_sT_sCf.append(temp)\n",
    "    \n",
    "nci_1pct_sT_sCf_df = pd.merge(nci_1pct_sT_sCf[0], nci_1pct_sT_sCf[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    nci_1pct_sT_sCf_df = pd.merge(nci_1pct_sT_sCf_df, nci_1pct_sT_sCf[elem], on = ['road_type', 'hour'], how='left')    \n",
    "    \n",
    "nci_1pct_sT_sCf_df['congestion_index_mean'] = nci_1pct_sT_sCf_df.iloc[: ,2:11].mean(axis=1)\n",
    "nci_1pct_sT_sCf_df.insert(13, 'sample_size', \"1-pct\")\n",
    "nci_1pct_sT_sCf_df.insert(14, 'alpha', 0.75)\n",
    "nci_1pct_sT_sCf_df.insert(15, 'stuck_time', 3000.0)\n",
    "nci_1pct_sT_sCf_df.insert(16,'global_seed', \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nci_1pct_all = pd.concat([nci_1pct_df, nci_1pct_sCf_df, nci_1pct_rGs_df,  nci_1pct_sT_df, nci_1pct_sT_sCf_df ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nci_1pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/nci_all_1pct_samples.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nci_5pct = []\n",
    "for elem in range(1,11,1):\n",
    "    if (elem ==6): \n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "        temp = pd.read_csv(path, sep=\",\")\n",
    "        temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "        nci_5pct.append(temp)\n",
    "\n",
    "\n",
    "    else:\n",
    "    #       /home/lola/math_cluster/output/output-lausitz-5.0-pct-1-fCf_sCF_0.05_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\"+ str(elem) + \"-fCf_sCF_0.05_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\" \n",
    "        temp = pd.read_csv(path, sep=\",\")\n",
    "        temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "        nci_5pct.append(temp)\n",
    "\n",
    "\n",
    "\n",
    "nci_5pct_df = pd.merge(nci_5pct[0], nci_5pct[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    nci_5pct_df = pd.merge(nci_5pct_df, nci_5pct[elem], on = ['road_type', 'hour'], how='left')    \n",
    "    \n",
    "nci_5pct_df['congestion_index_mean'] = nci_5pct_df.iloc[: ,2:11].mean(axis=1)\n",
    "nci_5pct_df.insert(13, 'sample_size', \"5-pct\")\n",
    "nci_5pct_df.insert(14, 'alpha', 1.0)\n",
    "nci_5pct_df.insert(15, 'stuck_time', 30.0)\n",
    "nci_5pct_df.insert(16,'global_seed', \"default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nci_5pct_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "    if (elem == 6):\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_0.05_sCF_0.10574_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\" \n",
    "        temp = pd.read_csv(path, sep=\",\")\n",
    "        temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "        nci_5pct_sCf.append(temp)\n",
    "\n",
    "    else: \n",
    "    #           /home/lola/math_cluster/output/output-lausitz-5.0-pct-7-fCf_0.05_sCF_0.10574_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\"+ str(elem) + \"-fCf_0.05_sCF_0.10574_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\" \n",
    "        temp = pd.read_csv(path, sep=\",\")\n",
    "        temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "        nci_5pct_sCf.append(temp)\n",
    "\n",
    "nci_5pct_sCf_df = pd.merge(nci_5pct_sCf[0], nci_5pct_sCf[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    nci_5pct_sCf_df = pd.merge(nci_5pct_sCf_df, nci_5pct_sCf[elem], on = ['road_type', 'hour'], how='left')    \n",
    "    \n",
    "nci_5pct_sCf_df['congestion_index_mean'] = nci_5pct_sCf_df.iloc[: ,2:11].mean(axis=1)\n",
    "nci_5pct_sCf_df.insert(13, 'sample_size', \"5-pct\")\n",
    "nci_5pct_sCf_df.insert(14, 'alpha',0.75)\n",
    "nci_5pct_sCf_df.insert(15, 'stuck_time', 30.0)\n",
    "nci_5pct_sCf_df.insert(16,'global_seed', \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random global seed\n",
    "nci_5pct_rGs = []\n",
    "\n",
    "rGs = [ 4711,3254, 2306, 6384,4338, 6003, 5502, 9377, 5621, 9002 ]\n",
    "for seed in rGs:\n",
    "    if (seed == 4711):\n",
    "        global_seed = \"rnd_\" + str(seed)\n",
    "        temp = nci_5pct_df[[\"road_type\", \"hour\", \"congestion_index_1\"]].copy()\n",
    "        nci_5pct_rGs.append(temp)\n",
    "    elif (seed == 3254):\n",
    "        global_seed  = \"rnd_\" + str(seed)\n",
    "        path =\"/home/lola/math_cluster/output/output-lausitz-5.0-pct-1-fCf_sCF_0.05_gS_3254_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "        temp = pd.read_csv(path, sep = \",\")\n",
    "        temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(rGs.index(seed) + 1)}, inplace = True)\n",
    "        nci_5pct_rGs.append(temp)\n",
    "        \n",
    "    else:\n",
    "        if (seed == 6384 or seed == 6003):\n",
    "            continue\n",
    "        global_seed  = \"rnd_\" + str(seed)\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-1-fCf_sCF_0.05_gS_\" + str(seed) + \"_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "        temp = pd.read_csv(path, sep = \",\")\n",
    "        temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(rGs.index(seed) + 1)}, inplace = True)\n",
    "        nci_5pct_rGs.append(temp)\n",
    "\n",
    "\n",
    "nci_5pct_rGs_df = pd.merge(nci_5pct_rGs[0], nci_5pct_rGs[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,8,1):\n",
    "    nci_5pct_rGs_df = pd.merge(nci_5pct_rGs_df, nci_5pct_rGs[elem], on = ['road_type', 'hour'], how='left')\n",
    "\n",
    "nci_5pct_rGs_df['congestion_index_mean'] = nci_5pct_rGs_df.iloc[: ,2:9].mean(axis=1)\n",
    "nci_5pct_rGs_df.insert(11, 'sample_size', \"5-pct\")\n",
    "nci_5pct_rGs_df.insert(12, 'alpha', 1.0)\n",
    "nci_5pct_rGs_df.insert(13, 'stuck_time', 30.0)\n",
    "nci_5pct_rGs_df.insert(14,'global_seed', \"rnd\")\n",
    "\n",
    "\n",
    "#  [Errno 2] No such file or directory: '/home/lola/math_cluster/output/output-lausitz-5.0-pct-1-fCf_sCF_0.05_gS_6384_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv'\n",
    "# 6003 auch nicht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 pct, alpha 1.0, sT scaled\n",
    "nci_5pct_sT = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str(elem) + \"-fCf_sCF_0.05_gS_4711_sT_600.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "    temp = pd.read_csv(path)\n",
    "    temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "    nci_5pct_sT.append(temp)\n",
    "\n",
    "nci_5pct_sT_df = pd.merge(nci_5pct_sT[0], nci_5pct_sT[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    nci_5pct_sT_df = pd.merge(nci_5pct_sT_df, nci_5pct_sT[elem], on = ['road_type', 'hour'], how='left')\n",
    "\n",
    "\n",
    "nci_5pct_sT_df['congestion_index_mean'] = nci_5pct_sT_df.iloc[: ,2:11].mean(axis=1)\n",
    "nci_5pct_sT_df.insert(13, 'sample_size', \"5-pct\")\n",
    "nci_5pct_sT_df.insert(14, 'alpha',1.0)\n",
    "nci_5pct_sT_df.insert(15, 'stuck_time', 600.0)\n",
    "nci_5pct_sT_df.insert(16,'global_seed', \"default\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nci_5pct_sT_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str(elem) + \"-fCf_0.05_sCF_0.10574_gS_4711_sT_600.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "    temp = pd.read_csv(path)\n",
    "    temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "    nci_5pct_sT_sCf.append(temp)\n",
    "\n",
    "nci_5pct_sT_sCf_df = pd.merge(nci_5pct_sT_sCf[0], nci_5pct_sT_sCf[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    nci_5pct_sT_sCf_df = pd.merge(nci_5pct_sT_sCf_df, nci_5pct_sT_sCf[elem], on = ['road_type', 'hour'], how='left')\n",
    "    \n",
    "nci_5pct_sT_sCf_df['congestion_index_mean'] = nci_5pct_sT_sCf_df.iloc[: ,2:11].mean(axis=1)\n",
    "nci_5pct_sT_sCf_df.insert(13, 'sample_size', \"5-pct\")\n",
    "nci_5pct_sT_sCf_df.insert(14, 'alpha',0.75)\n",
    "nci_5pct_sT_sCf_df.insert(15, 'stuck_time', 600.0)\n",
    "nci_5pct_sT_sCf_df.insert(16,'global_seed', \"default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nci_5pct_all = pd.concat([nci_5pct_df, nci_5pct_sCf_df,nci_5pct_rGs_df, nci_5pct_sT_df, nci_5pct_sT_sCf_df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nci_5pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/nci_all_5pct_samples.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 pct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flowCapF = [\"0.1\"]\n",
    "storCapF =  [\"0.1\", \"0.17783\"]\n",
    "stuckTimes = [\"30.0\", \"300.0\"]\n",
    "\n",
    "nci_10pct = pd.DataFrame()\n",
    "\n",
    "counter = 0\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "        for sT in stuckTimes:\n",
    "            for sampleNr in range(1,11,1):\n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.1\") & (sCf == \"0.1\") & (sT == \"30.0\")):\n",
    "                    path = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" +str(sampleNr) + \"-fCf_sCF_0.1_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"       \n",
    "                elif((fCf == \"0.1\") & (sCf ==  \"0.17783\") & (sT == \"30.0\")):\n",
    "                    path = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" + str(sampleNr) + \"-fCf_0.1_sCF_0.17783_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "                elif((fCf == \"0.1\") & (sCf == \"0.1\") & (sT == \"300.0\")):\n",
    "                    path = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\" + str(sampleNr) + \"-fCf_sCF_0.1_gS_4711_sT_300.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "                elif((fCf == \"0.1\") & (sCf ==  \"0.17783\") & (sT == \"300.0\")):\n",
    "                    path = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\" + str(sampleNr) + \"-fCf_0.1_sCF_0.17783_gS_4711_sT_300.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"  \n",
    "                else: \n",
    "                    print(\"case not found\")\n",
    "                    break\n",
    "                temp = pd.read_csv(path, compression= \"gzip\", sep=\";\")\n",
    "                temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nci_10pct = []\n",
    "for elem in range(1,11,1):\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\"+ str(elem) + \"-fCf_sCF_0.1_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\" \n",
    "        temp = pd.read_csv(path)\n",
    "        temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "        nci_10pct.append(temp)\n",
    "\n",
    "nci_10pct_df = pd.merge(nci_10pct[0], nci_10pct[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    nci_10pct_df = pd.merge(nci_10pct_df, nci_10pct[elem], on = ['road_type', 'hour'], how='left')\n",
    "    \n",
    "nci_10pct_df['congestion_index_mean'] = nci_10pct_df.iloc[: ,2:11].mean(axis=1)        \n",
    "nci_10pct_df.insert(13, 'sample_size', \"10-pct\")\n",
    "nci_10pct_df.insert(14, 'alpha',1.0)\n",
    "nci_10pct_df.insert(15, 'stuck_time', 30.0)\n",
    "nci_10pct_df.insert(16,'global_seed', \"default\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nci_10pct_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\"+ str(elem) + \"-fCf_0.1_sCF_0.17783_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\" \n",
    "        temp = pd.read_csv(path)\n",
    "        temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "        nci_10pct_sCf.append(temp)\n",
    "\n",
    "nci_10pct_sCf_df = pd.merge(nci_10pct_sCf[0], nci_10pct_sCf[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    nci_10pct_sCf_df = pd.merge(nci_10pct_sCf_df, nci_10pct_sCf[elem], on = ['road_type', 'hour'], how='left')\n",
    "\n",
    "\n",
    "nci_10pct_sCf_df['congestion_index_mean'] = nci_10pct_sCf_df.iloc[: ,2:11].mean(axis=1)  \n",
    "nci_10pct_sCf_df.insert(13, 'sample_size', \"10-pct\")\n",
    "nci_10pct_sCf_df.insert(14, 'alpha',0.75)\n",
    "nci_10pct_sCf_df.insert(15, 'stuck_time', 30.0)\n",
    "nci_10pct_sCf_df.insert(16,'global_seed', \"default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nci_10pct_sT = []\n",
    "for elem in range(1,11,1):\n",
    "    path = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\"+ str(elem) + \"-fCf_sCF_0.1_gS_4711_sT_300.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "    temp = pd.read_csv(path, sep=\",\")\n",
    "    temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "    nci_10pct_sT.append(temp)\n",
    "\n",
    "nci_10pct_sT_df = pd.merge(nci_10pct_sT[0], nci_10pct_sT[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,10,1):\n",
    "    nci_10pct_sT_df = pd.merge(nci_10pct_sT_df, nci_10pct_sT[elem], on = ['road_type', 'hour'], how='left')\n",
    "\n",
    "\n",
    "nci_10pct_sT_df['congestion_index_mean'] = nci_10pct_sT_df.iloc[: ,2:11].mean(axis=1)  \n",
    "\n",
    "\n",
    "nci_10pct_sT_df.insert(13, 'sample_size', \"10-pct\")\n",
    "nci_10pct_sT_df.insert(14, 'alpha',1.0)\n",
    "nci_10pct_sT_df.insert(15, 'stuck_time', 300.0)\n",
    "nci_10pct_sT_df.insert(16,'global_seed', \"default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nci_10pct_sT_sCf = []\n",
    "for elem in range(1,11,1):\n",
    "    if (elem == 2):\n",
    "        path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-10-pct-2-fCf_0.1_sCF_0.17783_gS_4711_sT_300.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "        temp = pd.read_csv(path, sep=\",\")\n",
    "        temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "        nci_10pct_sT_sCf.append(temp)\n",
    "    else:\n",
    "        path = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\" +str(elem) + \"-fCf_0.1_sCF_0.17783_gS_4711_sT_300.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "        temp = pd.read_csv(path, sep=\",\")\n",
    "        temp.rename(columns={\"congestion_index\": \"congestion_index_\" + str(elem)}, inplace = True)\n",
    "        nci_10pct_sT_sCf.append(temp)\n",
    "\n",
    "\n",
    "nci_10pct_sT_sCf_df = pd.merge(nci_10pct_sT_sCf[0], nci_10pct_sT_sCf[1], on=['road_type','hour'], how='left')\n",
    "for elem in range(2,9,1):\n",
    "    nci_10pct_sT_sCf_df = pd.merge(nci_10pct_sT_sCf_df, nci_10pct_sT_sCf[elem], on = ['road_type', 'hour'], how='left')\n",
    "\n",
    "\n",
    "nci_10pct_sT_sCf_df['congestion_index_mean'] = nci_10pct_sT_sCf_df.iloc[: ,2:10].mean(axis=1) \n",
    "nci_10pct_sT_sCf_df.insert(12, 'sample_size', \"10-pct\")\n",
    "nci_10pct_sT_sCf_df.insert(13, 'alpha',0.75)\n",
    "nci_10pct_sT_sCf_df.insert(14, 'stuck_time', 300.0)\n",
    "nci_10pct_sT_sCf_df.insert(15,'global_seed', \"default\")\n",
    "\n",
    "\n",
    "# 2 nicht da, fehlerhaft!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nci_10pct_all = pd.concat([nci_10pct_df, nci_10pct_sCf_df, nci_10pct_sT_df, nci_10pct_sT_sCf_df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nci_10pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/nci_10pct_samples_some_missing.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_sCF_0.25_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "nci_25pct = pd.DataFrame(pd.read_csv(path, sep = \",\"))\n",
    "nci_25pct.rename(columns={\"congestion_index\": \"congestion_index_mean\"}, inplace = True)\n",
    "nci_25pct.insert(3, 'sample_size', \"25-pct\")\n",
    "nci_25pct.insert(4, 'alpha',1.0)\n",
    "nci_25pct.insert(5, 'stuck_time', 30.0)\n",
    "nci_25pct.insert(6,'global_seed', \"default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_0.25_sCF_0.35355_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "nci_25pct_sCf = pd.DataFrame(pd.read_csv(path, sep = \",\"))\n",
    "nci_25pct_sCf.rename(columns={\"congestion_index\": \"congestion_index_mean\"}, inplace = True)\n",
    "nci_25pct_sCf.insert(3, 'sample_size', \"25-pct\")\n",
    "nci_25pct_sCf.insert(4, 'alpha',0.75)\n",
    "nci_25pct_sCf.insert(5, 'stuck_time', 30.0)\n",
    "nci_25pct_sCf.insert(6,'global_seed', \"default\")\n",
    "\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-25-pct-1-fCf_sCF_0.25_gS_4711_sT_120.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "nci_25pct_sT = pd.DataFrame(pd.read_csv(path, sep = \",\"))\n",
    "nci_25pct_sT.rename(columns={\"congestion_index\": \"congestion_index_mean\"}, inplace = True)\n",
    "nci_25pct_sT.insert(3, 'sample_size', \"25-pct\")\n",
    "nci_25pct_sT.insert(4, 'alpha',1.0)\n",
    "nci_25pct_sT.insert(5, 'stuck_time',120.0)\n",
    "nci_25pct_sT.insert(6,'global_seed', \"default\")\n",
    "\n",
    "# hat gefehlt, hinzugefügt am 22. 07.2025 \n",
    "path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25-pct-1-fCf_0.25_sCF_0.35355_gS_4711_sT_120.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "nci_25pct_sT_sCf = pd.DataFrame(pd.read_csv(path, sep = \",\"))\n",
    "nci_25pct_sT_sCf.insert(3, 'sample_size', \"25-pct\")\n",
    "nci_25pct_sT_sCf.insert(4, 'sample_nr', 1)\n",
    "nci_25pct_sT_sCf.insert(5, 'alpha',0.75)\n",
    "nci_25pct_sT_sCf.insert(6, 'stuck_time',120.0)\n",
    "nci_25pct_sT_sCf.insert(7,'global_seed', \"default\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nci_25pct_all = pd.concat([nci_25pct, nci_25pct_sCf, nci_25pct_sT, nci_25pct_sT_sCf], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nci_25pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/nci_25pct_samples_sCf_sT_missing.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_sCF_0.5_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "nci_50pct = pd.DataFrame(pd.read_csv(path, sep = \",\"))\n",
    "nci_50pct.rename(columns={\"congestion_index\": \"congestion_index_mean\"}, inplace = True)\n",
    "nci_50pct.insert(3, 'sample_size', \"50-pct\")\n",
    "nci_50pct.insert(4, 'alpha',1.0)\n",
    "nci_50pct.insert(5, 'stuck_time', 30.0)\n",
    "nci_50pct.insert(6,'global_seed', \"default\")\n",
    "\n",
    "\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_0.5_sCF_0.5946_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "nci_50pct_sCf = pd.DataFrame(pd.read_csv(path, sep = \",\"))\n",
    "nci_25pct_sCf.rename(columns={\"congestion_index\": \"congestion_index_mean\"}, inplace = True)\n",
    "nci_50pct_sCf.insert(3, 'sample_size', \"50-pct\")\n",
    "nci_50pct_sCf.insert(4, 'alpha',0.75)\n",
    "nci_50pct_sCf.insert(5, 'stuck_time', 30.0)\n",
    "nci_50pct_sCf.insert(6,'global_seed', \"default\")\n",
    "\n",
    "\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_sCF_0.5_gS_4711_sT_60.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "nci_50pct_sT = pd.DataFrame(pd.read_csv(path, sep = \",\"))\n",
    "nci_50pct_sT.rename(columns={\"congestion_index\": \"congestion_index_mean\"}, inplace = True)\n",
    "nci_50pct_sT.insert(3, 'sample_size', \"50-pct\")\n",
    "nci_50pct_sT.insert(4, 'alpha',1.0)\n",
    "nci_50pct_sT.insert(5, 'stuck_time', 60.0)\n",
    "nci_50pct_sT.insert(6,'global_seed', \"default\")\n",
    "\n",
    "path = \"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_0.5_sCF_0.5946_gS_4711_sT_60.0_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "nci_50pct_sT_sCf = pd.DataFrame(pd.read_csv(path, sep = \",\"))\n",
    "nci_50pct_sT_sCf.rename(columns={\"congestion_index\": \"congestion_index_mean\"}, inplace = True)\n",
    "nci_50pct_sT_sCf.insert(3, 'sample_size', \"50-pct\")\n",
    "nci_50pct_sT_sCf.insert(4, 'alpha',0.75)\n",
    "nci_50pct_sT_sCf.insert(5, 'stuck_time', 60.0)\n",
    "nci_50pct_sT_sCf.insert(6,'global_seed', \"default\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25-pct-doubled-fCf_0.5_sCF_0.5_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "nci_25pct_doubled = pd.DataFrame(pd.read_csv(path, sep = \",\"))\n",
    "nci_25pct_doubled.rename(columns={\"congestion_index\": \"congestion_index_mean\"}, inplace = True)\n",
    "nci_25pct_doubled.insert(3, 'sample_size', \"25-pct-doubled\")\n",
    "nci_25pct_doubled.insert(4, 'alpha',1.0)\n",
    "nci_25pct_doubled.insert(5, 'stuck_time', 30.0)\n",
    "nci_25pct_doubled.insert(6,'global_seed', \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nci_50pct_all = pd.concat([nci_50pct, nci_50pct_sCf, nci_50pct_sT, nci_50pct_sT_sCf, nci_25pct_doubled], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nci_50pct_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/nci_50pct_samples.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/math_cluster/output/output-lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "nci_100pct = pd.DataFrame(pd.read_csv(path, sep = \",\"))\n",
    "nci_100pct.rename(columns={\"congestion_index\": \"congestion_index_mean\"}, inplace = True)\n",
    "nci_100pct.insert(3, 'sample_size', \"100-pct\")\n",
    "nci_100pct.insert(4, 'alpha',1.0)\n",
    "nci_100pct.insert(5, 'stuck_time', 30.0)\n",
    "nci_100pct.insert(6,'global_seed', \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25.0-pct-quadrupled-fCf_1.0_sCF_1.0_gS_4711_3765/analysis/traffic/traffic_stats_by_road_type_and_hour.csv\"\n",
    "nci_25pct_quadrupled = pd.DataFrame(pd.read_csv(path, sep = \",\"))\n",
    "nci_25pct_quadrupled.rename(columns={\"congestion_index\": \"congestion_index_mean\"}, inplace = True)\n",
    "nci_25pct_quadrupled.insert(3, 'sample_size', \"25-pct-quadrupled\")\n",
    "nci_25pct_quadrupled.insert(4, 'alpha',1.0)\n",
    "nci_25pct_quadrupled.insert(5, 'stuck_time', 30.0)\n",
    "nci_25pct_quadrupled.insert(6,'global_seed', \"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concat and write csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nci_all = pd.concat([nci_1pct_all, nci_5pct_all, nci_10pct_all, nci_25pct_all, nci_50pct_all, nci_100pct, nci_25pct_quadrupled], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nci_all.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/nci_100pct_only_5pct_rGs_6384_6003_missing.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data on one motorway, primary and residential link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readStatsByLinkAndReturnValue(pathToFile, LinkId, sampleSize, sampleNr, alpha, stuckTime ):\n",
    "    scaling_factor_to_100pct = 100.0/ sampleSize\n",
    "    df = pd.read_csv(pathToFile)\n",
    "    temp = df[df['link_id'] == LinkId].copy()\n",
    "    temp['road_capacity_utilization'] = temp['road_capacity_utilization']*scaling_factor_to_100pct\n",
    "    temp['simulated_traffic_volume'] = temp['simulated_traffic_volume']*scaling_factor_to_100pct\n",
    "    temp['vol_car'] = temp['vol_car']*scaling_factor_to_100pct\n",
    "    temp.insert(9,'sample_nr', sampleNr)\n",
    "    temp.insert(10, 'sample_size', str(sampleSize) + \"-pct\")\n",
    "    temp.insert(11, 'alpha', alpha)\n",
    "    temp.insert(12, 'stuck_time', stuckTime)\n",
    "    return temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readStatsByLinkAndReturnShortCongLinks(pathToFile, sampleSize, sampleNr, alpha, stuckTime, cI_value, link_length):\n",
    "    scaling_factor_to_100pct = 100.0/ sampleSize\n",
    "    df = pd.read_csv(pathToFile)\n",
    "    temp = df[(df['congestion_index']<= cI_value) & (df['lane_km'] <= link_length)].copy()\n",
    "    temp['road_capacity_utilization'] = temp['road_capacity_utilization']*scaling_factor_to_100pct\n",
    "    temp['simulated_traffic_volume'] = temp['simulated_traffic_volume']*scaling_factor_to_100pct\n",
    "    temp['vol_car'] = temp['vol_car']*scaling_factor_to_100pct\n",
    "    temp.insert(9,'sample_nr', sampleNr)\n",
    "    temp.insert(10, 'sample_size', str(sampleSize) + \"-pct\")\n",
    "    temp.insert(11, 'alpha', alpha)\n",
    "    temp.insert(12, 'stuck_time', stuckTime)\n",
    "    return temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.01\"]\n",
    "storCapF =  [\"0.01\", \"0.03162\"]\n",
    "\n",
    "# initialize empty data frames\n",
    "df_3_links_mw_pr_res_1pct = pd.DataFrame()\n",
    "df_link_cong_len_leq_100m_1pct = pd.DataFrame()\n",
    "\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "        for sampleNr in range(1,11,1):\n",
    "            # calculate adjusted stuck time\n",
    "            default_stuck_time = 30.0\n",
    "            adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "            # declare sample size as str \"1-pct\"\n",
    "            sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "            sample_size = float(fCf)*100\n",
    "\n",
    "            if ((fCf == \"0.01\") & (sCf == '0.01')):\n",
    "                # declare alpha\n",
    "                alpha = 1.0\n",
    "                # paths for case 1 and 3 \n",
    "                path_case1 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\"+ str(sampleNr) +\"-fCf_sCF_\" + sCf + \"_gS_default_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                path_case3 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" +str(sampleNr) +\"-fCf_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                \n",
    "                temp_mw_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                temp_mw_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                temp_pr_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                temp_pr_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                temp_rs_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                temp_rs_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                # concatenate with existing values\n",
    "                df_3_links_mw_pr_res_1pct = pd.concat([df_3_links_mw_pr_res_1pct, temp_mw_case1, temp_mw_case3, temp_pr_case1, temp_pr_case3, temp_rs_case1, temp_rs_case3 ], ignore_index= True)\n",
    "\n",
    "                temp_case1_C100m = readStatsByLinkAndReturnShortCongLinks(path_case1, sample_size, sampleNr, alpha, default_stuck_time, 0.5, 0.1)\n",
    "                temp_case3_C100m = readStatsByLinkAndReturnShortCongLinks(path_case3, sample_size, sampleNr, alpha, adjusted_stuck_time, 0.5, 0.1)\n",
    "\n",
    "                df_link_cong_len_leq_100m_1pct = pd.concat([df_link_cong_len_leq_100m_1pct, temp_case1_C100m, temp_case3_C100m ], ignore_index= True)\n",
    "\n",
    "\n",
    "            else:\n",
    "                alpha = 0.75\n",
    "                path_case2 = \"/home/lola/math_cluster/output/output-lausitz-1pct-\" +str(sampleNr) + \"-fCf_0.01_sCF_\" + sCf + \"_gS_default_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                path_case4 = \"/home/lola/math_cluster/output/output-lausitz-1-pct-\" + str(sampleNr) + \"-fCf_\" + fCf + \"_sCF_\" + sCf + \"_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                \n",
    "                temp_mw_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                temp_mw_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                temp_pr_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                temp_pr_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                temp_rs_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                temp_rs_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                # concatenate with existing values\n",
    "                df_3_links_mw_pr_res_1pct = pd.concat([df_3_links_mw_pr_res_1pct, temp_mw_case2, temp_mw_case4, temp_pr_case2, temp_pr_case4, temp_rs_case2, temp_rs_case4 ], ignore_index= True)\n",
    "\n",
    "                temp_case2_C100m = readStatsByLinkAndReturnShortCongLinks(path_case2, sample_size, sampleNr, alpha, default_stuck_time, 0.5, 0.1)\n",
    "                temp_case4_C100m = readStatsByLinkAndReturnShortCongLinks(path_case4, sample_size, sampleNr, alpha, adjusted_stuck_time, 0.5, 0.1)\n",
    "\n",
    "                df_link_cong_len_leq_100m_1pct = pd.concat([df_link_cong_len_leq_100m_1pct, temp_case2_C100m, temp_case4_C100m ], ignore_index= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.05\"]\n",
    "storCapF =  [\"0.05\", \"0.10574\"]\n",
    "\n",
    "\n",
    "df_3_links_mw_pr_res_5pct = pd.DataFrame()\n",
    "df_link_cong_len_leq_100m_5pct = pd.DataFrame()\n",
    "\n",
    "\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,11,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "                sample_size = float(fCf)*100\n",
    "                # declare path based on case \n",
    "\n",
    "                \n",
    "                if((fCf == \"0.05\") & (sCf == \"0.05\")):\n",
    "                    alpha = 1.0\n",
    "                    if (sampleNr == 6):\n",
    "                        path_case1  = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_sCF_0.05_gS_4711_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                        path_case3 =  path = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str(sampleNr) + \"-fCf_sCF_0.05_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "\n",
    "                    else: \n",
    "                        path_case1 = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(sampleNr) + \"-fCf_sCF_0.05_gS_4711_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                        path_case3 =  path = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str(sampleNr) + \"-fCf_sCF_0.05_gS_4711_sT_\" + str(adjusted_stuck_time) + \"_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    \n",
    "                    temp_mw_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_mw_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_pr_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_pr_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_rs_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_rs_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    df_3_links_mw_pr_res_5pct = pd.concat([df_3_links_mw_pr_res_5pct, temp_mw_case1, temp_mw_case3, temp_pr_case1, temp_pr_case3, temp_rs_case1, temp_rs_case3 ], ignore_index= True)\n",
    "\n",
    "                    # count how many links are congested with a length of less thann 100m\n",
    "\n",
    "                    temp_case1_C100m = readStatsByLinkAndReturnShortCongLinks(path_case1, sample_size, sampleNr, alpha, default_stuck_time, 0.5, 0.1)\n",
    "                    temp_case3_C100m = readStatsByLinkAndReturnShortCongLinks(path_case3, sample_size, sampleNr, alpha, adjusted_stuck_time, 0.5, 0.1)\n",
    "\n",
    "                    df_link_cong_len_leq_100m_5pct = pd.concat([df_link_cong_len_leq_100m_5pct, temp_case1_C100m, temp_case3_C100m ], ignore_index= True)\n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    # case 2\n",
    "                    if(sampleNr == 6):\n",
    "                        path_case2 = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-6-fCf_0.05_sCF_0.10574_gS_4711_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                        path_case4 = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str(sampleNr) + \"-fCf_0.05_sCF_0.10574_gS_4711_sT_600.0_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    else:\n",
    "                        path_case2 = \"/home/lola/math_cluster/output/output-lausitz-5.0-pct-\" + str(sampleNr) + \"-fCf_0.05_sCF_0.10574_gS_4711_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                        path_case4 = \"/home/lola/math_cluster/output/output-lausitz-5-pct-\" + str(sampleNr) + \"-fCf_0.05_sCF_0.10574_gS_4711_sT_600.0_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    temp_mw_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_mw_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_pr_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_pr_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_rs_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_rs_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    df_3_links_mw_pr_res_5pct = pd.concat([df_3_links_mw_pr_res_5pct, temp_mw_case2, temp_mw_case4, temp_pr_case2, temp_pr_case4, temp_rs_case2, temp_rs_case4 ], ignore_index= True)\n",
    "\n",
    "                    # count how many links are congested with a length of less thann 100m\n",
    "                    temp_case2_C100m = readStatsByLinkAndReturnShortCongLinks(path_case2, sample_size, sampleNr, alpha, default_stuck_time, 0.5, 0.1)\n",
    "                    temp_case4_C100m = readStatsByLinkAndReturnShortCongLinks(path_case4, sample_size, sampleNr, alpha, adjusted_stuck_time, 0.5, 0.1)\n",
    "\n",
    "                    df_link_cong_len_leq_100m_5pct = pd.concat([df_link_cong_len_leq_100m_5pct, temp_case2_C100m, temp_case4_C100m ], ignore_index= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.1\"]\n",
    "storCapF =  [\"0.1\", \"0.17783\"]\n",
    "\n",
    "df_3_links_mw_pr_res_10pct = pd.DataFrame()\n",
    "df_link_cong_len_leq_100m_10pct = pd.DataFrame()\n",
    "\n",
    "counter = 0\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,11,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "                sample_size = float(fCf)*100\n",
    "                # declare path based on case\n",
    "                if((fCf == \"0.1\") & (sCf == \"0.1\")):\n",
    "                    alpha = 1.0\n",
    "                    path_case1 = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" +str(sampleNr) + \"-fCf_sCF_0.1_gS_4711_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    path_case3 = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\" + str(sampleNr) + \"-fCf_sCF_0.1_gS_4711_sT_300.0_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    \n",
    "                    temp_mw_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_mw_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_pr_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_pr_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_rs_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_rs_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    df_3_links_mw_pr_res_10pct = pd.concat([df_3_links_mw_pr_res_10pct, temp_mw_case1, temp_mw_case3, temp_pr_case1, temp_pr_case3, temp_rs_case1, temp_rs_case3 ], ignore_index= True)\n",
    "\n",
    "                    #count how many links are congested with a length of less than 100m\n",
    "                    temp_case1_C100m = readStatsByLinkAndReturnShortCongLinks(path_case1, sample_size, sampleNr, alpha, default_stuck_time, 0.5, 0.1)\n",
    "                    temp_case3_C100m = readStatsByLinkAndReturnShortCongLinks(path_case3, sample_size, sampleNr, alpha, adjusted_stuck_time, 0.5, 0.1)\n",
    "\n",
    "                    df_link_cong_len_leq_100m_10pct = pd.concat([df_link_cong_len_leq_100m_10pct, temp_case1_C100m, temp_case3_C100m ], ignore_index= True)\n",
    "               \n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    if(sampleNr==2):\n",
    "                        path_case4 = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-10-pct-2-fCf_0.1_sCF_0.17783_gS_4711_sT_300.0_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                        temp_mw_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "                        temp_pr_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "                        temp_rs_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "                        df_3_links_mw_pr_res_10pct = pd.concat([df_3_links_mw_pr_res_10pct, temp_mw_case4, temp_pr_case4, temp_rs_case4 ], ignore_index= True)\n",
    "\n",
    "                        temp_case4_C100m = readStatsByLinkAndReturnShortCongLinks(path_case4, sample_size, sampleNr, alpha, adjusted_stuck_time, 0.5, 0.1)\n",
    "                        df_link_cong_len_leq_100m_10pct = pd.concat([df_link_cong_len_leq_100m_10pct, temp_case4_C100m ], ignore_index= True)\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        path_case4 = \"/home/lola/math_cluster/output/output-lausitz-10-pct-\" + str(sampleNr) + \"-fCf_0.1_sCF_0.17783_gS_4711_sT_300.0_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                        temp_mw_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "                        temp_pr_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "                        temp_rs_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "                        df_3_links_mw_pr_res_10pct = pd.concat([df_3_links_mw_pr_res_10pct, temp_mw_case4, temp_pr_case4, temp_rs_case4 ], ignore_index= True)\n",
    "\n",
    "                        temp_case4_C100m = readStatsByLinkAndReturnShortCongLinks(path_case4, sample_size, sampleNr, alpha, adjusted_stuck_time, 0.5, 0.1)\n",
    "                        df_link_cong_len_leq_100m_10pct = pd.concat([df_link_cong_len_leq_100m_10pct, temp_case4_C100m ], ignore_index= True)\n",
    "                          \n",
    "                    \n",
    "                    path_case2 = \"/home/lola/math_cluster/output/output-lausitz-10.0-pct-\" + str(sampleNr) + \"-fCf_0.1_sCF_0.17783_gS_4711_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    temp_mw_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_pr_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_rs_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    \n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    df_3_links_mw_pr_res_10pct = pd.concat([df_3_links_mw_pr_res_10pct, temp_mw_case2,  temp_pr_case2,  temp_rs_case2 ], ignore_index= True)\n",
    "\n",
    "                    # count how many links are congested with a length of less than 100m\n",
    "\n",
    "                    temp_case2_C100m = readStatsByLinkAndReturnShortCongLinks(path_case2, sample_size, sampleNr, alpha, default_stuck_time, 0.5, 0.1)\n",
    "                    \n",
    "\n",
    "                    df_link_cong_len_leq_100m_10pct = pd.concat([df_link_cong_len_leq_100m_10pct, temp_case2_C100m, temp_case4_C100m ], ignore_index= True)\n",
    "\n",
    "\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.25\"]\n",
    "storCapF =  [\"0.25\", \"0.35355\"]\n",
    "\n",
    "df_3_links_mw_pr_res_25pct = pd.DataFrame()\n",
    "df_link_cong_len_leq_100m_25pct = pd.DataFrame()\n",
    "counter = 0\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,2,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "                sample_size = float(fCf)*100\n",
    "                # declare path based on case\n",
    "                if((fCf == \"0.25\") & (sCf == \"0.25\")):\n",
    "                    alpha = 1.0\n",
    "                    path_case1 = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_sCF_0.25_gS_4711_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    path_case3 = \"/home/lola/math_cluster/output/output-lausitz-25-pct-1-fCf_sCF_0.25_gS_4711_sT_120.0_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    \n",
    "                    temp_mw_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_mw_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_pr_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_pr_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_rs_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_rs_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    df_3_links_mw_pr_res_25pct = pd.concat([df_3_links_mw_pr_res_25pct, temp_mw_case1, temp_mw_case3, temp_pr_case1, temp_pr_case3, temp_rs_case1, temp_rs_case3 ], ignore_index= True)\n",
    "\n",
    "                    #count how many links are congested with a length of less than 100m\n",
    "                    temp_case1_C100m = readStatsByLinkAndReturnShortCongLinks(path_case1, sample_size, sampleNr, alpha, default_stuck_time, 0.5, 0.1)\n",
    "                    temp_case3_C100m = readStatsByLinkAndReturnShortCongLinks(path_case3, sample_size, sampleNr, alpha, adjusted_stuck_time, 0.5, 0.1)\n",
    "\n",
    "                    df_link_cong_len_leq_100m_25pct = pd.concat([df_link_cong_len_leq_100m_25pct, temp_case1_C100m, temp_case3_C100m ], ignore_index= True)\n",
    "\n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    path_case2 = \"/home/lola/math_cluster/output/output-lausitz-25.0-pct-fCf_0.25_sCF_0.35355_gS_4711_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    path_case4 = \"/home/lola/Nextcloud/Masterarbeit/03_Outputs_From_RunsLausitz/output-lausitz-25-pct-1-fCf_0.25_sCF_0.35355_gS_4711_sT_120.0_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    temp_mw_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_mw_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_pr_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_pr_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_rs_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_rs_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    df_3_links_mw_pr_res_25pct = pd.concat([df_3_links_mw_pr_res_25pct, temp_mw_case2, temp_mw_case4, temp_pr_case2, temp_pr_case4, temp_rs_case2, temp_rs_case4 ], ignore_index= True)\n",
    "\n",
    "                    # count how many links are congested with a length of less thann 100m\n",
    "                    temp_case2_C100m = readStatsByLinkAndReturnShortCongLinks(path_case2, sample_size, sampleNr, alpha, default_stuck_time, 0.5, 0.1)\n",
    "                    temp_case4_C100m = readStatsByLinkAndReturnShortCongLinks(path_case4, sample_size, sampleNr, alpha, adjusted_stuck_time, 0.5, 0.1)\n",
    "\n",
    "                    df_link_cong_len_leq_100m_25pct = pd.concat([df_link_cong_len_leq_100m_25pct, temp_case2_C100m, temp_case4_C100m ], ignore_index= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 50 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"0.5\"]\n",
    "storCapF =  [\"0.5\", \"0.5946\"]\n",
    "\n",
    "df_3_links_mw_pr_res_50pct = pd.DataFrame()\n",
    "df_link_cong_len_leq_100m_50pct = pd.DataFrame()\n",
    "\n",
    "counter = 0\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,2,1):\n",
    "                # calculate adjusted stuck time\n",
    "                default_stuck_time = 30.0\n",
    "                adjusted_stuck_time = 30.0/float(flowCapF[0])\n",
    "                # declare sample size as str \"1-pct\"\n",
    "                sample_size_as_string = str(int(float(fCf)*100)) + \"-pct\"\n",
    "                sample_size = float(fCf)*100\n",
    "                # declare path based on case \n",
    "                if((fCf == \"0.5\") & (sCf == \"0.5\")):\n",
    "                    alpha = 1.0\n",
    "                    path_case1 = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_sCF_0.5_gS_4711_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    path_case3 = \"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_sCF_0.5_gS_4711_sT_60.0_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    temp_mw_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_mw_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_pr_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_pr_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_rs_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_rs_case3 = readStatsByLinkAndReturnValue(pathToFile=path_case3, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    df_3_links_mw_pr_res_50pct = pd.concat([df_3_links_mw_pr_res_50pct, temp_mw_case1, temp_mw_case3, temp_pr_case1, temp_pr_case3, temp_rs_case1, temp_rs_case3 ], ignore_index= True)\n",
    "\n",
    "                    #count how many links are congested with a length of less than 100m\n",
    "                    temp_case1_C100m = readStatsByLinkAndReturnShortCongLinks(path_case1, sample_size, sampleNr, alpha, default_stuck_time, 0.5, 0.1)\n",
    "                    temp_case3_C100m = readStatsByLinkAndReturnShortCongLinks(path_case3, sample_size, sampleNr, alpha, adjusted_stuck_time, 0.5, 0.1)\n",
    "\n",
    "                    df_link_cong_len_leq_100m_50pct = pd.concat([df_link_cong_len_leq_100m_50pct, temp_case1_C100m, temp_case3_C100m ], ignore_index= True)\n",
    "\n",
    "                else:\n",
    "                    alpha = 0.75\n",
    "                    path_case2 = \"/home/lola/math_cluster/output/output-lausitz-50.0-pct-fCf_0.5_sCF_0.5946_gS_4711_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    path_case4 = \"/home/lola/math_cluster/output/output-lausitz-50-pct-1-fCf_0.5_sCF_0.5946_gS_4711_sT_60.0_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                    \n",
    "                    temp_mw_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_mw_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_pr_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_pr_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    temp_rs_case2 = readStatsByLinkAndReturnValue(pathToFile=path_case2, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=default_stuck_time)\n",
    "                    temp_rs_case4 = readStatsByLinkAndReturnValue(pathToFile=path_case4, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=alpha, stuckTime=adjusted_stuck_time)\n",
    "\n",
    "                    # concatenate with existing values\n",
    "                    df_3_links_mw_pr_res_50pct = pd.concat([df_3_links_mw_pr_res_50pct, temp_mw_case2, temp_mw_case4, temp_pr_case2, temp_pr_case4, temp_rs_case2, temp_rs_case4 ], ignore_index= True)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 100 pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCapF = [\"1.0\"]\n",
    "storCapF =  [\"1.0\"]\n",
    "\n",
    "\n",
    "df_3_links_mw_pr_res_100pct = pd.DataFrame()\n",
    "df_link_cong_len_leq_100m_100pct = pd.DataFrame()\n",
    "\n",
    "counter = 0\n",
    "for fCf in flowCapF:\n",
    "    for sCf in storCapF:\n",
    "            for sampleNr in range(1,2,1):\n",
    "                # declare path based on case \n",
    "                alpha = 1.0\n",
    "                sample_size = float(fCf)*100\n",
    "                path = \"/home/lola/math_cluster/output/output-lausitz-100.0-pct-fCf_sCF_1.0_gS_4711_3765/analysis/traffic/traffic_stats_by_link_daily.csv\"\n",
    "                temp_mw_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"314328566\", sampleSize= sample_size, sampleNr = sampleNr, alpha=1.0, stuckTime=30.0)\n",
    "                temp_pr_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"314040202\", sampleSize= sample_size, sampleNr = sampleNr, alpha=1.0, stuckTime=30.0)\n",
    "                temp_rs_case1 = readStatsByLinkAndReturnValue(pathToFile=path_case1, LinkId=\"130268155\", sampleSize= sample_size, sampleNr = sampleNr, alpha=1.0, stuckTime=30.0)\n",
    "\n",
    "                df_3_links_mw_pr_res_100pct = pd.concat([df_3_links_mw_pr_res_100pct, temp_mw_case1, temp_pr_case1, temp_rs_case1 ], ignore_index= True)\n",
    "\n",
    "                #count how many links are congested with a length of less than 100m\n",
    "                temp_case1_C100m = readStatsByLinkAndReturnShortCongLinks(path_case1, sample_size, sampleNr, alpha, default_stuck_time, 0.5, 0.1)\n",
    "\n",
    "                df_link_cong_len_leq_100m_100pct = pd.concat([df_link_cong_len_leq_100m_100pct, temp_case1_C100m ], ignore_index= True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### concat and write output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3_links_mw_pr_res_1_100 = pd.concat([df_3_links_mw_pr_res_1pct, df_3_links_mw_pr_res_5pct, df_3_links_mw_pr_res_10pct, df_3_links_mw_pr_res_25pct, df_3_links_mw_pr_res_50pct, df_3_links_mw_pr_res_100pct], ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3_links_mw_pr_res_1_100.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/traffic_stats_by_link_314328566_314040202_130268155.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_link_cong_len_leq_100m_1_100 = pd.concat([df_link_cong_len_leq_100m_1pct, df_link_cong_len_leq_100m_5pct, df_link_cong_len_leq_100m_10pct, df_link_cong_len_leq_100m_25pct, df_link_cong_len_leq_100m_50pct, df_link_cong_len_leq_100m_100pct], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_link_cong_len_leq_100m_1pct[(df_link_cong_len_leq_100m_1pct[\"alpha\"] == 1.0) & (df_link_cong_len_leq_100m_1pct[\"stuck_time\"] == 30.0) & (df_link_cong_len_leq_100m_1pct[\"sample_nr\"] == 10.0)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_link_cong_len_leq_100m_1_100.to_csv('/home/lola/Nextcloud/Masterarbeit/03_Outputs/traffic_stats_by_link_with_cI_leq0.5_and_length_leq100m_1_100pct.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
